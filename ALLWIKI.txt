Darwinism

Darwinism is a theory of biological evolution developed by the English naturalist Charles Darwin (1809–1882) and others, stating that all species of organisms arise and develop through the natural selection of small, inherited variations that increase the individual's ability to compete, survive, and reproduce. Also called Darwinian theory, it originally included the broad concepts of transmutation of species or of evolution which gained general scientific acceptance after Darwin published On the Origin of Species in 1859, including concepts which predated Darwin's theories. It subsequently referred to the specific concepts of natural selection, the Weismann barrier, or the central dogma of molecular biology. Though the term usually refers strictly to biological evolution, creationists have appropriated it to refer to the origin of life, and it has even been applied to concepts of cosmic evolution, both of which have no connection to Darwin's work. It is therefore considered the belief and acceptance of Darwin's and of his predecessors' work—in place of other theories, including divine design and extraterrestrial origins.
English biologist Thomas Henry Huxley coined the term Darwinism in April 1860. It was used to describe evolutionary concepts in general, including earlier concepts published by English philosopher Herbert Spencer. Many of the proponents of Darwinism at that time, including Huxley, had reservations about the significance of natural selection, and Darwin himself gave credence to what was later called Lamarckism. The strict neo-Darwinism of German evolutionary biologist August Weismann gained few supporters in the late 19th century. During the approximate period of the 1880s to about 1920, sometimes called "the eclipse of Darwinism," scientists proposed various alternative evolutionary mechanisms which eventually proved untenable. The development of the modern synthesis in the early 20th century, incorporating natural selection with population genetics and Mendelian genetics, revived Darwinism in an updated form.
While the term Darwinism has remained in use amongst the public when referring to modern evolutionary theory, it has increasingly been argued by science writers such as Olivia Judson and Eugenie Scott that it is an inappropriate term for modern evolutionary theory. For example, Darwin was unfamiliar with the work of the Moravian scientist and Augustinian friar Gregor Mendel, and as a result had only a vague and inaccurate understanding of heredity. He naturally had no inkling of later theoretical developments and, like Mendel himself, knew nothing of genetic drift, for example. In the United States, creationists often use the term "Darwinism" as a pejorative term in reference to beliefs such as scientific materialism, but in the United Kingdom the term has no negative connotations, being freely used as a shorthand for the body of theory dealing with evolution, and in particular, with evolution by natural selection.Social Darwinism

The term Social Darwinism is used to refer to various ways of thinking and theories that emerged in the second half of the 19th century and tried to apply the evolutionary concept of natural selection to human society. The term itself emerged in the 1880s, and it gained widespread currency when used after 1944 by opponents of these ways of thinking. The majority of those who have been categorized as social Darwinists did not identify themselves by such a label.
Scholars debate the extent to which the various Social Darwinist ideologies reflect Charles Darwin's own views on human social and economic issues. His writings have passages that can be interpreted as opposing aggressive individualism, while other passages appear to promote it. Some scholars argue that Darwin's view gradually changed and came to incorporate views from other theorists such as Herbert Spencer. Spencer published his Lamarckian evolutionary ideas about society before Darwin first published his theory in 1859, and both Spencer and Darwin promoted their own conceptions of moral values. Spencer supported laissez-faire capitalism on the basis of his Lamarckian belief that struggle for survival spurred self-improvement which could be inherited. An important proponent in Germany was Ernst Haeckel, who popularized Darwin's thought (and personal interpretation of it) and used it as well to contribute to a new creed, the Monist movement.Eddie Lacy

Eddie Darwin Lacy Jr. (born June 2, 1990) is an American football running back for the Seattle Seahawks of the National Football League (NFL). He played college football at Alabama, where he was a member of three BCS National Championship teams in the 2009, 2011, and 2012 seasons. He was drafted by the Green Bay Packers in the second round of the 2013 NFL Draft.Universal Darwinism

Universal Darwinism (also known as generalized Darwinism, universal selection theory, or Darwinian metaphysics) refers to a variety of approaches that extend the theory of Darwinism beyond its original domain of biological evolution on Earth. Universal Darwinism aims to formulate a generalized version of the mechanisms of variation, selection and heredity proposed by Charles Darwin, so that they can apply to explain evolution in a wide variety of other domains, including psychology, economics, culture, medicine, computer science and physics.Charles Darwin

Charles Robert Darwin,  (; 12 February 1809 – 19 April 1882) was an English naturalist, geologist and biologist, best known for his contributions to the science of evolution. He established that all species of life have descended over time from common ancestors and, in a joint publication with Alfred Russel Wallace, introduced his scientific theory that this branching pattern of evolution resulted from a process that he called natural selection, in which the struggle for existence has a similar effect to the artificial selection involved in selective breeding.
Darwin published his theory of evolution with compelling evidence in his 1859 book On the Origin of Species, overcoming scientific rejection of earlier concepts of transmutation of species. By the 1870s, the scientific community and much of the general public had accepted evolution as a fact. However, many favoured competing explanations and it was not until the emergence of the modern evolutionary synthesis from the 1930s to the 1950s that a broad consensus developed in which natural selection was the basic mechanism of evolution. Darwin's scientific discovery is the unifying theory of the life sciences, explaining the diversity of life.
Darwin's early interest in nature led him to neglect his medical education at the University of Edinburgh; instead, he helped to investigate marine invertebrates. Studies at the University of Cambridge (Christ's College) encouraged his passion for natural science. His five-year voyage on HMS Beagle established him as an eminent geologist whose observations and theories supported Charles Lyell's uniformitarian ideas, and publication of his journal of the voyage made him famous as a popular author.
Puzzled by the geographical distribution of wildlife and fossils he collected on the voyage, Darwin began detailed investigations and in 1838 conceived his theory of natural selection. Although he discussed his ideas with several naturalists, he needed time for extensive research and his geological work had priority. He was writing up his theory in 1858 when Alfred Russel Wallace sent him an essay that described the same idea, prompting immediate joint publication of both of their theories. Darwin's work established evolutionary descent with modification as the dominant scientific explanation of diversification in nature. In 1871 he examined human evolution and sexual selection in The Descent of Man, and Selection in Relation to Sex, followed by The Expression of the Emotions in Man and Animals (1872). His research on plants was published in a series of books, and in his final book, The Formation of Vegetable Mould, through the Actions of Worms (1881), he examined earthworms and their effect on soil.
Darwin has been described as one of the most influential figures in human history, and he was honoured by burial in Westminster Abbey.Darwinism (book)

Darwinism: An Exposition of the Theory of Natural Selection with Some of Its Applications is an 1889 book on biological evolution by Alfred Russel Wallace, the co-discoverer of evolution by natural selection together with Charles Darwin. This was a book Wallace wrote as a defensive response to the scientific critics of natural selection. Of all Wallace's books, it is cited by scholarly publications the most.
In the preface to Darwinism, Wallace had used the term pure-Darwinism which proposed a "greater efficacy" for natural selection. The book is notable for defending August Weismann's theory of heredity and rejecting the inheritance of acquired characteristics and the concept of sexual selection which Darwin gave credence to. George Romanes dubbed this view as "Wallaceism", noting that in contrast to Darwin, this position was advocating a "pure theory of natural selection to the exclusion of any supplementary theory." The book is seen as laying the foundation for the neo-Darwinian theory of evolution.Flowering plant

The flowering plants (angiosperms), also known as Angiospermae or Magnoliophyta, are the most diverse group of land plants, with 416 families, approx. 13,164 known genera and a total of c. 295,383 known species. Like gymnosperms, angiosperms are seed-producing plants; they are distinguished from gymnosperms by characteristics including flowers, endosperm within the seeds, and the production of fruits that contain the seeds. Etymologically, angiosperm means a plant that produces seeds within an enclosure, in other words, a fruiting plant. The term "angiosperm" comes from the Greek composite word (angeion, "case" or "casing", and sperma, "seed") meaning "enclosed seeds", after the enclosed condition of the seeds.
The ancestors of flowering plants diverged from gymnosperms in the Triassic Period, during the range 245 to 202 million years ago (mya), and the first flowering plants are known from 160 mya. They diversified extensively during the Lower Cretaceous, became widespread by 120 mya, and replaced conifers as the dominant trees from 100 to 60 mya.Atoll

An atoll ( , , , ,  or ), sometimes called a coral atoll, is a ring-shaped coral reef including a coral rim that encircles a lagoon partially or completely. There may be coral islands/cays on the rim. The coral of the atoll often sits atop the rim of an extinct seamount or volcano which has eroded or subsided partially beneath the water. The lagoon forms over the volcanic crater or caldera while the higher rim remains above water or at shallow depths that permit the coral to grow and form the reefs. For the atoll to persist, continued erosion or subsidence must be at a rate slow enough to permit reef growth upwards and outwards to replace the lost height.Neural Darwinism

Neural Darwinism, a large scale theory of brain function by Gerald Edelman, was initially published in 1978, in a book called The Mindful Brain (MIT Press). It was extended and published in the 1987 book Neural Darwinism – The Theory of Neuronal Group Selection.
In 1972, Edelman was awarded the Nobel Prize in Medicine or Physiology (shared with Rodney Porter of Great Britain) for his work in immunology showing how the population of lymphocytes capable of binding to a foreign antigen is increased by differential clonal multiplication following antigen discovery. Essentially, this proved that the human body is capable of creating complex adaptive systems as a result of local events with feedback. Edelman's interest in selective systems expanded into the fields of neurobiology and neurophysiology, and in Neural Darwinism, Edelman puts forth a theory called "neuronal group selection". It contains three major parts:
Anatomical connectivity in the brain occurs via selective mechanochemical events that take place epigenetically during development. This creates a diverse primary repertoire by differential reproduction.
Once structural diversity is established anatomically, a second selective process occurs during postnatal behavioral experience through epigenetic modifications in the strength of synaptic connections between neuronal groups. This creates a diverse secondary repertoire by differential amplification.
Reentrant signaling between neuronal groups allows for spatiotemporal continuity in response to real-world interactions. In "The Remembered Present" (1989) and later, "Bright Air, Brilliant Fire: On the Matter of the Mind" (1992) and "A Universe of Consciousness: How Matter Becomes Imagination" (2001; coauthored with Giulio Tononi), Edelman argues that thalamocortical and corticocortical reentrant signaling are critical to generating and maintaining conscious states in mammals.Quantum Darwinism

Quantum Darwinism is a theory claiming to explain the emergence of the classical world from the quantum world as due to a process of Darwinian natural selection; where the many possible quantum states are selected against in favor of a stable pointer state. It was proposed in 2003 by Wojciech Zurek and a group of collaborators including Ollivier, Poulin, Paz and Blume-Kohout. The development of the theory is due to the integration of a number of Zurek’s research topics pursued over the course of twenty-five years including: pointer states, einselection and decoherence.
A study in 2010 is claimed to provide preliminary supporting evidence of quantum Darwinism with scars of a quantum dot "becoming a family of mother-daughter states" indicating they could "stabilize into multiple pointer states." However, the claimed evidence is also subject to the circularity criticism by Kastner (see Implications below). Basically, the de facto phenomenon of decoherence that underlies the claims of Quantum Darwinism may not really arise in a unitary-only dynamics. Thus, even if there is decoherence, this does not show that macroscopic pointer states naturally emerge without some form of collapse.SQLite

SQLite ( or ) is a relational database management system contained in a C programming library. In contrast to many other database management systems, SQLite is not a client–server database engine. Rather, it is embedded into the end program.
SQLite is ACID-compliant and implements most of the SQL standard, using a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.
SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. SQLite has bindings to many programming languages.SQLite Workbench

SQLite Workbench is a free online GUI to create, edit and manage SQLite databases. It also allows exporting data or accessing it through a remote REST API call, giving a full cloud database based on SQLite.Data control language

Data Control Language (DCL).
A data control language (DCL) is a syntax similar to a computer programming language used to control access to data stored in a database (Authorization). In particular, it is a component of Structured Query Language (SQL).
Examples of DCL commands include:
GRANT to allow specified users to perform specified tasks.
REVOKE to cancel previously granted or denied permissions.
The operations for which privileges may be granted to or revoked from a user or role apply to both the Data definition language (DDL) and the Data manipulation language (DML), and may include CONNECT, SELECT, INSERT, UPDATE, DELETE, EXECUTE, and USAGE.
In the Oracle database, executing a DCL command issues an implicit commit. Hence you cannot roll back the command.
In PostgreSQL, executing DCL is transactional, and can be rolled back.
SQLite does not have any DCL commands as it does not have usernames or logins. Instead, SQLite depends on file system permissions to define who can open and access a database.D. Richard Hipp

Dwayne Richard Hipp (born April 9, 1961) is the architect and primary author of SQLite as well as the Fossil SCM. He also authored the Lemon Parser Generator and CVSTrac. CVSTrac became the inspiration for Trac. He was also a member of the Tcl core team.PhpLiteAdmin

phpLiteAdmin is an open source tool written in PHP intended to handle the administration of SQLite over the World Wide Web. Its feature set, interface, and overall user experience is comparable to that of phpMyAdmin for MySQL. In the same way that SQLite is a flat file database, phpLiteAdmin is distributed in the form of a single PHP file (currently approx. 200 KiB in size). Its ease of installation, portability, and small size go hand in hand with SQLite.SpatiaLite

SpatiaLite is a spatial extension to SQLite, providing vector geodatabase functionality. It is similar to PostGIS, Oracle Spatial, and SQL Server with spatial extensions, although SQLite/SpatiaLite aren't based on client-server architecture: they adopt a simpler personal architecture. i.e. the whole SQL engine is directly embedded within the application itself: a complete database simply is an ordinary file which can be freely copied (or even deleted) and transferred from one computer/OS to a different one without any special precaution.
SpatiaLite extends SQLite's existing spatial support to cover the OGC's SFS specification. It isn't necessary to use SpatiaLite to manage spatial data in SQLite, which has its own implementation of R-tree indexes and geometry types. But SpatiaLite is needed for advanced spatial queries and to support multiple map projections. SpatiaLite is provided natively for Linux and Windows as a software library as well several utilities that incorporate the SpatiaLite library. These utilities include command line tools that extend SQLite's own with spatial macros, a graphical GUI for manipulating Spatialite databases and their data, and a simple desktop GIS tool for browsing data.
Being a single binary file, SpatiaLite is also being used as a GIS vector format to exchange geospatial data.WxSQLite3

wxSQLite3 is a C++ wrapper around the public domain SQLite 3.x database and is specifically designed for use in programs based on the wxWidgets library.
wxSQLite3 does not try to hide the underlying database, in contrary almost all special features of the current SQLite version 3.18.0 are supported, like for example the creation of user defined scalar or aggregate functions. Since SQLite stores strings in UTF-8 encoding, the wxSQLite3 methods provide automatic conversion between wxStrings and UTF-8 strings. This works best for the Unicode builds of wxWidgets. In ANSI builds the current locale conversion object (wxConvCurrent) is used for conversion to/from UTF-8. Special care has to be taken if external administration tools are used to modify the database contents, since not all of these tools operate in Unicode resp. UTF-8 mode.
Since version 1.7.0 optional support for key based database encryption (128 bit AES) is also included. Starting with version 1.9.6 of wxSQLite3 the encryption extension is compatible with the SQLite amalgamation source and includes the extension functions module. Support for 256 bit AES encryption has been added in version 1.9.8.
Since version 3.5.0 the SQLite library is an integrated part of wxSQLite3PhpSQLiteAdmin

phpSQLiteAdmin is a name of two independent web applications, written in PHP, for managing SQLite databases.
phpSQLiteAdmin is a web-based client which leverages PHP scripting and the SQLite file-database system to provide a simple way for users to create databases, create tables, and query their own data using non-industry-standard SQLite syntax.Web storage

Web storage, sometimes known as DOM storage (Document Object Model storage), provides web application software methods and protocols used for storing data in a web browser. Web storage supports persistent data storage, similar to cookies but with a greatly enhanced capacity and no information stored in the HTTP request header. There are two main web storage types: local storage and session storage, behaving similarly to persistent cookies and session cookies respectively.
Web storage is being standardized by the World Wide Web Consortium (W3C). It was originally part of the HTML5 specification, but is now in a separate specification. It is supported by Internet Explorer 8, Mozilla-based browsers (e.g., Firefox 2+, officially from 3.5), Safari 4, Google Chrome 4 (session storage is from 5), and Opera 10.50. As of 14 March 2011 Opera and IE9 support the storage events.List of in-memory databases

This article is a list of in-memory database system software.
Forrester Research created the first in-memory database platform wave in 2015.
Many DBMS support in-memory-only storage engines, including:
Altibase
MySQL
Adaptive Server Enterprise
Raima
solidDBAstrology

Astrology is the study of the movements and relative positions of celestial objects as a means for divining information about human affairs and terrestrial events. Astrology has been dated to at least the 2nd millennium BCE, and has its roots in calendrical systems used to predict seasonal shifts and to interpret celestial cycles as signs of divine communications. Many cultures have attached importance to astronomical events, and some – such as the Indians, Chinese, and Maya – developed elaborate systems for predicting terrestrial events from celestial observations. Western astrology, one of the oldest astrological systems still in use, can trace its roots to 19th–17th century BCE Mesopotamia, from which it spread to Ancient Greece, Rome, the Arab world and eventually Central and Western Europe. Contemporary Western astrology is often associated with systems of horoscopes that purport to explain aspects of a person's personality and predict significant events in their lives based on the positions of celestial objects; the majority of professional astrologers rely on such systems.
Throughout most of its history astrology was considered a scholarly tradition and was common in academic circles, often in close relation with astronomy, alchemy, meteorology, and medicine. It was present in political circles, and is mentioned in various works of literature, from Dante Alighieri and Geoffrey Chaucer to William Shakespeare, Lope de Vega and Calderón de la Barca.
During the 20th century and following the wide-scale adoption of the scientific method, astrology has been challenged successfully on both theoretical and experimental grounds, and has been shown to have no scientific validity or explanatory power. Astrology thus lost its academic and theoretical standing, and common belief in it has largely declined. While polling studies have demonstrated that approximately 25% of Americans, Canadians, and Britons say they continue to believe that star and planet positions affect their lives, astrology is now recognized as pseudoscience.Astrology and the classical elements

Astrology has used the concept of classical elements from antiquity up until the present. In Western astrology and Indian astrology four elements are used, namely Fire, Earth, Air and Water.Leo (astrology)

Leo (♌) (Greek: Λέων, Latin: Leōn), is the fifth astrological sign of the zodiac, originating from the constellation of Leo. It comes after Cancer (Greek: Καρκίνος, Latin: Karkinos) and before Virgo (Greek: Παρθένος, Latin: Parthenos). Under the tropical zodiac, the Sun transits this area approximately between July 23 and August 22; the sign spans the 120th to 150th degree of celestial longitude.Planets in astrology

Planets in astrology have a meaning different from the modern astronomical understanding of what a planet is. Before the age of telescopes, the night sky was thought to consist of two very similar components: fixed stars, which remained motionless in relation to each other, and "wandering stars" (Ancient Greek: ἀστέρες πλανῆται asteres planetai), which moved relative to the fixed stars over the course of the year.
To the Greeks and the other earliest astronomers, this group comprised the five planets visible to the naked eye, and excluded the Earth. Although strictly the term "planet" applied only to those five objects, the term was latterly broadened, particularly in the Middle Ages, to include the Sun and the Moon (sometimes referred to as "Lights"), making a total of seven planets. Astrologers retain this definition today.
To ancient astrologers, the planets represented the will of the gods and their direct influence upon human affairs. To modern astrologers the planets represent basic drives or urges in the unconscious, or energy flow regulators representing dimensions of experience. They express themselves with different qualities in the twelve signs of the zodiac and in the twelve houses. The planets are also related to each other in the form of aspects.
Modern astrologers differ on the source of the planets' influence. Hone writes that the planets exert it directly through gravitation or another, unknown influence. Others hold that the planets have no direct influence in themselves, but are mirrors of basic organizing principles in the universe. In other words, the basic patterns of the universe repeat themselves everywhere, in fractal-like fashion, and "as above, so below". Therefore, the patterns that the planets make in the sky reflect the ebb and flow of basic human impulses. The planets are also associated, especially in the Chinese tradition, with the basic forces of nature.
Listed below are the specific meanings and domains associated with the astrological planets since ancient times, with the main focus on the Western astrological tradition. The planets in Hindu astrology are known as the Navagraha or "nine realms". In Chinese astrology, the planets are associated with the life forces of yin and yang and the five elements, which play an important role in the Chinese form of geomancy known as Feng Shui. Astrologers differ on the signs associated with each planet's exaltation.Hindu astrology

Jyotisha (or Jyotishyam from Sanskrit jyotiṣa, from jyótis- "light, heavenly body") is the traditional Hindu system of astrology, also known as Hindu astrology, Indian astrology, and more recently Vedic astrology. The term Hindu astrology has been in use as the English equivalent of Jyotiṣa since the early 19th century, whereas Vedic astrology is a relatively recent term, entering common usage in the 1980s with self-help publications on Āyurveda or Yoga. Vedanga Jyotisha is one of the earliest texts about astronomy within the Vedas. However, some authors have claimed that the horoscopic astrology in the Indian subcontinent came from Hellenistic influences, post-dating the Vedic period. In epics Ramayana and Mahabharata, only electional astrology, omens, dreams and physiognomy are used.
Following a judgement of the Andhra Pradesh High Court in 2001, which favoured astrology, some Indian universities offer advanced degrees in Hindu astrology.
Astrology is rejected by the scientific community as pseudoscience.Western astrology

Western astrology is the system of astrology most popular in Western countries. Western astrology is historically based on Ptolemy's Tetrabiblos (2nd century CE), which in turn was a continuation of Hellenistic and ultimately Babylonian traditions.
Western astrology is largely horoscopic, that is, it is a form of divination based on the construction of a horoscope for an exact moment, such as a person's birth, in which various cosmic bodies are said to have an influence. Astrology in western popular culture is often reduced to sun sign astrology, which considers only the individual's date of birth (i.e. the "position of the Sun" at that date).
Astrology is generally regarded as pseudoscientific. It has consistently failed objective tests.Sagittarius (astrology)

Sagittarius (♐) (Greek: Τοξότης Toxotes, Latin: Sagittarius) is the ninth astrological sign, which is associated with the constellation Sagittarius and spans 240–270th degrees of the zodiac. Under the tropical zodiac, the sun transits this sign between approximately November 23 and December 21. Greek mythology associates Sagittarius with the centaur Chiron, who mentored Achilles, a Greek hero of the Trojan War, in archery.
Sagittarius, half human and half horse, is the centaur of mythology, the learned healer whose higher intelligence forms a bridge between Earth and Heaven. Also known as the Archer, Sagittarius is represented by the symbol of a bow and arrow.Taurus (astrology)

Taurus (Latin for "the Bull"; symbol: , Unicode: ♉) is the second astrological sign in the present zodiac. It spans the 30–60th degree of the zodiac. The Sun is in the sign of Taurus from about April 21 until about May 21 (Western astrology) or from about May 16 to June 16  (Sidereal astrology). People born between these dates, depending on which system of astrology they subscribe to, may be called Taureans.Cancer (astrology)

Cancer( ♋️ )  (Greek: Καρκίνος, Karkinos; Latin: Cancer) is the fourth astrological sign in the Zodiac, originating from the constellation of Cancer. It spans the 90° to 120° of the zodiac, between 90° and 120° of celestial longitude. Under the tropical zodiac, the Sun transits this area on average between June 22 and July 22, and under the sidereal zodiac, the Sun transits this area between approximately July 16 and August 15.
In astrology, Cancer is the Cardinal sign of the Water Trigon, which is made up of Cancer, Pisces, and Scorpio. It is considered a negative sign, whose domicile, or ruling planet, is the Moon. Though some depictions of Cancer feature a lobster, the sign is most often represented by the crab, based on the Karkinos, a giant crab that harassed Heracles during his fight with the Hydra.Ophiuchus (astrology)

Ophiuchus (⛎) () has sometimes been used in sidereal astrology as a thirteenth sign in addition to the twelve signs of the tropical Zodiac, because the eponymous constellation Ophiuchus (Greek: Ὀφιοῦχος "Serpent-bearer") as defined by the 1930 International Astronomical Union's constellation boundaries is situated behind the sun from November 30 to December 18.
The idea appears to have originated in 1970 with Stephen Schmidt's suggestion of a 14-sign zodiac (also including Cetus as a sign). A 13-sign zodiac has been suggested by Walter Berg and by Mark Yazaki in 1995, a suggestion that achieved some popularity in Japan, where Ophiuchus is known as Hebitsukai-Za (蛇遣座 (へびつかいざ), "The Serpent Bearer").
In sidereal and tropical astrology (including sun sign astrology), a 12-sign zodiac is used based on dividing the ecliptic into 12 equal parts rather than the IAU constellation boundaries. That is, astrological signs do not correspond to the constellations which are their namesakes, particularly not in the case of the tropical system where the divisions are fixed relative to the equinox, moving relative to the constellations.Book of Enoch

The Book of Enoch (also 1 Enoch; Ge'ez: መጽሐፈ ሄኖክ mätṣḥäfä henok) is an ancient Jewish religious work, ascribed by tradition to Enoch, the great-grandfather of Noah, although modern scholars estimate the older sections (mainly in the Book of the Watchers) to date from about 300 BC, and the latest part (Book of Parables) probably to the first century BC.
It is not part of the biblical canon as used by Jews, apart from Beta Israel. Most Christian denominations and traditions may accept the Books of Enoch as having some historical or theological interest, but they generally regard the Books of Enoch as non-canonical or non-inspired. It is regarded as canonical by the Ethiopian Orthodox Tewahedo Church and Eritrean Orthodox Tewahedo Church, but not by any other Christian groups.
It is wholly extant only in the Ge'ez language, with Aramaic fragments from the Dead Sea Scrolls and a few Greek and Latin fragments. For this and other reasons, the traditional Ethiopian belief is that the original language of the work was Ge'ez, whereas non-Ethiopian scholars tend to assert that it was first written in either Aramaic or Hebrew; Ephraim Isaac suggests that the Book of Enoch, like the Book of Daniel, was composed partially in Aramaic and partially in Hebrew. No Hebrew version is known to have survived. It is asserted in the book itself that its author was Enoch, before the Biblical Flood.
Some of the authors of the New Testament were familiar with some of the content of the story. A short section of 1 Enoch (1:9) is cited in the New Testament, Epistle of Jude, Jude 1:14–15, and is attributed there to "Enoch the Seventh from Adam" (1 En 60:8), although this section of 1 Enoch is a midrash on Deuteronomy 33. Several copies of the earlier sections of 1 Enoch were preserved among the Dead Sea Scrolls.Second Book of Enoch

The Second Book of Enoch (usually abbreviated 2 Enoch, and otherwise variously known as Slavonic Enoch or The Secrets of Enoch) is a pseudepigraphic text (a text whose claimed authorship is unfounded) of the Old Testament. It is usually considered to be part of the Apocalyptic literature. The dating often preferred for the writing of 2 Enoch is late 1st century CE. The text has been preserved in full only in Slavonic, but in 2009 it was announced that Coptic fragments of the book had been identified. Greek is indicated as the language from which the Slavonic version was translated. 2 Enoch is not regarded as scripture by Jews or any Christian group. It was rediscovered and published at the end of the 19th century.
Most scholars consider 2 Enoch to be composed by an unknown Jewish sectarian group, while some authors think it is a 1st-century Christian text. Very few scholars consider it a later Christian work.
2 Enoch is distinct from the Book of Enoch, known as 1 Enoch. There is also an unrelated 3 Enoch. The numbering of these texts has been applied by scholars to distinguish the texts from one another.Enoch (ancestor of Noah)

Enoch (; Hebrew: חֲנוֹךְ, Modern H̱anokh, Tiberian Ḥănōḵ; Arabic: أَخْنُوخ‎‎ ʼAkhnūkh, [commonly in Qur'ānic literature]: إِدْرِيس ʼIdrīs) is a figure in Biblical literature. "In the seventh generation from Adam," he was considered the author of the Book of Enoch and also called Enoch the scribe of judgment. In addition to an appearance in the Book of Genesis of the Hebrew Bible, Enoch is the subject of many Jewish, Christian, and Muslim writings.
Enoch was the son of Jared (Genesis 5:19–21), the father of Methuselah, and the great-grandfather of Noah. At 65 years old, he begot Methuselah. Regim and Gaidad are also mentioned as his sons according to 2 Enoch.
The Bible says that Enoch lived 365 years before he was taken by God. The text reads that Enoch "walked with God: and he was no more; for God took him" (Gen 5:21–24), which some Christians interpret as Enoch entering Heaven alive.
This Enoch is not to be confused with Cain's son Enoch (Genesis 4:17). The Christian New Testament has three references to Enoch from the lineage of Seth (Luke 3:37, Hebrews 11:5, Jude 1:14–15).Enoch (son of Cain)

According to the Book of Genesis, Enoch (; Hebrew: חֲנוֹך‎‎ Ḥanōḵ) was a son of Cain, grandson of Adam, and father of Irad. After Cain arrived in the Land of Nod, to which he was evicted by the Lord as his punishment for murdering his brother Abel, his wife became pregnant and bore Cain's first child, whom he named Enoch.
After the birth of Enoch, the Hebrew text of Genesis 4:17 is unclear. Either Cain built a city and named it after Enoch, or else Enoch built a city.
According to Jubilees 4:9, Enoch's mother/aunt was named Awan.
This Enoch is not to be confused with Noah's ancestor Enoch.War

War is a state of armed conflict between societies. It is generally characterized by extreme aggression, destruction, and mortality, using regular or irregular military forces. An absence of war is usually called "peace". Warfare refers to the common activities and characteristics of types of war, or of wars in general. Total war is warfare that is not restricted to purely legitimate military targets, and can result in massive civilian or other non-combatant suffering and casualties.
While some scholars see war as a universal and ancestral aspect of human nature, others argue it is a result of specific socio-cultural or ecological circumstances.
The deadliest war in history, in terms of the cumulative number of deaths since its start, is the Second World War, from 1939 to 1945, with 60–85 million deaths, followed by the Mongol conquests at up to 60 million. As concerns a belligerent's losses in proportion to its prewar population, the most destructive war in modern history may have been the Paraguayan War (see Paraguayan War casualties). In 2013 war resulted in 31,000 deaths, down from 72,000 deaths in 1990. In 2003, Richard Smalley identified war as the sixth (of ten) biggest problem facing humanity for the next fifty years. War usually results in significant deterioration of infrastructure and the ecosystem, a decrease in social spending, famine, large-scale emigration from the war zone, and often the mistreatment of prisoners of war or civilians. For instance, of the nine million people who were on the territory of Soviet Belarus in 1941, some 1.6 million were killed by the Germans in actions away from battlefields, including about 700,000 prisoners of war, 500,000 Jews, and 320,000 people counted as partisans (the vast majority of whom were unarmed civilians). Another byproduct of some wars is the prevalence of propaganda by some or all parties in the conflict, and increased revenues by weapons manufacturers.Natural Language Toolkit

The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a cookbook.
NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning. NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.Natural language processing

Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, dialog systems, or some combination thereof.Outline of natural language processing

Natural language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language. This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on. Natural language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing.Computer language

Computer language may refer to:
Programming language, a formal language designed to communicate instructions to a machine, particularly a computer
Command language, a language used to control the tasks of the computer itself, such as starting other programs
Machine language or machine code, a set of instructions executed directly by a computer's central processing unit
Markup language, a grammar for annotating a document in a way that is syntactically distinguishable from the text, such as HTML
Style sheet language, a computer language that expresses the presentation of structured documents, such as CSS
Configuration language, a language used to write configuration files
Construction language, a general category that includes configuration languages, toolkit languages, and programming languages
Query language, a language used to make queries in databases and information systems
Modeling language, a formal language used to express information or knowledge, often for use in computer system design
Hardware description language, used to model integrated circuitsList of toolkits

A toolkit is an assembly of tools; set of basic building units for user interfaces.
The word toolkit may refer to:
Abstract Window Toolkit
Accessibility Toolkit
Adventure Game Toolkit
B-Toolkit
Battlefield Mod Development Toolkit
Cheminformatics toolkits
Dojo Toolkit
Fox toolkit
Globus Toolkit
GTK+, the GIMP Toolkit
Google Web Toolkit (GWT)
Harmony (toolkit), an incomplete set of software widgets
Helsinki Finite-State Technology (HFST)
Insight Segmentation and Registration Toolkit
IT Mill Toolkit
Luxor (toolkit)
Learnosity Toolkit
Molecular Modelling Toolkit
Multidimensional hierarchical toolkit
Sun Java Wireless Toolkit
OCR SDK, OCR Toolkit
OpenGL Utility Toolkit
Open Inventor 3D graphics API
Qt (toolkit)
Motif (software)
Natural Language Toolkit
Portable, Extensible Toolkit for Scientific Computation
RWTH FSA Toolkit
Rialto Toolkit
Scedu Tender Readiness Toolkit
Sprite Animation Toolkit
Standard Widget Toolkit
Synthesis Toolkit
Template Toolkit
The Coroner's Toolkit, computer programs for digital forensic analysis
User Interface Toolkit (UIM)
X ToolkitDialog system

A dialog system or conversational agent (CA) is a computer system intended to converse with a human, with a coherent structure. Dialog systems have employed text, speech, graphics, haptics, gestures and other modes for communication on both the input and output channel.
What does and does not constitute a dialog system may be debatable. The typical GUI wizard does engage in some sort of dialog, but it includes very few of the common dialog system components, and dialog state is trivial.OpenNLP

The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, language detection and coreference resolution. These tasks are usually required to build more advanced text processing services.Object Constraint Language

The Object Constraint Language (OCL) is a declarative language describing rules applying to Unified Modeling Language (UML) models developed at IBM and is now part of the UML standard. Initially, OCL was merely a formal specification language extension for UML. OCL may now be used with any Meta-Object Facility (MOF) Object Management Group (OMG) meta-model, including UML. The Object Constraint Language is a precise text language that provides constraint and object query expressions on any MOF model or meta-model that cannot otherwise be expressed by diagrammatic notation. OCL is a key component of the new OMG standard recommendation for transforming models, the Queries/Views/Transformations (QVT) specification.Translate Toolkit

The Translate Toolkit is a localization and translation toolkit. It provides a set of tools for working with localization file formats and files that might need localization. The toolkit also provides an API on which to develop other localization tools.
The toolkit is written in the Python programming language. It is free software originally developed and released by Translate.org.za in 2002 and is now maintained by Translate.org.za and community developers.
Translate Toolkit uses Enchant as spellchecker.RWTH FSA Toolkit

The RWTH FSA Toolkit is a highly efficient C++ library that handles finite state machines; in particular it deals with weighted and unweighted automata and transducers. It has been designed to be used in a variety of natural language processing applications and was tested with speech recognition and machine translation. It is available under an open-source license.
The name is taken from the developing University RWTH Aachen.Federal Reserve System

The Federal Reserve System (also known as the Federal Reserve or simply the Fed) is the central banking system of the United States. It was created on December 23, 1913, with the enactment of the Federal Reserve Act, after a series of financial panics (particularly the panic of 1907) led to the desire for central control of the monetary system in order to alleviate financial crises. Over the years, events such as the Great Depression in the 1930s and the Great Recession during the 2000s, have led to the expansion of the roles and responsibilities of the Federal Reserve System.
The U.S. Congress established three key objectives for monetary policy in the Federal Reserve Act: maximizing employment, stabilizing prices, and moderating long-term interest rates. The first two objectives are sometimes referred to as the Federal Reserve's dual mandate. Its duties have expanded over the years, and as of 2009 also include supervising and regulating banks, maintaining the stability of the financial system and providing financial services to depository institutions, the U.S. government, and foreign official institutions. The Fed conducts research into the economy and provides numerous publications, such as the Beige Book and the FRED database.
The Federal Reserve System is composed of several layers. It is governed by the presidentially appointed Board of Governors or Federal Reserve Board (FRB). Twelve regional Federal Reserve Banks, located in cities throughout the nation, oversee the privately owned U.S. member banks. Nationally chartered commercial banks are required to hold stock in the Federal Reserve Bank of their region, which entitles them to elect some of their board members. The Federal Open Market Committee (FOMC) sets monetary policy; it consists of all seven members of the Board of Governors and the twelve regional bank presidents, though only five bank presidents vote at any given time: the president of the New York Fed and four others who rotate through one-year terms. There are also various advisory councils. Thus, the Federal Reserve System has both public and private components. The structure is considered unique among central banks. It is also unusual in that the United States Department of the Treasury, an entity outside of the central bank, prints the currency used.
Although an instrument of the U.S. Government, the Federal Reserve System considers itself "an independent central bank because its monetary policy decisions do not have to be approved by the President or anyone else in the executive or legislative branches of government, it does not receive funding appropriated by the Congress, and the terms of the members of the Board of Governors span multiple presidential and congressional terms." The federal government sets the salaries of the board's seven governors. The federal government receives all the system's annual profits, after a statutory dividend of 6% on member banks' capital investment is paid, and an account surplus is maintained. In 2015, the Federal Reserve made a profit of $100.2 billion and transferred $97.7 billion to the U.S. Treasury.History of the Federal Reserve System

This article is about the history of the United States Federal Reserve System from its creation to the present.Federal Reserve Board of Governors

The Board of Governors of the Federal Reserve System, commonly known as the Federal Reserve Board, is the main governing body of the Federal Reserve System. It is charged with overseeing the Federal Reserve Banks and with helping implement monetary policy of the United States. Governors are appointed by the President of the United States and confirmed by the Senate for staggered 14-year terms.Federal Reserve Bank

A Federal Reserve Bank is a regional bank of the Federal Reserve System, the central banking system of the United States. There are twelve in total, one for each of the twelve Federal Reserve Districts that were created by the Federal Reserve Act of 1913. The banks are jointly responsible for implementing the monetary policy set forth by the Federal Open Market Committee, and are divided as follows:

Some banks also possess branches, with the whole system being headquartered at the Eccles Building in Washington, D.C.Federal Reserve Act

The Federal Reserve Act (ch. 6, 38 Stat. 251, enacted December 23, 1913, 12 U.S.C. ch. 3) is an Act of Congress that created and established the Federal Reserve System, the central banking system of the United States, and which created the authority to issue Federal Reserve Notes (now commonly known as the U.S. Dollar) and Federal Reserve Bank Notes as legal tender. The Act was signed into law by President Woodrow Wilson.Chair of the Federal Reserve

The Chair of the Board of Governors of the Federal Reserve System is the head of the central banking system of the United States. The position is known colloquially as "Chair of the Fed" or "Fed Chair". The chair is the "active executive officer" of the Board of Governors of the Federal Reserve System.
The chair is chosen by the President of the United States from among the members of the Board of Governors; and serves for four-year-terms after appointment. A chair may be appointed for several consecutive terms. William Martin was the longest serving chair, holding the position from 1951 to 1970. The current chair is Janet Yellen, the first woman to hold the position. She began her term on February 1, 2014, and previously served as the Vice-Chair from 2010 to 2014. The current term will end on or about February 1, 2018.Structure of the Federal Reserve System

The Federal Reserve System is composed of five parts:
The presidentially appointed Board of Governors (or Federal Reserve Board), an independent federal government agency located in Washington, D.C.
The Federal Open Market Committee (FOMC), composed of the seven members of the Federal Reserve Board and five of the twelve Federal Reserve Bank presidents, which oversees open market operations, the principal tool of U.S. monetary policy.
Twelve regional Federal Reserve Banks located in major cities throughout the nation, which divide the nation into twelve Federal Reserve districts. The Federal Reserve Banks act as fiscal agents for the U.S. Treasury, and each has its own nine-member board of directors.
Numerous other private U.S. member banks, which own required amounts of non-transferable stock in their regional Federal Reserve Banks.
Various advisory councils.
According to the board of governors of the Federal Reserve, "It is not 'owned' by anyone and is 'not a private, profit-making institution'. Instead, it is an independent entity within the government, having both public purposes and private aspects." The U.S. Government does not own shares in the Federal Reserve System or its component banks, but does receive all of the system's annual profits after a statutory dividend of 6% on their capital investment is paid to member banks and a capital account surplus is maintained. The government also exercises some control over the Federal Reserve by appointing and setting the salaries of the system's highest-level employees.
The division of the responsibilities of a central bank into several separate and independent parts, some private and some public, results in a structure that is considered unique among central banks. It is also unusual in that an entity outside of the central bank – the U.S. Department of the Treasury – creates the currency used.Federal Reserve Bank of New York

The Federal Reserve Bank of New York is one of the 12 Federal Reserve Banks of the United States. It is located at 33 Liberty Street, New York, NY. It is responsible for the Second District of the Federal Reserve System, which encompasses New York State, the 12 northern counties of New Jersey, Fairfield County in Connecticut, Puerto Rico, and the U.S. Virgin Islands. Working within the Federal Reserve System, the New York Federal Reserve Bank implements monetary policy, supervises and regulates financial institutions and helps maintain the nation's payment systems.
Among the other regional banks, New York Federal Reserve Bank and its president are considered first among equals. Its current president is William C. Dudley. It is by far the largest (by assets), most active (by volume) and most influential of the 12 regional Federal Reserve Banks.Federal Reserve Note

Federal Reserve Notes, also United States banknotes or U.S. banknotes, are the banknotes currently used in the United States of America. Denominated in United States dollars, Federal Reserve Notes are printed by the United States Bureau of Engraving and Printing on paper made by Crane & Co. of Dalton, Massachusetts. Federal Reserve Notes are the only type of U.S. banknote currently produced. Federal Reserve Notes are authorized by Section 16 of the Federal Reserve Act of 1913 and are issued to the Federal Reserve Banks at the discretion of the Board of Governors of the Federal Reserve System. The notes are then put into circulation by the Federal Reserve Banks, at which point they become liabilities of the Federal Reserve Banks and obligations of the United States.
Federal Reserve Notes are legal tender, with the words "this note is legal tender for all debts, public and private" printed on each note. They have replaced United States Notes, which were once issued by the Treasury Department. Federal Reserve Notes are backed by the assets of the Federal Reserve Banks, which serve as collateral under Section 16. These assets are generally Treasury securities which have been purchased by the Federal Reserve through its Federal Open Market Committee in a process called debt monetizing. This monetized debt can increase the money supply, either with the issuance of new Federal Reserve Notes or with the creation of debt money (deposits). This increase in the monetary base leads to a larger increase in the money supply through fractional-reserve banking as deposits are lent and re-deposited where they form the basis of further loans.Federal Reserve Police

The U.S. Federal Reserve Police is the law enforcement arm of the Federal Reserve System, the central banking system of the United States. The Federal Reserve Board Police in Washington, D.C., is not part of the same entity as Federal Reserve System Law Enforcement Units located in the 12 districts (covering all 50 states) throughout the nation.Java virtual machine

A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program. There are three notions of the JVM: specification, implementation, and instance. The specification is a document that formally describes what is required of a JVM implementation. Having a single specification ensures all implementations are interoperable. A JVM implementation is a computer program that meets the requirements of the JVM specification. An instance of a JVM is an implementation running in a process that executes a computer program compiled into Java bytecode.
Java Runtime Environment (JRE) is a software package that contains what is required to run a Java program. It includes a Java Virtual Machine implementation together with an implementation of the Java Class Library. The Oracle Corporation, which owns the Java trademark, distributes a Java Runtime environment with their Java Virtual Machine called HotSpot.
Java Development Kit (JDK) is a superset of a JRE and contains tools for Java programmers, e.g. a javac compiler. The Java Development Kit is provided free of charge either by Oracle Corporation directly, or by the OpenJDK open source project, which is governed by Oracle.List of Java virtual machines

This article provides non-exhaustive lists of Java SE Java virtual machines (JVMs). It does not include a large number of Java ME vendors. Note that Java EE runs on the standard Java SE JVM but that some vendors specialize in providing a modified JVM optimized for Java EE applications. A large amount of Java development work takes place on Windows, Solaris, Linux and FreeBSD, primarily with the Oracle JVMs. Note the further complication of different 32-bit/64-bit varieties.
The primary reference Java VM implementation is HotSpot, produced by Oracle Corporation.Microsoft Java Virtual Machine

The Microsoft Java Virtual Machine (MSJVM) is a discontinued proprietary Java virtual machine from Microsoft. It was first made available for Internet Explorer 3 so that users could run Java applets when browsing on the World Wide Web. It was the fastest Windows-based implementation of a Java virtual machine for the first two years after its release. Sun Microsystems, the creator of Java, sued Microsoft in October 1997 for incompletely implementing the Java 1.1 standard. It was also named in the United States v. Microsoft Corp antitrust civil actions, as an implementation of Microsoft's "Embrace, extend and extinguish" strategy. In 2001, Microsoft settled the lawsuit with Sun and discontinued its Java implementation.Java Virtual Machine Tools Interface

Java Virtual Machine Tool Interface (JVMTI, or more properly, JVM TI) was introduced in J2SE 5.0 (Tiger). This interface allows a program to inspect the state and to control the execution of applications running in the Java Virtual Machine (JVM). JVMTI is designed to provide an Application Programming Interface (API) for the development of tools that need access to the state of the JVM. Examples for such tools are debuggers or profilers.
The JVMTI is a native interface of the JVM. A library, written in C or C++, is loaded during the initialization of the JVM. The library has access to the JVM state by calling JVMTI and JNI (Java Native Interface) functions and can register to receive JVMTI events using event handler functions that are called by the JVM when such an event occurs.
JVMTI was defined through the Java Community Process by JSR-163, the specification for the Java Platform Profiling Architecture. The JVMTI replaces the JVMPI (Java Virtual Machine Profiling Interface) and the JVMDI (Java Virtual Machine Debug Interface). The JVMPI and the JVMDI are declared as being deprecated in J2SE 5.0 and were removed in Java SE6.
JVMTI is the lowest level of the Java Platform Debugger Architecture.Virtual machine

In computing, a virtual machine (VM) is an emulation of a computer system. Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.
There are different kinds of virtual machines, each with different functions:
System virtual machines (also termed full virtualization VMs) provide a substitute for a real machine. They provide functionality needed to execute entire operating systems. A hypervisor uses native execution to share and manage hardware, allowing for multiple environments which are isolated from one another, yet exist on the same physical machine. Modern hypervisors use hardware-assisted virtualization, virtualization-specific hardware, primarily from the host CPUs.
Process virtual machines are designed to execute computer programs in a platform-independent environment.
Some virtual machines, such as QEMU, are designed to also emulate different architectures and allow execution of software applications and operating systems written for another CPU or architecture. Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel's support for multiple isolated user space instances, which are usually called containers and may look and feel like real machines to the end users.Comparison of Java virtual machinesJava bytecode

Java bytecode is the instruction set of the Java virtual machine (JVM).HotSpot

HotSpot, released as Java HotSpot Performance Engine, is a Java virtual machine for desktop and server computers, maintained and distributed by Oracle Corporation. It features improved performance via methods such as just-in-time compilation and adaptive optimization.Classpath (Java)

Classpath is a parameter in the Java Virtual Machine or the Java compiler that specifies the location of user-defined classes and packages. The parameter may be set either on the command-line, or through an environment variable.Dalvik (software)

Dalvik is a discontinued process virtual machine (VM) in Google's Android operating system that executes applications written for Android. Dalvik is an integral part of the Android software stack in Android versions 4.4 "KitKat" and earlier, which is typically used on mobile devices such as mobile phones and tablet computers, and more recently on devices such as smart TVs and wearables. Dalvik is open-source software, originally written by Dan Bornstein, who named it after the fishing village of Dalvík in Eyjafjörður, Iceland.
Programs for Android are commonly written in Java and compiled to bytecode for the Java virtual machine, which is then translated to Dalvik bytecode and stored in .dex (Dalvik EXecutable) and .odex (Optimized Dalvik EXecutable) files; related terms odex and de-odex are associated with respective bytecode conversions. The compact Dalvik Executable format is designed for systems that are constrained in terms of memory and processor speed.
The successor of Dalvik is Android Runtime (ART), which uses the same bytecode and .dex files (but not .odex files), with the succession aiming at performance improvements transparent to the end users. The new runtime environment was included for the first time in Android 4.4 "KitKat" as a technology preview, and replaced Dalvik entirely in later versions; Android 5.0 "Lollipop" is the first version in which ART is the only included runtime.United States

The United States of America (USA), commonly known as the United States (U.S.) or America (), is a federal republic composed of 50 states, a federal district, five major self-governing territories, and various possessions. Forty-eight states and the federal district are contiguous and located in North America between Canada and Mexico. The state of Alaska is in the northwest corner of North America, bordered by Canada to the east and across the Bering Strait from Russia to the west. The state of Hawaii is an archipelago in the mid-Pacific Ocean. The U.S. territories are scattered about the Pacific Ocean and the Caribbean Sea, stretching across nine official time zones. The extremely diverse geography, climate and wildlife of the United States make it one of the world's 17 megadiverse countries.
At 3.8 million square miles (9.8 million km2) and with over 324 million people, the United States is the world's third- or fourth-largest country by total area and the third-most populous. The capital is Washington, D.C., and the largest city is New York City; twelve other major metropolitan areas—each with at least 4.5 million inhabitants—are Los Angeles, Chicago, Dallas, Houston, Washington, D.C., Philadelphia, Miami, Atlanta, Boston, San Francisco, Phoenix, and Riverside–San Bernardino.
Paleo-Indians migrated from Asia to the North American mainland at least 15,000 years ago. European colonization began in the 16th century. The United States emerged from 13 British colonies along the East Coast. Numerous disputes between Great Britain and the colonies following the Seven Years' War led to the American Revolution, which began in 1775, and the subsequent Declaration of Independence in 1776. The war ended in 1783 with the United States becoming the first country to gain independence from a European power. The current constitution was adopted in 1788, with the first ten amendments, collectively named the Bill of Rights, being ratified in 1791 to guarantee many fundamental civil liberties. The United States embarked on a vigorous expansion across North America throughout the 19th century, acquiring new territories, displacing Native American tribes, and gradually admitting new states until it spanned the continent by 1848. During the second half of the 19th century, the American Civil War led to the end of legal slavery in the country. By the end of that century, the United States extended into the Pacific Ocean, and its economy, driven in large part by the Industrial Revolution, began to soar. The Spanish–American War and World War I confirmed the country's status as a global military power. The United States emerged from World War II as a global superpower, the first country to develop nuclear weapons, the only country to use them in warfare, and a permanent member of the United Nations Security Council. The end of the Cold War and the dissolution of the Soviet Union in 1991 left the United States as the world's sole superpower.
The U.S. is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), and other international organizations. The United States is a highly developed country, with the world's largest economy by nominal GDP and second-largest economy by PPP, accounting for approximately a quarter of global GDP. The U.S. economy is the fastest-growing in the Americas and is largely post-industrial, characterized by the dominance of services and knowledge-based activities, although the manufacturing sector remains the second-largest in the world. Though its population is only 4.3% of the world total, Americans hold 33.2% of the total wealth in the world, the largest share of global wealth concentrated in a single country. The United States ranks among the highest nations in several measures of socioeconomic performance, including average wage, human development, per capita GDP, and productivity per person. The U.S. is the foremost military power in the world, making up a third of global military spending. It is also a global leader in science and technology.List of Metropolitan Statistical Areas

The United States Office of Management and Budget (OMB) has defined 388 Metropolitan Statistical Areas (MSAs) for the United States and seven for Puerto Rico. The OMB defines a Metropolitan Statistical Area as one or more adjacent counties or county equivalents that have at least one urban core area of at least 50,000 population, plus adjacent territory that has a high degree of social and economic integration with the core as measured by commuting ties.President of the United States

The President of the United States (informally referred to as "POTUS") is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.
The president is considered to be the world's most powerful political figure, as the leader of the only contemporary global superpower. The role includes being the commander-in-chief of the world's most expensive military with the second largest nuclear arsenal and leading the nation with the largest economy by nominal GDP. The office of President holds significant hard and soft power both domestically and abroad.
In ordering the three branches of government, Article II of the Constitution vests the executive power of the United States in the president. The power includes the execution and enforcement of federal law, alongside the responsibility of appointing federal executive, diplomatic, regulatory and judicial officers, and concluding treaties with foreign powers with the advice and consent of the Senate. The president is further empowered to grant federal pardons and reprieves, and to convene and adjourn either or both houses of Congress under extraordinary circumstances. The president is largely responsible for dictating the legislative agenda of the party to which the president is a member. The president also directs the foreign and domestic policy of the United States. In addition, Article One of the United States Constitution assigns to the president the power to sign legislation into law or to veto it. Since the office of President was established in 1789, its power has grown substantially, as has the power of the federal government as a whole.
The president is indirectly elected by the people through the Electoral College to a four-year term, and is one of only two nationally elected federal officers, the other being the Vice President of the United States. Nine vice presidents became president by virtue of a president's intra-term death or resignation.
The Twenty-second Amendment prohibits anyone from being elected president for a third term. It also prohibits a person from being elected to the presidency more than once if that person previously had served as president, or acting president, for more than two years of another person's term as president. In all, 44 individuals have served 45 presidencies (counting Grover Cleveland's two non-consecutive terms separately) spanning 57 full four-year terms. On January 20, 2017, Donald Trump was sworn in as the 45th and current president.U.S. state

A U.S. state is a constituent political entity of the United States of America. There are currently 50 states, which are bound together in a union with each other. Each state holds governmental jurisdiction over a defined geographic territory, and shares its sovereignty with the United States federal government. Due to the shared sovereignty between each state and the federal government, Americans are citizens of both the federal republic and of the state in which they reside. State citizenship and residency are flexible, and no government approval is required to move between states, except for persons covered by certain types of court orders (e.g., paroled convicts and children of divorced spouses who are sharing custody).
States range in population from just under 600,000 (Wyoming) to over 39 million (California), and in area from 1,214 square miles (3,140 km2) (Rhode Island) to 663,268 square miles (1,717,860 km2) (Alaska). Four states use the term commonwealth rather than state in their full official names.
States are divided into counties or county-equivalents, which may be assigned some local governmental authority but are not sovereign. County or county-equivalent structure varies widely by state. State governments are allocated power by the people (of each respective state) through their individual constitutions. All are grounded in republican principles, and each provides for a government, consisting of three branches: executive, legislative, and judicial.
States possess a number of powers and rights under the United States Constitution. States and their residents are represented in the United States Congress, a bicameral legislature consisting of the Senate and the House of Representatives. Each state is also entitled to select a number of electors (equal to the total number of representatives and senators from that state) to vote in the Electoral College, the body that directly elects the President of the United States. Additionally, each state has the opportunity to ratify constitutional amendments, and, with the consent of Congress, two or more states may enter into interstate compacts with one another.
Historically, the tasks of local law enforcement, public education, public health, regulating intrastate commerce, and local transportation and infrastructure have generally been considered primarily state responsibilities, although all of these now have significant federal funding and regulation as well. Over time, the Constitution has been amended, and the interpretation and application of its provisions have changed. The general tendency has been toward centralization and incorporation, with the federal government playing a much larger role than it once did. There is a continuing debate over states' rights, which concerns the extent and nature of the states' powers and sovereignty in relation to the federal government and the rights of individuals.
The Constitution grants to Congress the authority to admit new states into the Union. Since the establishment of the United States in 1776, the number of states has expanded from the original 13 to 50. Alaska and Hawaii are the most recent states admitted, both in 1959. The Constitution is silent on the question of whether states have the power to secede (withdraw) from the Union. Shortly after the Civil War, the U.S. Supreme Court, in Texas v. White, held that a state cannot unilaterally do so.Terrorism in the United States

A common definition of terrorism is the systematic or threatened use of violence in order to intimidate a population or government and thereby affect political, religious, or ideological change. This article serves as a list and compilation of acts of terrorism, attempts of terrorism, and other such items pertaining to terrorist activities within the domestic borders of the United States by non-state actors or spies acting in the interests of or persons acting without approval of state actors.Mass shooting

A mass shooting is an incident involving multiple victims of firearms-related violence. The United States' Congressional Research Service acknowledges that there is not a broadly accepted definition, and defines a "public mass shooting" as one in which four or more people selected indiscriminately, not including the perpetrator, are killed, echoing the FBI definition of the term "mass murder". Another unofficial definition of a mass shooting is an event involving the shooting (not necessarily resulting in death) of four or more people with no cooling-off period. Related terms include school shooting and massacre.
A mass shooting may be committed by individuals or organizations in public or non-public places. Terrorist groups in recent times have used the tactic of mass shootings to fulfill their political aims. Individuals who commit mass shootings may fall into any of a number of categories, including killers of family, of coworkers, of students, and of random strangers. Individuals' motives for shooting vary.
Responses to mass shootings take a variety of forms, depending on the context: number of casualties, the country and political climate, among other factors. The news media and other types of media cover mass shootings extensively, and, often, sensationally, and the effect of that coverage has been examined. Countries such as Australia and the United Kingdom have changed their gun laws in the wake of mass shootings. In contrast, the United States' constitution currently prohibits laws which disallow firearm ownership outright.List of U.S. states by income

This is a list of U.S. states by income.Race and ethnicity in the United States

The United States has a racially and ethnically diverse population. The United States Census officially recognizes six racial categories: White American, Black or African American, Native American and Alaska Native, Asian American, Native Hawaiian and Other Pacific Islander, and people of two or more races; a category called "some other race" is also used in the census and other surveys, but is not official. The United States Census Bureau also classifies Americans as "Hispanic or Latino" and "Not Hispanic or Latino", which identifies Hispanic and Latino Americans as an ethnicity (not a race) distinct from others that composes the largest minority group in the nation.
The United States Supreme Court unanimously held that "race" is not limited to Census designations on the "race question" but extends to all ethnicities, and thus can include Jewish and Arab as well as Polish or Italian or Irish, etc. In fact, the Census asks an "Ancestry Question" which covers the broader notion of ethnicity initially in the 2000 Census long form and now in the American Community Survey.
As of July 2016, white Americans are the racial majority. African Americans are the largest racial minority, amounting to 13.3% of the population. Hispanic and Latino Americans amount to 17.8% of the total U.S. population, making up the largest ethnic minority. The White, non-Hispanic or Latino population make up 61.3% of the nation's total, with the total White population (including White Hispanics and Latinos) being 76.9%.
White Americans are the majority in every region except Hawaii, but contribute the highest proportion of the population in the Midwestern United States, at 85% per the Population Estimates Program (PEP), or 83% per the American Community Survey (ACS). Non-Hispanic Whites make up 79% of the Midwest's population, the highest ratio of any region. However, 35% of White Americans (whether all White Americans or non-Hispanic/Latino only) live in the South, the most of any region.
55% of the African American population lives in the South. A plurality or majority of the other official groups reside in the West. This region is home to 42% of Hispanic and Latino Americans, 46% of Asian Americans, 48% of American Indians and Alaska Natives, 68% of Native Hawaiians and Other Pacific Islanders, 37% of the "two or more races" population (Multiracial Americans), and 46% of those designated "some other race".Territories of the United States

Territories of the United States are sub-national administrative divisions directly overseen by the United States (U.S.) federal government. Unlike U.S. states and Indian tribes that have sovereignty alongside the federal government, only one territory has sovereignty. The territories are classified by whether they are "incorporated" (i.e., part of the U.S. proper) and whether they have an "organized" government through an Organic Act passed by the U.S. Congress.
The U.S. has sixteen territories in the Caribbean Sea, the south Pacific Ocean, and the western portion of the north Pacific Ocean. Five of them are permanently inhabited and are classified as unincorporated territories. The other eleven are small islands, atolls, and reefs with no native or permanent population. Of those eleven, only one is classified as an incorporated territory.
Historically, territories were created to govern newly acquired land while the borders of the U.S. were still evolving. Most territories eventually attained statehood. Other territories administered by the U.S. eventually became independent countries, such as the Philippines, Micronesia, the Marshall Islands, and Palau. Micronesia, the Marshall Islands, and Palau gained independence under the Compact of Free Association (COFA), which gives the U.S. full authority over aid and defense in exchange for continuing access to U.S. health care, government services such as the Federal Communications Commission and the U.S. Postal Service, and the right for COFA citizens to work freely in the U.S. and vice versa.
Many organized incorporated territories of the United States existed from 1789 to 1959. The first were the Northwest and the Southwest territories and the last were the Alaska Territory and the Hawaii Territory. Thirty-one of these territories applied for and were granted statehood. In the process of organizing and promoting territories to statehood, some areas of a territory lacking sufficient development and population densities were temporarily orphaned from parts of a larger territory when residents voted on whether to petition Congress for statehood. For example, when a portion of the Missouri Territory became the state of Missouri, the remaining portion of the territory, consisting of all the states of Iowa, Nebraska, South Dakota, and North Dakota, most of Kansas, Wyoming, and Montana, and parts of Colorado and Minnesota, effectively became an unorganized territory.United States Congress

The United States Congress is the bicameral legislature of the federal government of the United States consisting of two chambers: the Senate and the House of Representatives.
The Congress meets in the Capitol in Washington, D.C. Both senators and representatives are chosen through direct election, though vacancies in the Senate may be filled by a gubernatorial appointment. Congress has 535 voting members: 435 Representatives and 100 Senators. The House of Representatives has six non-voting members in addition to its 435 voting members. These members can, however, sit on congressional committees and introduce legislation. These members represent Puerto Rico, American Samoa, Guam, the Northern Mariana Islands, the U.S. Virgin Islands, and Washington, D.C.
The members of the House of Representatives serve two-year terms representing the people of a single constituency, known as a "district". Congressional districts are apportioned to states by population using the United States Census results, provided that each state has at least one congressional representative. Each state, regardless of population or size, has two senators. Currently, there are 100 senators representing the 50 states. Each senator is elected at-large in their state for a six-year term, with terms staggered, so every two years approximately one-third of the Senate is up for election.
To be eligible for election, a candidate must be aged at least 25 (House) or 30 (Senate), have been a citizen of the United States for seven (House) or nine (Senate) years, and be an inhabitant of the state of which they represent.
The Congress was created by the Constitution of the United States and first met in 1789, replacing in its legislative function the Congress of the Confederation. Although not constitutionally mandated, in practice since the 19th century, Congress members are typically affiliated to the Republican Party or to the Democratic Party and only rarely to a third party or as independents.Origin of water on Earth

The origin of water on Earth, or the reason that there is clearly more liquid water on Earth than on the other rocky planets of the Solar System, is not completely understood. There exist numerous more or less mutually compatible hypotheses as to how water may have accumulated on Earth's surface over the past 4.5 billion years in sufficient quantity to form oceans.History of Earth

The history of Earth concerns the development of planet Earth from its formation to the present day. Nearly all branches of natural science have contributed to the understanding of the main events of Earth's past. The age of the Earth is approximately one-third of the age of the universe. An immense amount of geological change has occurred in that timespan, accompanied by the emergence of life and its subsequent evolution.
Earth formed around 4.54 billion years ago by accretion from the solar nebula. Volcanic outgassing probably created the primordial atmosphere and then the ocean, but the early atmosphere contained almost no oxygen and so would not have supported known forms of life. Much of the Earth was molten because of frequent collisions with other bodies which led to extreme volcanism. A giant impact collision with a planet-sized body named Theia while Earth was in its earliest stage, also known as Early Earth, is thought to have been responsible for forming the Moon. Over time, the Earth cooled, causing the formation of a solid crust, and allowing liquid water to exist on the surface.
The geological time scale (GTS) depicts the larger spans of time, from the beginning of the Earth to the present, and it chronicles some definitive events of Earth history. The Hadean eon represents time before the reliable (fossil) record of life beginning on Earth; it began with the formation of the planet and ended at 4.0 billion years ago as defined by international convention. The Archean and Proterozoic eons follow; they produced the abiogenesis of life on Earth and then the evolution of early life. The succeeding eon is the Phanerozoic, which is represented by its three component eras: the Palaeozoic; the Mesozoic, which spanned the rise, reign, and climactic extinction of the non-avian dinosaurs; and the Cenozoic, which presented the subsequent development of dominant mammals on Earth.
Hominins, the earliest direct ancestors of the human clade, rose sometime during the latter part of the Miocene epoch; the precise time marking the first hominins is broadly debated over a current range of 13 to 4 million years ago. The succeeding Quaternary period is the time of recognizable humans, i.e., the genus Homo, but that period's two million-year-plus term of the recent times is too small to be visible at the scale of the GTS graphic. (Notes re the graphic: Ga means "billion years"; Ma, "million years".)
The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils such as stromatolites found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in southwestern Greenland as well as "remains of biotic life" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth … then it could be common in the universe."
Photosynthetic organisms appeared between 3.2 and 2.4 billion years ago and began enriching the atmosphere with oxygen. Life remained mostly small and microscopic until about 580 million years ago, when complex multicellular life arose, developed over time, and culminated in the Cambrian Explosion about 541 million years ago. This event drove a rapid diversification of life forms on Earth that produced most of the major phyla known today, and it marked the end of the Proterozoic Eon and the beginning of the Cambrian Period of the Paleozoic Era. More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million are documented, but over 86 percent have not been described. Scientists recently reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.
The Earth's crust has constantly changed since its formation. Likewise, life has constantly changed since its first appearance. Species continue to evolve, taking on new forms, splitting into daughter species or going extinct in the process of adapting or dying in response to ever-changing physical environments. The process of plate tectonics continues to shape the Earth's continents and oceans and the life they harbor. Human activity is now a dominant force affecting global change, adversely affecting the biosphere, the Earth's surface, hydrosphere, and atmosphere, with the loss of wild lands, over-exploitation of the oceans, production of greenhouse gases, degradation of the ozone layer, and general degradation of soil, air, and water quality.Water distribution on Earth

The water distribution on earth shows that most water in the Earth's atmosphere and crust comes from the world ocean's saline seawater, while freshwater accounts for only 2.5% of the total. Because the oceans that cover roughly 70% of the area of the Earth reflect blue light, the Earth appears blue from space, and is often referred to as the blue planet and the Pale Blue Dot. An estimated 1.5 to 11 times the amount of water in the oceans may be found hundreds of miles deep within the Earth's interior, although not in liquid form.
The oceanic crust is young, thin and dense, with none of the rocks within it dating from any older than the breakup of Pangaea. Because water is much denser than any gas, this means that water will flow into the "depressions" formed as a result of the high density of oceanic crust. (On a planet like Venus, with no water, the depressions appear to form a vast plain above which rise plateaux). Since the low density rocks of the continental crust contain large quantities of easily eroded salts of the alkali and alkaline earth metals, salt has, over billions of years, accumulated in the oceans as a result of evaporation returning the fresh water to land as rain and snow.
As a result, the vast bulk of the water on Earth is regarded as saline or salt water, with an average salinity of 35‰ (or 3.5%, roughly equivalent to 34 grams of salts in 1 kg of seawater), though this varies slightly according to the amount of runoff received from surrounding land. In all, water from oceans and marginal seas, saline groundwater and water from saline closed lakes amount to over 97% of the water on Earth, though no closed lake stores a globally significant amount of water. Saline groundwater is seldom considered except when evaluating water quality in arid regions.
The remainder of the Earth's water constitutes the planet's fresh water resource. Typically, fresh water is defined as water with a salinity of less than 1 percent that of the oceans - i.e. below around 0.35‰. Water with a salinity between this level and 1‰ is typically referred to as marginal water because it is marginal for many uses by humans and animals. The ratio of salt water to fresh water on Earth is around 40 to 1.
The planet's fresh water is also very unevenly distributed. Although in warm periods such as the Mesozoic and Paleogene when there were no glaciers anywhere on the planet all fresh water was found in rivers and streams, today most fresh water exists in the form of ice, snow, groundwater and soil moisture, with only 0.3% in liquid form on the surface. Of the liquid surface fresh water, 87% is contained in lakes, 11% in swamps, and only 2% in rivers. Small quantities of water also exist in the atmosphere and in living beings. Of these sources, only river water is generally valuable.
Most lakes are in very inhospitable regions such as the glacial lakes of Canada, Lake Baikal in Russia, Lake Khövsgöl in Mongolia, and the African Great Lakes. The North American Great Lakes, which contain 21% of the world's fresh water by volume, are the exception. They are located in a hospitable region, which is heavily populated. The Great Lakes Basin is home to 33 million people. The Canadian cities of Toronto, Hamilton, Ontario, St. Catharines, Niagara, Oshawa, Windsor, and Barrie, and the United States cities of Duluth, Milwaukee, Chicago, Gary, Detroit, Cleveland, Buffalo, and Rochester, are all located on shores of the Great Lakes.
Although the total volume of groundwater is known to be much greater than that of river runoff, a large proportion of this groundwater is saline and should therefore be classified with the saline water above. There is also a lot of fossil groundwater in arid regions that has never been renewed for thousands of years; this must not be seen as renewable water.
However, fresh groundwater is of great value, especially in arid countries such as India. Its distribution is broadly similar to that of surface river water, but it is easier to store in hot and dry climates because groundwater storages are much more shielded from evaporation than are dams. In countries such as Yemen, groundwater from erratic rainfall during the rainy season is the major source of irrigation water.
Because groundwater recharge is much more difficult to accurately measure than surface runoff, groundwater is not generally used in areas where even fairly limited levels of surface water are available. Even today, estimates of total groundwater recharge vary greatly for the same region depending on what source is used, and cases where fossil groundwater is exploited beyond the recharge rate (including the Ogallala Aquifer) are very frequent and almost always not seriously considered when they were first developed.Abiogenesis

Abiogenesis (British English: , ), biopoiesis, or informally the origin of life, is the natural process by which life arises from non-living matter, such as simple organic compounds. On Earth, the transition from non-living to living entities was not a single event but a process of increasing complexity. Abiogenesis is studied through a combination of paleontology, chemistry, and extrapolation from the characteristics of modern organisms, and aims to determine how pre-life chemical reactions gave rise to life on Earth.
The study of abiogenesis can be geophysical, chemical, or biological, with more recent approaches attempting a synthesis of all three, as life arose under conditions that are strikingly different from those on Earth today. Life itself is dependent upon the specialized chemistry of carbon and water and is largely based upon five different families of chemicals. Lipids are fatty molecules comprising large chemical chains of hydrocarbons and play an important role in the structure of living cell membranes, actively and passively determining the transport of other molecules into and out of cells. Carbohydrates are sugars, and as monomer units can be assembled into polymers called polysaccharides, such as cellulose, the rigid chemical of most plant cell walls. Nitrogenous bases are organic molecules in which the amine group of nitrogen, combined with two hydrogen atoms, plays an important part. Chlorophyll is based upon a porphyrin ring derived from amine monomer units, and is important in the capture of the energy needed for life. Nucleic acid monomers are made from a carbohydrate monosaccharide, a nitrogenous base and one or more high energy phosphate groups. When joined together they form the unit of inheritance, the gene, made from DNA or RNA, which translates the genetic information into protein structures. The monomer unit of a protein is usually one of 20 amino acids, comprising an amine group, a hydrocarbon, and a carboxylic acid. Through a condensation reaction, in which the carboxylic acid of one amino acid is linked to the amine of another with removal of a water molecule, a peptide bond is formed. Polymers of amino acids are termed proteins and these molecules provide many catalytic metabolic functions for living processes. Any successful theory of abiogenesis must explain the origins and interactions of these five classes of molecules.
Many approaches to abiogenesis investigate how self-replicating molecules, or their components, came into existence. It is generally thought that current life on Earth is descended from an RNA world, although RNA-based life may not have been the first life to have existed. The classic Miller–Urey experiment and similar research demonstrated that most amino acids, the basic chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Various external sources of energy that may have triggered these reactions have been proposed, including lightning and radiation. Other approaches ("metabolism-first" hypotheses) focus on understanding how catalysis in chemical systems on the early Earth might have provided the precursor molecules necessary for self-replication. Complex organic molecules have been found in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.
The panspermia hypothesis alternatively suggests that microscopic life was distributed to the early Earth by meteoroids, asteroids and other small Solar System bodies and that life may exist throughout the Universe. It is speculated that the biochemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the age of the universe was only 10 to 17 million years old. The panspermia hypothesis proposes that life originated outside the Earth, not how life came to be.
Nonetheless, Earth remains the only place in the Universe known to harbour life, and fossil evidence from the Earth informs most studies of abiogenesis. More than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. The age of the Earth is about 4.54 billion years old; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, and possibly as early as the Eoarchean Era (between 3.6 and 4.0 billion years ago), after geological crust started to solidify following the molten Hadean Eon. In May 2017, evidence of the earliest known life on land may have been found in 3.48-billion-year-old geyserite and other related mineral deposits (often found around hot springs and geysers) uncovered in the Pilbara Craton of Western Australia. However, there have been a number of discoveries that suggested the earliest appearance of life on Earth was even earlier. Currently, microfossils within hydrothermal vent precipitates dated from 3.77 to 4.28 billion years old found in Quebec, Canada may be the oldest record of life on Earth, suggesting "an almost instantaneous emergence of life" after ocean formation 4.4 billion years ago. According to biologist Stephen Blair Hedges, "If life arose relatively quickly on Earth … then it could be common in the universe."Ancient Egypt

Ancient Egypt was a civilization of ancient Northeastern Africa, concentrated along the lower reaches of the Nile River in the place that is now the country Egypt. It is one of six historic civilizations to arise independently. Egyptian civilization followed prehistoric Egypt and coalesced around 3150 BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer). The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.
Egypt reached the pinnacle of its power in the New Kingdom, during the Ramesside period, where it rivalled the Hittite Empire, Assyrian Empire and Mitanni Empire, after which it entered a period of slow decline. Egypt was invaded or conquered by a succession of foreign powers, such as the Canaanites/Hyksos, Libyans, the Nubians, the Assyrians, Babylonians, the Achaemenid Persians, and the Macedonians in the Third Intermediate Period and the Late Period of Egypt. In the aftermath of Alexander the Great's death, one of his generals, Ptolemy Soter, established himself as the new ruler of Egypt. This Greek Ptolemaic Kingdom ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.
The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to defeat foreign enemies and assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.
The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Egypt left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.Ancient Egyptian deities

Ancient Egyptian deities are the gods and goddesses worshipped in ancient Egypt. The beliefs and rituals surrounding these gods formed the core of ancient Egyptian religion, which emerged sometime in prehistory. Deities represented natural forces and phenomena, and the Egyptians supported and appeased them through offerings and rituals so that these forces would continue to function according to maat, or divine order. After the founding of the Egyptian state around 3100 BC, the authority to perform these tasks was controlled by the pharaoh, who claimed to be the gods' representative and managed the temples where the rituals were carried out.
The gods' complex characteristics were expressed in myths and in intricate relationships between deities: family ties, loose groups and hierarchies, and combinations of separate gods into one. Deities' diverse appearances in art—as animals, humans, objects, and combinations of different forms—also alluded, through symbolism, to their essential features.
In different eras, various gods were said to hold the highest position in divine society, including the solar deity Ra, the mysterious god Amun, and the mother goddess Isis. The highest deity was usually credited with the creation of the world and often connected with the life-giving power of the sun. Some scholars have argued, based in part on Egyptian writings, that the Egyptians came to recognize a single divine power that lay behind all things and was present in all the other deities. Yet they never abandoned their original polytheistic view of the world, except possibly during the era of Atenism in the 14th century BC, when official religion focused exclusively on the impersonal sun god Aten.
Gods were assumed to be present throughout the world, capable of influencing natural events and the course of human lives. People interacted with them in temples and unofficial shrines, for personal reasons as well as for larger goals of state rites. Egyptians prayed for divine help, used rituals to compel deities to act, and called upon them for advice. Humans' relations with their gods were a fundamental part of Egyptian society.Ancient Egyptian religion

Ancient Egyptian religion was a complex system of polytheistic beliefs and rituals which were an integral part of ancient Egyptian society. It centered on the Egyptians' interaction with many deities who were believed to be present in, and in control of, the forces of nature. Rituals such as prayers and offerings were efforts to provide for the gods and gain their favor. Formal religious practice centered on the pharaoh, the king of Egypt, who was believed to possess a divine power by virtue of his position. He acted as the intermediary between his people and the gods and was obligated to sustain the gods through rituals and offerings so that they could maintain order in the universe. The state dedicated enormous resources to Egyptian rituals and to the construction of the temples.
Individuals could interact with the gods for their own purposes, appealing for their help through prayer or compelling them to act through magic. These practices were distinct from, but closely linked with, the formal rituals and institutions. The popular religious tradition grew more prominent in the course of Egyptian history as the status of the Pharaoh declined. Another important aspect was the belief in the afterlife and funerary practices. The Egyptians made great efforts to ensure the survival of their souls after death, providing tombs, grave goods, and offerings to preserve the bodies and spirits of the deceased.
The religion had its roots in Egypt's prehistory and lasted for more than 3,000 years. The details of religious belief changed over time as the importance of particular gods rose and declined, and their intricate relationships shifted. At various times, certain gods became preeminent over the others, including the sun god Ra, the creator god Amun, and the mother goddess Isis. For a brief period, in the theology promulgated by the Pharaoh Akhenaten, a single god, the Aten, replaced the traditional pantheon. Ancient Egyptian religion and mythology left behind many writings and monuments, along with significant influences on ancient and modern cultures.Ancient Egyptian race controversy

The question of the race of ancient Egyptians was raised historically as a product of the early racial concepts of the 18th and 19th centuries, and was linked to models of racial hierarchy primarily based on craniometry, anthropometry and genetics. A variety of views circulated about the racial identity of the Egyptians and the source of their culture. These were typically identified in terms of a distinction between the Caucasoid and Negroid racial categories. Some scholars argued that ancient Egyptian culture was influenced by other Afroasiatic-speaking populations in Northeast Africa, the Maghreb or the Middle East, while others pointed to influences from various Nubian groups or populations in Europe.Cats in ancient Egypt

Cats (Felis silvestris catus), known in ancient Egypt as "Mau", were considered sacred in ancient Egyptian society. Based on recent DNA comparisons of living species, it has been estimated that cats were first domesticated from the Middle Eastern subspecies of the wildcat about 10,000 years ago in the Fertile Crescent. Thousands of years later, the peoples in what would later be Upper and Lower Egypt had a religion centering on the worship of animals, including cats.
Praised for controlling vermin and its ability to kill snakes such as cobras, the domesticated cat became a symbol of grace and poise.
As domestication was not as steadfast with cats as today, wealthy families would often curate examples of well bred felines, show them, and pride themselves in the coloration and behavioural adaptations that are seen in today's organized shows.
The goddess Mafdet, the deification of justice and execution, was a lion-headed goddess. The cat goddess Bast (also known as Bastet) eventually replaced Mafdet, and Bast's image softened over time and she became the deity representing protection, fertility, and motherhood.
As a revered animal and one important to Egyptian society and religion, some cats received the same mummification after death as humans. Mummified cats were given in offering to Bast. In 1888, an Egyptian farmer uncovered a large tomb with mummified cats and kittens. This discovery outside the town of Beni Hasan had eighty thousand cat mummies, dated after 1000 BC. The punishments for harming cats were severe.

Cats were one of the most recognizable species in Egyptian culture and were domesticated much later than dogs. Two types of smaller cats appeared in ancient Egypt: the jungle cat (Felis chaus) and the African wild cat (Felis silvestris libyca). The African wild cat was domesticated from the Predynastic Period onward. Wild cats naturally preyed upon the rats and other vermin that ate from the royal granaries. They earned their place in towns and cities by killing mice, venomous snakes, and other pests. They were worshipped by the Egyptians and given jewelry in hieroglyphics.
Small cats would often be found underneath women's chairs on reliefs, evoking fertility and sexuality. The other variety of cat, the lion, was also prevalent in Egyptian culture. Although most lions receded to the south around the Predynastic Period, lions were rare in pharaonic times, but were extremely important in Egyptian iconography. Lions represented royal authority because of their aggressive nature and power.History of ancient Egypt

The history of ancient Egypt spans the period from the early prehistoric settlements of the northern Nile valley to the Roman conquest, in 30 BC. The Pharaonic Period is dated from the 32nd century BC, when Upper and Lower Egypt were unified, until the country fell under Macedonian rule, in 332 BC.Art of ancient Egypt

Ancient Egyptian art is the painting, sculpture, architecture and other arts produced by the civilization of ancient Egypt in the lower Nile Valley from about 3000 BC to 30 AD. Ancient Egyptian art reached a high level in painting and sculpture, and was both highly stylized and symbolic. It was famously conservative, and Egyptian styles changed remarkably little over more than three thousand years. Much of the surviving art comes from tombs and monuments and thus there is an emphasis on life after death and the preservation of knowledge of the past.
Ancient Egyptian art included paintings, sculpture in wood (now rarely surviving), stone and ceramics, drawings on papyrus, faience, jewelry, ivories, and other art media. It displays an extraordinarily vivid representation of the ancient Egyptian's socioeconomic status and belief systems.Military of ancient Egypt

Ancient Egypt was an ancient civilization of eastern North Africa, concentrated along the northern reaches of the Nile River in Egypt. The civilization coalesced around 3150 BC with the political unification of Upper and Lower Egypt under the first pharaoh, and it developed over the next three millennia. Its history occurred in a series of stable kingdoms, separated by periods of relative instability known as intermediate periods. Ancient Egypt reached its pinnacle during the New Kingdom, after which it entered a period of slow decline. Egypt was conquered by a succession of foreign powers in this late period, and the rule of the pharaohs officially ended in 31 BC when the early Roman Empire conquered Egypt and made it a province. Although the Egyptian military forces in the Old and Middle kingdoms were well maintained, the new form that emerged in the New Kingdom showed the state becoming more organized to serve its needs.
For most parts of its long history, ancient Egypt was unified under one government. The main military concern for the nation was to keep enemies out. The arid plains they wanted to get rid of and deserts surrounding Egypt were inhabited by nomadic tribes who occasionally tried to raid or settle in the fertile Nile River valley. Nevertheless, the great expanses of the desert formed a barrier that protected the river valley and was almost impossible for massive armies to cross. The Egyptians built fortresses and outposts along the borders east and west of the Nile Delta, in the Eastern Desert, and in Nubia to the south. Small garrisons could prevent minor incursions, but if a large force was detected a message was sent for the main army corps. Most Egyptian cities lacked city walls and other defenses.
The history of ancient Egypt is divided into three kingdoms and two intermediate periods. During the three kingdoms Egypt was unified under one government. During the intermediate periods (the periods of time between kingdoms) government control was in the hands of the various nomes (provinces within Egypt) and various foreigners. The geography of Egypt served to isolate the country and allowed it to thrive. This circumstance set the stage for many of Egypt's military conquests. They enfeebled their enemies by using small projectile weapons, like bows and arrows. They also had chariots which they used to charge at the enemy.Slavery in ancient Egypt

Slavery in Ancient Egypt was established in the New Kingdom (1550-1175 BCE), with slaves along with servants and peasants making up 80% of the population.
 Interpretation of the textual evidence of slaves in Ancient Egypt is indistinct and has been difficult to differentiate between “slave” and “servant” by word usage alone. There were three types of enslavement in Ancient Egypt: chattel slavery, bonded labor, and forced labor.Clothing in ancient Egypt

Ancient Egyptian clothes refers to clothing worn in ancient Egypt from the end of the Neolithic period (prior to 3100 BC) to the collapse of the Ptolemaic dynasty with the death of Cleopatra VII in 30 BC. Egyptian clothing was filled with a variety of colors. Adorned with precious gems and jewels, the fashions of the Ancient Egyptians were made for not only beauty but also comfort. Egyptian fashion was created to keep cool while in the hot desert.Biblical manuscript

A biblical manuscript is any handwritten copy of a portion of the text of the Bible. The word Bible comes from the Greek biblia (books); manuscript comes from Latin manu (hand) and scriptum (written). Biblical manuscripts vary in size from tiny scrolls containing individual verses of the Jewish scriptures (see Tefillin) to huge polyglot codices (multi-lingual books) containing both the Hebrew Bible (Tanakh) and the New Testament, as well as extracanonical works.
The study of biblical manuscripts is important because handwritten copies of books can contain errors. The science of textual criticism attempts to reconstruct the original text of books, especially those published prior to the invention of the printing press.Manuscript

A manuscript (abbreviated MS for singular and MSS for plural) is any document written by hand or typewritten, as opposed to being mechanically printed or reproduced in some indirect or automated way. More recently, it is understood to be an author's written, typed, or word-processed copy of a work, as distinguished from the print of the same. Before the arrival of printing, all documents and books were manuscripts. Manuscripts are not defined by their contents, which may combine writing with mathematical calculations, maps, explanatory figures or illustrations. Manuscripts may be in book form, scrolls or in codex format. Illuminated manuscripts are enriched with pictures, border decorations, elaborately embossed initial letters or full-page illustrations.Biblical inerrancy

Biblical inerrancy, as formulated in the "Chicago Statement on Biblical Inerrancy", is the doctrine that the Protestant Bible "is without error or fault in all its teaching"; or, at least, that "Scripture in the original manuscripts does not affirm anything that is contrary to fact".
A formal statement in favor of biblical inerrancy was published in the Journal of the Evangelical Theological Society in 1978. The signatories to the "Chicago Statement on Biblical Inerrancy" admit that "inspiration, strictly speaking, applies only to the autographic text of Scripture". However, even though there may be no extant original manuscripts of the Bible, those which exist can be considered inerrant, because, as the statement reads: "the autographic text of Scripture, ... in the providence of God can be ascertained from available manuscripts with great accuracy".
Some equate inerrancy with infallibility; others do not. Biblical inerrancy should not be confused with biblical literalism.
Inerrancy has been much more of an issue in American evangelicalism than in British evangelicalism. According to Stephen R. Holmes, it "plays almost no role in British evangelical life".
There is a minority of biblical inerrantists who go further than the "Chicago Statement on Biblical Inerrancy", arguing that the original text has been perfectly preserved and passed down through time. This is sometimes called Textus Receptus Onlyism, as it is believed the Greek text by this name (Latin for received text) is a perfect and inspired copy of the original and supersedes earlier manuscript copies. This position is based on the idea that only the original language God spoke in is inspired, and that God was pleased to preserve that text throughout history by the hands of various scribes and copyists. There are others who not only believe the original text has been supernaturally preserved without error in its copies, but that the English translation made from that supposed perfect manuscript was also supernaturally composed. This position is known by its opponents as King James Onlyism or KJV Onlyism. One of its most vocal, prominent and thorough proponents was Peter Ruckman, whose followers were generally known as Ruckmanites. He was generally considered to hold the most extreme form of this position. Ultimately both positions suffer from the same historical and textual problems, but KJV Onlyism adds another layer of difficulty to overcome.
The copies of the original language texts that are used by modern translators as the source for translations of the books of the Bible are reconstructions of the original text. Today's versions are based upon scholarly comparison of thousands of biblical manuscripts (such as the Dead Sea Scrolls) and thousands of biblical citations in the writings of the early Church Fathers.
The "doctrine of the inerrancy of scripture" held by the Catholic Church, as expressed by the Second Vatican Council, is that "the books of Scripture must be acknowledged as teaching solidly, faithfully and without error that truth which God wanted put into sacred writings for the sake of salvation."Biblical Manuscripts in the Freer Collection

The Biblical Manuscripts in the Freer Collection, is a collection of six biblical manuscripts, dating from the 3rd to 6th centuries. Most manuscripts are written in Greek, one in Coptic. They are important witnesses of the history of the text of New Testament and Septuagint. It was established by Charles Freer (1854–1919), industrialist from Detroit, Michigan. The collection is located at the Freer Gallery of Art in Washington. It is a noted collection of biblical manuscripts outside Europe.
All these manuscripts were purchased at the beginning of the 20th century in Egypt by Charles Freer. Four manuscripts were bought on 19 December 1906 from an Arab dealer named Ali in Giza, not afar from Cairo. Freer paid £1,600. Upon the next expedition to Egypt, Freer met with Ali and acquired a Coptic codex of the Psalms and the earliest papyrus codex of the Minor Prophets.
Formerly these manuscripts were held in Detroit, Michigan, in the private collection of Freer. He gave to the United States his art collections and funds for a building to house them. The building cost $1,000,000, all of which was paid by Freer. The Freer Gallery of Art was opened in 1923.Biblical studies

Biblical studies is the academic application of a set of diverse disciplines to the study of the Hebrew and Christian scriptures, the Bible. For its theory and methods, the field draws on disciplines ranging from archaeology, ancient history, cultural backgrounds, textual criticism, literary criticism, historical backgrounds, philology, and social science.
Many secular as well as religious universities and colleges offer courses in biblical studies, usually in departments of religious studies, theology, Judaic studies, history, or comparative literature. Biblical scholars do not necessarily have a faith commitment to the texts they study, but many do.List of Hebrew Bible manuscripts

A Hebrew Bible manuscript is a handwritten copy of a portion of the text of the Hebrew Bible (Tanakh) made on papyrus, parchment, or paper, and written in the Hebrew language. (Some of the Biblical text and notations may be in Aramaic.) The oldest manuscripts were written in a form of scroll, the medieval manuscripts usually were written in a form of codex. The late manuscripts written after the 9th century use the Masoretic Text. The important manuscripts are associated with Aaron ben Asher (especially Codex Leningradensis).
The original manuscripts and early copies of the Old Testament disappeared over time, because of wars, (especially the destruction of the First and Second Temples), and other intentional destructions. As a result, the lapse of time between the original manuscripts and their surviving copies is much longer than in the case of the New Testament manuscripts.
The first list of the Old Testament manuscripts in Hebrew, made by Benjamin Kennicott (1776–1780) and published by Oxford, listed 615 manuscripts from libraries in England and on the Continent. Giovanni de Rossi (1784–1788) published a list of 731 manuscripts. The main manuscript discoveries in modern times are those of the Cairo Geniza (c. 1890) and the Dead Sea Scrolls (1947). In the old synagogue in Cairo were discovered 260,000 Hebrew manuscripts, 10,000 of which are biblical manuscripts. There are more than 200 biblical manuscripts among the Dead Sea Scrolls, some of them were written in the Paleo-Hebrew alphabet. They were written before the year 70 AD. 14 scroll manuscripts were discovered in Masada in 1963–1965.
The largest organized collection of Hebrew Old Testament manuscripts in the world is housed in the Russian National Library ("Second Firkovitch Collection") in Saint Petersburg.
Codex Leningradensis is the oldest complete manuscript of the Hebrew Bible in Hebrew. Manuscripts earlier than the 13th century are very rare. The majority of the manuscripts have survived in a fragmentary condition.Chester Beatty Papyri

The Chester Beatty Biblical Papyri or simply the Chester Beatty Papyri are a group of early papyrus manuscripts of biblical texts. The manuscripts are in Greek and are of Christian origin. There are eleven manuscripts in the group, seven consisting of portions of Old Testament books, three consisting of portions of the New Testament (Gregory-Aland no. P45, P46, and P47), and one consisting of portions of the Book of Enoch and an unidentified Christian homily. Most are dated to the 3rd century. They are housed in part at the Chester Beatty Library in Dublin, Ireland, and in part at the University of Michigan, among a few other locations.
The papyri were most likely first obtained by dealers in illegal antiquities. Because of this, the exact circumstances of the find are not clear. One account is that the manuscripts were in jars in a Coptic graveyard near the ruins of the ancient city of Aphroditopolis. Other theories have proposed that the collection was found near the Fayum instead of Aphroditopolis, or that the location was a Christian church or monastery instead of a graveyard. Most of the papyri were bought from a dealer by Alfred Chester Beatty, after whom the manuscripts are named, although some leaves and fragments were acquired by the University of Michigan and a few other collectors and institutions.
The papyri were first announced on November 19, 1931, although more leaves were acquired over the next decade. Frederic G. Kenyon published the manuscripts in The Chester Beatty Biblical Papyri: Descriptions and Texts of Twelve Manuscripts on Papyrus of the Greek Bible, in an 8 volume work that spanned 1933-58. The papyri are usually cataloged as P. Chester Beatty followed by a corresponding Roman numeral between I-XII, one for each manuscript.
The term "Chester Beatty Papyri" can also generally refer to the collection of manuscripts that Alfred Chester Beatty acquired over his lifetime, which include non-Biblical papyri such as the Chester Beatty Medical Papyrus.False memory

A false memory is the psychological phenomenon where a person recalls something that did not happen. False memory is often considered in legal cases regarding childhood sexual abuse. This phenomenon was initially investigated by psychological pioneers Pierre Janet and Sigmund Freud. Freud wrote The Aetiology of Hysteria, where he discussed repressed memories of childhood sexual trauma in their relation to hysteria. Elizabeth Loftus has, since her debuting research project in 1974, been a lead researcher in memory recovery and false memories. False memory syndrome recognizes false memory as a prevalent part of one's life in which it affects the person's mentality and day-to-day life. False memory syndrome differs from false memory in that the syndrome is heavily influential in the orientation of a person's life, while false memory can occur without this significant effect. The syndrome takes effect because the person believes the influential memory to be true. However, its research is controversial and the syndrome is excluded from identification as a mental disorder and, therefore, is also excluded from the Diagnostic and Statistical Manual of Mental Disorders. False memory is an important part of psychological research because of the ties it has to a large number of mental disorders, such as PTSD.Hermann Ebbinghaus

Hermann Ebbinghaus (January 24, 1850 – February 26, 1909) was a German psychologist who pioneered the experimental study of memory, and is known for his discovery of the forgetting curve and the spacing effect. He was also the first person to describe the learning curve. He was the father of the eminent neo-Kantian philosopher Julius Ebbinghaus.Memory

Memory is the faculty of the mind by which information is encoded, stored, and retrieved.
Memory is vital to experiences and related to limbic systems, it is the retention of information over time for the purpose of influencing future action. If we could not remember past events, we could not learn or develop language, relationships, nor personal identity (Eysenck, 2012).
Often memory is understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory (Baddely, 2007). The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to with various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store data through various categorical models or systems (Baddely, 2007).
Explicit and implicit functions of memory are also known as declarative and non-declarative systems (Squire, 2009). These systems involve the purposeful intention of memory retrieval and storage, or lack thereof. Declarative, or explicit, memory is the conscious storage and recollection of data (Graf & Schacter, 1985). Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning (Eysenck, 2012), while episodic memory refers to information that is encoded along a spatial and temporal plane (Schacter & Addis, 2007; Szpunar, 2010). Declarative memory is usually the primary process thought of when referencing memory (Eysenck, 2012).
Non-declarative, or implicit, memory is the unconscious storage and recollection of information (Foerde & Poldrack, 2009). An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon (Eysenck, 2012; Foerde & Poldrack, 2009; Tulving & Schacter, 1990). Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated (Tulving & Schacter, 1990), whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning (Eysenck, 2012; Foerde & Poldrack, 2009).
Memory is not a perfect processor, and is affected by many factors. The manner information is encoded, stored, and retrieved can all be corrupted. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage (Eysenck, 2012). Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus (Squire, 2009). Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory (Eysenck, 2012). Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of memory.
Memory loss is usually described as forgetfulness or amnesia.Eidetic memory

Eidetic memory (; sometimes called photographic memory) is an ability to vividly recall images from memory after only a few instances of exposure, with high precision for a brief time after exposure, without using a mnemonic device. Although the terms eidetic memory and photographic memory are popularly used interchangeably, they are also distinguished, with eidetic memory referring to the ability to view memories like photographs for a few minutes, and photographic memory referring to the ability to recall pages of text or numbers, or similar, in great detail. When the concepts are distinguished, eidetic memory is reported to occur in a small number of children and as something generally not found in adults, while true photographic memory has never been demonstrated to exist.
The word eidetic comes from the Greek word εἶδος (pronounced [êːdos], eidos, "seen").Memory erasure

Memory erasure is the selective artificial removal of memories or associations from the mind.
There are many reasons that research is being done on the selective removal of memories. Potential patients for this research include patients suffering from psychiatric disorders such as post traumatic stress disorder, or substance use disorder, among others.
Memory erasure has been shown to be possible in some experimental conditions; some of the techniques currently being investigated are: drug-induced amnesia, selective memory suppression, destruction of neurons, interruption of memory, reconsolidation, and the disruption of specific molecular mechanisms.Implicit memory

Implicit memory is one of the two main types of long-term human memory. It is acquired and used unconsciously, and can affect thoughts and behaviours. One of its most common forms is procedural memory, which helps people performing certain tasks without conscious awareness of these previous experiences.
Implicit memory's counterpart is known as explicit memory or declarative memory, which refers to the conscious, intentional recollection of factual information, previous experiences and concepts.
Evidence for implicit memory arises in priming, a process whereby subjects are measured by how they have improved their performance on tasks for which they have been subconsciously prepared. Implicit memory also leads to the illusion-of-truth effect, which suggests that subjects are more likely to rate as true those statements that they have already heard, regardless of their truthfulness.
In daily life, people rely on implicit memory every day in the form of procedural memory, the type of memory that allows people to remember how to tie their shoes or ride a bicycle without consciously thinking about these activities. Research into implicit memory indicates that it operates through a different mental process from explicit memory.Memory development

The development of memory in children becomes evident within the first 3 years of a child's life as they show considerable advances in declarative memory. This enhancement continues into adolescence with major developments in short term memory, working memory, long term memory and autobiographical memory.
Recent research on the development of memory has indicated that declarative, or explicit memory, may exist in infants who are even younger than two years old. For example, newborns who are less than 3 days old demonstrate a preference for their mother’s own voice.Spatial memory

In cognitive psychology and neuroscience, spatial memory is the part of memory responsible for recording information about one's environment and spatial orientation. For example, a person's spatial memory is required in order to navigate around a familiar city, just as a rat's spatial memory is needed to learn the location of food at the end of a maze. It is often argued that in both humans and animals, spatial memories are summarized as a cognitive map. Spatial memory has representations within working, short-term memory and long-term memory. Research indicates that there are specific areas of the brain associated with spatial memory. Many methods are used for measuring spatial memory in children, adults, and animals.Hyperthymesia

Hyperthymesia is the condition of possessing an extremely detailed autobiographical memory. People with hyperthymesia remember an abnormally vast number of their life experiences.
American neurobiologists Elizabeth Parker, Larry Cahill, and James McGaugh (2006) identified two defining characteristics of hyperthymesia: spending an excessive amount of time thinking about one's past, and displaying an extraordinary ability to recall specific events from one's past. The word "hyperthymesia" derives from Ancient Greek: hyper- ("excessive") and thymesis ("remembering").Childhood memory

Childhood memory refers to memories formed during childhood. Among its other roles, memory functions to guide present behaviour and to predict future outcomes. Memory in childhood is qualitatively and quantitatively different from the memories formed and retrieved in late adolescence and the adult years. Childhood memory research is relatively recent in relation to the study of other types of cognitive processes underpinning behaviour. Understanding the mechanisms by which memories in childhood are encoded and later retrieved has important implications in many areas. Research into childhood memory includes topics such as childhood memory formation and retrieval mechanisms in relation to those in adults, controversies surrounding infantile amnesia and the fact that adults have relatively poor memories of early childhood, the ways in which school environment and family environment influence memory, and the ways in which memory can be improved in childhood to improve overall cognition, performance in school, and well-being, both in childhood and in adulthood.New World Order (conspiracy theory)

The New World Order or NWO is claimed to be an emerging clandestine totalitarian world government by various conspiracy theories.
The common theme in conspiracy theories about a New World Order is that a secretive power elite with a globalist agenda is conspiring to eventually rule the world through an authoritarian world government—which will replace sovereign nation-states—and an all-encompassing propaganda whose ideology hails the establishment of the New World Order as the culmination of history's progress. Many influential historical and contemporary figures have therefore been purported to be part of a cabal that operates through many front organizations to orchestrate significant political and financial events, ranging from causing systemic crises to pushing through controversial policies, at both national and international levels, as steps in an ongoing plot to achieve world domination.
Before the early 1990s, New World Order conspiracism was limited to two American countercultures, primarily the militantly anti-government right and secondarily that part of fundamentalist Christianity concerned with the end-time emergence of the Antichrist. Skeptics such as Michael Barkun and Chip Berlet observed that right-wing populist conspiracy theories about a New World Order had not only been embraced by many seekers of stigmatized knowledge but had seeped into popular culture, thereby inaugurating a period during the late 20th and early 21st centuries in the United States where people are actively preparing for apocalyptic millenarian scenarios. Those political scientists are concerned that mass hysteria over New World Order conspiracy theories could eventually have devastating effects on American political life, ranging from escalating lone-wolf terrorism to the rise to power of authoritarian ultranationalist demagogues.List of conspiracy theories

For a list of genuine conspiracies, see List of political conspiracies
There are many unproven conspiracy theories with varying degrees of popularity, frequently related to clandestine government plans and elaborate murder plots. Conspiracy theories usually deny consensus or cannot be proven using the historical or scientific method, and are not to be confused with research concerning verified conspiracies such as Germany's pretense for invading Poland in World War II. Conspiracy theory is often considered the opposite of institutional analysis.Conspiracy theories in the Arab world

Conspiracy theories are a prevalent feature of Arab culture and politics - particularly fuelling anti semitism in the Arab World. Prof. Matthew Gray writes they "are a common and popular phenomenon." "Conspiracism is an important phenomenon in understanding Arab Middle Eastern politics ..." Variants include conspiracies involving colonialism, Zionism, superpowers, oil, and the war on terrorism, which may be referred to as a War against Islam. Roger Cohen theorizes that the popularity of conspiracy theories in the Arab world is "the ultimate refuge of the powerless", and Al-Mumin Said noted the danger of such theories in that they "keep us not only from the truth but also from confronting our faults and problems..."
Gray points out that actual conspiracies such as the British-French-Israeli 1956 Suez Crisis encourage speculation and creation of imagined conspiracies. After the 1967 war, conspiracy theories became popular. The war was perceived as a conspiracy by Israel and the US—or its opposite: a Soviet plot to bring Egypt into the Soviet sphere of influence. Thomas Friedman notes the numerous conspiracy theories concerning the Lebanese civil war. They "were usually the most implausible, wild-eyed conspiracy theories one could imagine ... Israelis, the Syrians, the Americans, the Soviets, or Henry Kissinger—anyone but the Lebanese—in the most elaborate plots to disrupt Lebanon's naturally tranquil state."Prokaryote

A prokaryote is a unicellular organism that lacks a membrane-bound nucleus (karyon), mitochondria, or any other membrane-bound organelle. The word prokaryote comes from the Greek πρό (pro) "before" and κάρυον (karyon) "nut or kernel". Prokaryotes can be divided into two domains, archaea and bacteria. In contrast, species with nuclei and organelles are placed in the domain Eukaryota.
In the prokaryotes, all the intracellular water-soluble components (proteins, DNA and metabolites) are located together in the cytoplasm enclosed by the cell membrane, rather than in separate cellular compartments. Bacteria, however, do possess protein-based bacterial microcompartments, which are thought to act as primitive organelles enclosed in protein shells. Some prokaryotes, such as cyanobacteria may form large colonies. Others, such as myxobacteria, have multicellular stages in their life cycles.
Molecular studies have provided insight into the evolution and interrelationships of the three domains of biological species. Eukaryotes are organisms, including humans, whose cells have a well defined membrane-bound nucleus (containing chromosomal DNA) and organelles. The division between prokaryotes and eukaryotes reflects the existence of two very different levels of cellular organization. Distinctive types of prokaryotes include extremophiles and methanogens; these are common in some extreme environments.Cell theory

In biology, cell theory is the historic scientific theory, now universally accepted, that living organisms are made up of cells. Cells are the basic unit of structure in all organisms and also the basic unit of reproduction. With continual improvements made to microscopes over time, magnification technology advanced enough to discover cells in the 17th century. This discovery is largely attributed to Robert Hooke, and began the scientific study of cells, also known as cell biology. Over a century later, many debates about cells began amongst scientists. Most of these debates involved the nature of cellular regeneration, and the idea of cells as a fundamental unit of life. Cell theory was eventually formulated in 1839. This is usually credited to Matthias Schleiden and Theodor Schwann. However, many other scientists like Rudolf Virchow contributed to the theory.
The three tenets to the cell theory are as described below:
All living organisms are composed of one or more cells. (However, this is considered a controversy because non-cellular life such as viruses are disputed as a life form. See Non-cellular life.)
The cell is the basic unit of structure and organization in organisms.
Cells arise from pre-existing cells.Chromosome

A chromosome (from ancient Greek: χρωμόσωμα, chromosoma, chroma means color, soma means body) is a DNA molecule with part or all of the genetic material (genome) of an organism.
Chromosomes are normally visible under a light microscope only when the cell is undergoing the metaphase of cell division. Before this happens, every chromosome is copied once (S phase), and the copy is joined to the original by a centromere, resulting in an X-shaped structure. The original chromosome and the copy are now called sister chromatids. During metaphase, when a chromosome is in its most condensed state, the X-shape structure is called a metaphase chromosome. In this highly condensed form chromosomes are easiest to distinguish and study.
Chromosomes vary widely between different organisms. Some species such as certain bacteria, which lack histones, also contain plasmids or other extrachromosomal DNA. These are circular structures in the cytoplasm that contain cellular DNA and play a role in horizontal gene transfer. In prokaryotes (see nucleoids) and viruses, the DNA is often densely packed and organized; in the case of archaea, by homology to eukaryotic histones, and in the case of bacteria, by histone-like proteins.
DNA condensation of the duplicated chromosomes during cell division (mitosis or meiosis) results either in a four-arm structure (pictured to the right) if the centromere is located in the middle of the chromosome or a two-arm structure if the centromere is located near one of the ends. Chromosomal recombination during meiosis and subsequent sexual reproduction play a significant role in genetic diversity. If these structures are manipulated incorrectly, through processes known as chromosomal instability and translocation, the cell may undergo mitotic catastrophe and die, or it may unexpectedly evade apoptosis, leading to the progression of cancer.
Some use the term chromosome in a wider sense, to refer to the individualized portions of chromatin in cells, either visible or not under light microscopy. However, others use the concept in a narrower sense, to refer to the individualized portions of chromatin during cell division, visible under light microscopy due to high condensation.Ribosomal RNA

Ribosomal ribonucleic acid (rRNA) is the RNA component of the ribosome, and is essential for protein synthesis in all living organisms. It constitutes the predominant material within the ribosome, which is approximately 60% rRNA and 40% protein by weight, or 3/5 of ribosome mass. Ribosomes contain two major rRNAs and 50 or more proteins. The ribosomal RNAs form two subunits, the large subunit (LSU) and small subunit (SSU). The LSU rRNA acts as a ribozyme, catalyzing peptide bond formation. rRNA sequences are widely used for working out evolutionary relationships among organisms, since they are of ancient origin and are found in all known forms of life.Organism

In biology, an organism (from Greek: οργανισμός, organismos) is any individual entity that exhibits the properties of life. It is a synonym for "life form".
Organisms are classified by taxonomy into specified groups such as the multicellular animals, plants, and fungi; or unicellular microorganisms such as a protists, bacteria, and archaea. All types of organisms are capable of reproduction, growth and development, maintenance, and some degree of response to stimuli. Humans are multicellular animals composed of many trillions of cells which differentiate during development into specialized tissues and organs.
An organism may be either a prokaryote or a eukaryote. Prokaryotes are represented by two separate domains—bacteria and archaea. Eukaryotic organisms are characterized by the presence of a membrane-bound cell nucleus and contain additional membrane-bound compartments called organelles (such as mitochondria in animals and plants and plastids in plants and algae, all generally considered to be derived from endosymbiotic bacteria). Fungi, animals and plants are examples of kingdoms of organisms within the eukaryotes.
Estimates on the number of Earth's current species range from 10 million to 14 million, of which only about 1.2 million have been documented. More than 99% of all species, amounting to over five billion species, that ever lived are estimated to be extinct. In 2016, a set of 355 genes from the last universal common ancestor (LUCA) of all living organisms living was identified.Fission (biology)

Fission, in biology, is the division of a single entity into two or more parts and the regeneration of those parts into separate entities resembling the original. The object experiencing fission is usually a cell, but the term may also refer to how organisms, bodies, populations, or species split into discrete parts. The fission may be binary fission, in which a single entity produces two parts, or multiple fission, in which a single entity produces multiple parts.Start codon

The start codon is the first codon of a messenger RNA (mRNA) transcript translated by a ribosome. The start codon always codes for methionine in eukaryotes and a modified Met (fMet) in prokaryotes. The most common start codon is AUG.
The start codon is often preceded by a 5' untranslated region (5' UTR). In prokaryotes this includes the ribosome binding site.Unicellular organism

A unicellular organism, also known as a single-celled organism, is an organism that consists of only one cell, unlike a multicellular organism that consists of more than one cell. Historically, simple unicellular organisms have been referred to as monads, though this term is also used more specifically to describe organisms of the genus Monas and similar flagellate ameboids. The main groups of unicellular organisms are bacteria, archaea, protozoa, unicellular algae, and unicellular fungi. Unicellular organisms fall into two general categories: prokaryotic organisms and eukaryotic organisms. Unicellular organisms are thought to be the oldest form of life, with early protocells possibly emerging 3.8–4 billion years ago.
Prokaryotes, most Protista, and some fungi are unicellular. Although some of these organisms live in colonies, they don't exhibit specialization. These organisms live together, and each cell in the colony is the same. However, each individual cell must carry out all life processes to survive. In contrast, even the simplest multicellular organisms have cells that depend on each other to survive.
Most multicellular organisms have a unicellular life-cycle stage. Gametes, for example, are reproductive unicells for multicellular organisms. Additionally, multicellularity appears to have evolved independently many times in the history of life.
Some organisms are partially uni- and multicellular, like Dictyostelium discoideum. Additionally, unicellular organisms can be multinucleate, like Myxogastria and Plasmodium.
Candidatus Magnetoglobus multicellular, related to Deltaproteobacteria, is a multicellular prokaryote. It is neither unicellular, nor a colony.Gene structure

Gene structure is the organisation of specialised sequence elements within a gene. Genes contain the information necessary for living cells to survive and reproduce. In most organisms, genes are made of DNA, where the particular DNA sequence determines the function of the gene. A gene is transcribed (copied) from DNA into RNA, which can either be non-coding (ncRNA) with a direct function, or an intermediate messenger (mRNA) that is then translated into protein. Each of these steps is controlled by specific sequence elements, or regions, within the gene. Every gene, therefore, requires multiple sequence elements to be functional. This includes the sequence that actually encodes the functional protein or ncRNA, as well as multiple regulatory sequence regions. These regions may be as short as a few base pairs, up to many thousands of base pairs long.
Much of gene structure is broadly similar between eukaryotes and prokaryotes. These common elements largely result from the shared ancestry of cellular life in organisms over 2 billion years ago. Key differences in gene structure between eukaryotes and prokaryotes reflect their divergent transcription and translation machinery. Understanding gene structure is the foundation of understanding gene annotation, expression, and function.Cell (biology)

The cell (from Latin cella, meaning "small room") is the basic structural, functional, and biological unit of all known living organisms. A cell is the smallest unit of life that can replicate independently, and cells are often called the "building blocks of life". The study of cells is called cell biology.
Cells consist of cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. Organisms can be classified as unicellular (consisting of a single cell; including bacteria) or multicellular (including plants and animals). While the number of cells in plants and animals varies from species to species, humans contain more than 10 trillion (1012) cells. Most plant and animal cells are visible only under a microscope, with dimensions between 1 and 100 micrometres.
The cell was discovered by Robert Hooke in 1665, who named the biological units for their resemblance to cells inhabited by Christian monks in a monastery. Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, that all cells come from preexisting cells, and that all cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells. Cells emerged on Earth at least 3.5 billion years ago.Web scraping

Web scraping (web harvesting or web data extraction) is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.
Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping).
Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration.
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an API to extract data from a web site. Companies like Amazon AWS and Google provide web scraping tools, services and public data available free of cost to end users.
Newer forms of web scraping involve listening to data feeds from web servers. For example, JSON is commonly used as a transport storage mechanism between the client and the web server.
There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing.Data scraping

Data scraping is a technique in which a computer program extracts data from human-readable output coming from another program.Web feed

On the World Wide Web, a web feed (or news feed) is a data format used for providing users with frequently updated content. Content distributors syndicate a web feed, thereby allowing users to subscribe a channel to it. Making a collection of web feeds accessible in one spot is known as aggregation, which is performed by a news aggregator. A web feed is also sometimes referred to as a syndicated feed.
A typical scenario of web-feed use might involve the following: a content provider publishes a feed link on its site which end users can register with an aggregator program (also called a feed reader or a news reader) running on their own machines; doing this is usually as simple as dragging the link from the web browser to the aggregator. When instructed, the aggregator asks all the servers in its feed list if they have new content; if so, the aggregator either makes a note of the new content or downloads it. One can schedule aggregators to check for new content periodically.
Web feeds exemplify pull technology, although they may appear to push content to the user.
The kinds of content delivered by a web feed are typically HTML (webpage content) or links to webpages and other kinds of digital media. Often when websites provide web feeds to notify users of content updates, they only include summaries in the web feed rather than the full content itself.
Many news websites, weblogs, schools, and podcasters operate web feeds.List of web testing tools

This is a list of Web testing tools, giving a general overview in terms of features, sometimes used for Web scraping.Illuminati: New World Order

Illuminati: New World Order (INWO) is a collectible card game (CCG) that was released in 1994 by Steve Jackson Games, based on their original boxed game Illuminati, which in turn was inspired by the 1975 book The Illuminatus! Trilogy by Robert Anton Wilson and Robert Shea. INWO won the Origins Award for Best Card Game in 1997.Illuminati

The Illuminati (plural of Latin illuminatus, "enlightened") is a name given to several groups, both real and fictitious. Historically, the name usually refers to the Bavarian Illuminati, an Enlightenment-era secret society founded on 1 May 1776. The society's goals were to oppose superstition, obscurantism, religious influence over public life, and abuses of state power. "The order of the day," they wrote in their general statutes, "is to put an end to the machinations of the purveyors of injustice, to control them without dominating them". The Illuminati—along with Freemasonry and other secret societies—were outlawed through edict, by the Bavarian ruler, Charles Theodore, with the encouragement of the Roman Catholic Church, in 1784, 1785, 1787 and 1790. In the several years following, the group was vilified by conservative and religious critics who claimed that they continued underground and were responsible for the French Revolution.
Many influential intellectuals and progressive politicians counted themselves as members, including Ferdinand of Brunswick and the diplomat Xavier von Zwack, who was the Order's second-in-command. It attracted literary men such as Johann Wolfgang von Goethe and Johann Gottfried Herder and the reigning dukes of Gotha and Weimar.
In subsequent use, "Illuminati" refers to various organisations which claim or are purported to have links to the original Bavarian Illuminati or similar secret societies, though these links are unsubstantiated. They are often alleged to conspire to control world affairs, by masterminding events and planting agents in government and corporations, in order to gain political power and influence and to establish a New World Order. Central to some of the most widely known and elaborate conspiracy theories, the Illuminati have been depicted as lurking in the shadows and pulling the strings and levers of power in dozens of novels, films, television shows, comics, video games, and music videos.New World Order (conspiracy theory)

The New World Order or NWO is claimed to be an emerging clandestine totalitarian world government by various conspiracy theories.
The common theme in conspiracy theories about a New World Order is that a secretive power elite with a globalist agenda is conspiring to eventually rule the world through an authoritarian world government—which will replace sovereign nation-states—and an all-encompassing propaganda whose ideology hails the establishment of the New World Order as the culmination of history's progress. Many influential historical and contemporary figures have therefore been purported to be part of a cabal that operates through many front organizations to orchestrate significant political and financial events, ranging from causing systemic crises to pushing through controversial policies, at both national and international levels, as steps in an ongoing plot to achieve world domination.
Before the early 1990s, New World Order conspiracism was limited to two American countercultures, primarily the militantly anti-government right and secondarily that part of fundamentalist Christianity concerned with the end-time emergence of the Antichrist. Skeptics such as Michael Barkun and Chip Berlet observed that right-wing populist conspiracy theories about a New World Order had not only been embraced by many seekers of stigmatized knowledge but had seeped into popular culture, thereby inaugurating a period during the late 20th and early 21st centuries in the United States where people are actively preparing for apocalyptic millenarian scenarios. Those political scientists are concerned that mass hysteria over New World Order conspiracy theories could eventually have devastating effects on American political life, ranging from escalating lone-wolf terrorism to the rise to power of authoritarian ultranationalist demagogues.Memory conformity

Memory conformity, also known as social contagion of memory, refers to a situation in which one person's report of a memory influences another person's report of that same experience. This interference often occurs when individuals discuss what they saw or experienced, and can result in the memories of those involved being influenced by the report of another person. Research on memory conformity has revealed that such suggestibility has far reaching consequences, with important legal and social implications. It is one of many social influences on memory.
A major component of memory conformity is source monitoring (or source memory). Source monitoring refers to the process by which an individual determines where they learned certain information (friend, TV show, teacher etc.). A source-monitoring error can lead to an incorrect internal attribution of a memory (a belief that the memory was made from first-hand experience), when in reality that information had an external source (someone else relayed that material/memory). Studies have shown that social interaction can increase source-monitoring errors, with some studies showing that participants attributed their memory to an incorrect source approximately 50% of the time.
Three ways that contribute to memory conformity are: normative influences, information influences and memory distortion. Normative and informational influences on memory are both social influences that can lead to conformity (a modification of behavior in response to actual or imagined pressure from others). Social influence can have a strong impact on the retrieval process of memories. Potential social conformity may be affected by factors such as power and confidence (both in oneself and in the credibility of a collaborator). This influence can alter memories, making them partially or entirely false. Memory distortion, closely tied with the misinformation effect, describes an impairment in memory that surfaces after exposure to misleading information.
Memory conformity is prominent in situations involving social interaction, media broadcasting and eyewitness testimony.Conformity

Conformity is the act of matching attitudes, beliefs, and behaviors to group norms. Norms are implicit, specific rules, shared by a group of individuals, that guide their interactions with others. This tendency to conform occurs in small groups and/or society as a whole, and may result from subtle unconscious influences, or direct and overt social pressure. Conformity can occur in the presence of others, or when an individual is alone. For example, people tend to follow social norms when eating or watching television, even when alone.
People often conform from a desire for security within a group—typically a group of a similar age, culture, religion, or educational status. This is often referred to as groupthink: a pattern of thought characterized by self-deception, forced manufacture of consent, and conformity to group values and ethics, which ignores realistic appraisal of other courses of action. Unwillingness to conform carries the risk of social rejection. Conformity is often associated with adolescence and youth culture, but strongly affects humans of all ages.
Although peer pressure may manifest negatively, conformity can have good or bad effects depending on the situation. Driving on the correct side of the road could be seen as beneficial conformity. With the right environmental influence, conforming, in early childhood years, allows one to learn and thus, adopt the appropriate behaviours necessary to interact and develop correctly within one's society. Conformity influences formation and maintenance of social norms, and helps societies function smoothly and predictably via the self-elimination of behaviors seen as contrary to unwritten rules. In this sense it can be perceived as a positive force that prevents acts that are perceptually disruptive or dangerous.
As conformity is a group phenomenon, factors such as group size, unanimity, cohesion, status, prior commitment and public opinion help determine the level of conformity an individual displays.False memory

A false memory is the psychological phenomenon where a person recalls something that did not happen. False memory is often considered in legal cases regarding childhood sexual abuse. This phenomenon was initially investigated by psychological pioneers Pierre Janet and Sigmund Freud. Freud wrote The Aetiology of Hysteria, where he discussed repressed memories of childhood sexual trauma in their relation to hysteria. Elizabeth Loftus has, since her debuting research project in 1974, been a lead researcher in memory recovery and false memories. False memory syndrome recognizes false memory as a prevalent part of one's life in which it affects the person's mentality and day-to-day life. False memory syndrome differs from false memory in that the syndrome is heavily influential in the orientation of a person's life, while false memory can occur without this significant effect. The syndrome takes effect because the person believes the influential memory to be true. However, its research is controversial and the syndrome is excluded from identification as a mental disorder and, therefore, is also excluded from the Diagnostic and Statistical Manual of Mental Disorders. False memory is an important part of psychological research because of the ties it has to a large number of mental disorders, such as PTSD.Memory

Memory is the faculty of the mind by which information is encoded, stored, and retrieved.
Memory is vital to experiences and related to limbic systems, it is the retention of information over time for the purpose of influencing future action. If we could not remember past events, we could not learn or develop language, relationships, nor personal identity (Eysenck, 2012).
Often memory is understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory (Baddely, 2007). The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to with various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store data through various categorical models or systems (Baddely, 2007).
Explicit and implicit functions of memory are also known as declarative and non-declarative systems (Squire, 2009). These systems involve the purposeful intention of memory retrieval and storage, or lack thereof. Declarative, or explicit, memory is the conscious storage and recollection of data (Graf & Schacter, 1985). Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning (Eysenck, 2012), while episodic memory refers to information that is encoded along a spatial and temporal plane (Schacter & Addis, 2007; Szpunar, 2010). Declarative memory is usually the primary process thought of when referencing memory (Eysenck, 2012).
Non-declarative, or implicit, memory is the unconscious storage and recollection of information (Foerde & Poldrack, 2009). An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon (Eysenck, 2012; Foerde & Poldrack, 2009; Tulving & Schacter, 1990). Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated (Tulving & Schacter, 1990), whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning (Eysenck, 2012; Foerde & Poldrack, 2009).
Memory is not a perfect processor, and is affected by many factors. The manner information is encoded, stored, and retrieved can all be corrupted. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage (Eysenck, 2012). Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus (Squire, 2009). Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory (Eysenck, 2012). Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of memory.
Memory loss is usually described as forgetfulness or amnesia.Eidetic memory

Eidetic memory (; sometimes called photographic memory) is an ability to vividly recall images from memory after only a few instances of exposure, with high precision for a brief time after exposure, without using a mnemonic device. Although the terms eidetic memory and photographic memory are popularly used interchangeably, they are also distinguished, with eidetic memory referring to the ability to view memories like photographs for a few minutes, and photographic memory referring to the ability to recall pages of text or numbers, or similar, in great detail. When the concepts are distinguished, eidetic memory is reported to occur in a small number of children and as something generally not found in adults, while true photographic memory has never been demonstrated to exist.
The word eidetic comes from the Greek word εἶδος (pronounced [êːdos], eidos, "seen").Hyperthymesia

Hyperthymesia is the condition of possessing an extremely detailed autobiographical memory. People with hyperthymesia remember an abnormally vast number of their life experiences.
American neurobiologists Elizabeth Parker, Larry Cahill, and James McGaugh (2006) identified two defining characteristics of hyperthymesia: spending an excessive amount of time thinking about one's past, and displaying an extraordinary ability to recall specific events from one's past. The word "hyperthymesia" derives from Ancient Greek: hyper- ("excessive") and thymesis ("remembering").Working memory

Working memory is a cognitive system with a limited capacity that is responsible for temporarily holding information available for processing. Working memory is important for reasoning and the guidance of decision making and behavior. Working memory is often used synonymously with short-term memory, but some theorists consider the two forms of memory distinct, assuming that working memory allows for the manipulation of stored information, whereas short-term memory only refers to the short-term storage of information. Working memory is a theoretical concept central to cognitive psychology, neuropsychology, and neuroscience.Long-term memory

Long-term memory (LTM) is the stage of the dual memory model proposed by the Atkinson and Shiffrin (from Stanford University), and informative knowledge can be stored for long periods of time.
Long-term memory is defined in contrast to short-term and working memory that persist for only about 18 to 30 seconds, while informative knowledge can remain as long-term memory indefinitely.
Long-term memory is commonly labelled as explicit memory (declarative), as well as episodic memory, semantic memory, autobiographical memory, and implicit memory (procedural memory).List of people claimed to possess an eidetic memory

A number of people claim to have eidetic memory, but science has never found a single verifiable case of photographic memory. Eidetic imagery is virtually nonexistent in adults. Most people showing amazing memory abilities use mnemonic strategies, mostly the method of loci. This includes all winners of the annual World Memory Championships and most of the known scientific cases of excellent memories, like S. V. Shereshevskii. Regardless, the following list contains people who have claimed photographic memory.Baddeley's model of working memory

Alan Baddeley and Graham Hitch proposed a model of working memory in 1974, in an attempt to present a more accurate model of primary memory (often referred to as short-term memory). Working memory splits primary memory into multiple components, rather than considering it to be a single, unified construct.
Baddeley & Hitch proposed their three part working memory model as an alternative to the short-term store in Atkinson & Shiffrin's 'multi-store' memory model (1968). This model is later expanded upon by Baddeley and other co-workers to add a fourth component, and has become the dominant view in the field of working memory. However, alternative models are developing (see working memory) providing a different perspective on the working memory system.
The original model of Baddeley & Hitch was composed of three main components; the central executive which acts as supervisory system and controls the flow of information from and to its slave systems: the phonological loop and the visuo-spatial sketchpad. The phonological loop stores verbal content, whereas the visuo-spatial sketchpad caters to visuo-spatial data. Both the slave systems only function as short-term storage centers. In 2000 Baddeley added a third slave system to his model, the episodic buffer.
Baddeley & Hitch's argument for the distinction of two domain-specific slave systems in the older model was derived from experimental findings with dual-task paradigms. Performance of two simultaneous tasks requiring the use of two separate perceptual domains (i.e. a visual and a verbal task) is nearly as efficient as performance of the tasks individually. In contrast, when a person tries to carry out two tasks simultaneously that use the same perceptual domain, performance is less efficient than when performing the tasks individually.Second Book of Enoch

The Second Book of Enoch (usually abbreviated 2 Enoch, and otherwise variously known as Slavonic Enoch or The Secrets of Enoch) is a pseudepigraphic text (a text whose claimed authorship is unfounded) of the Old Testament. It is usually considered to be part of the Apocalyptic literature. The dating often preferred for the writing of 2 Enoch is late 1st century CE. The text has been preserved in full only in Slavonic, but in 2009 it was announced that Coptic fragments of the book had been identified. Greek is indicated as the language from which the Slavonic version was translated. 2 Enoch is not regarded as scripture by Jews or any Christian group. It was rediscovered and published at the end of the 19th century.
Most scholars consider 2 Enoch to be composed by an unknown Jewish sectarian group, while some authors think it is a 1st-century Christian text. Very few scholars consider it a later Christian work.
2 Enoch is distinct from the Book of Enoch, known as 1 Enoch. There is also an unrelated 3 Enoch. The numbering of these texts has been applied by scholars to distinguish the texts from one another.Book of Enoch

The Book of Enoch (also 1 Enoch; Ge'ez: መጽሐፈ ሄኖክ mätṣḥäfä henok) is an ancient Jewish religious work, ascribed by tradition to Enoch, the great-grandfather of Noah, although modern scholars estimate the older sections (mainly in the Book of the Watchers) to date from about 300 BC, and the latest part (Book of Parables) probably to the first century BC.
It is not part of the biblical canon as used by Jews, apart from Beta Israel. Most Christian denominations and traditions may accept the Books of Enoch as having some historical or theological interest, but they generally regard the Books of Enoch as non-canonical or non-inspired. It is regarded as canonical by the Ethiopian Orthodox Tewahedo Church and Eritrean Orthodox Tewahedo Church, but not by any other Christian groups.
It is wholly extant only in the Ge'ez language, with Aramaic fragments from the Dead Sea Scrolls and a few Greek and Latin fragments. For this and other reasons, the traditional Ethiopian belief is that the original language of the work was Ge'ez, whereas non-Ethiopian scholars tend to assert that it was first written in either Aramaic or Hebrew; Ephraim Isaac suggests that the Book of Enoch, like the Book of Daniel, was composed partially in Aramaic and partially in Hebrew. No Hebrew version is known to have survived. It is asserted in the book itself that its author was Enoch, before the Biblical Flood.
Some of the authors of the New Testament were familiar with some of the content of the story. A short section of 1 Enoch (1:9) is cited in the New Testament, Epistle of Jude, Jude 1:14–15, and is attributed there to "Enoch the Seventh from Adam" (1 En 60:8), although this section of 1 Enoch is a midrash on Deuteronomy 33. Several copies of the earlier sections of 1 Enoch were preserved among the Dead Sea Scrolls.Enoch (ancestor of Noah)

Enoch (; Hebrew: חֲנוֹךְ, Modern H̱anokh, Tiberian Ḥănōḵ; Arabic: أَخْنُوخ‎‎ ʼAkhnūkh, [commonly in Qur'ānic literature]: إِدْرِيس ʼIdrīs) is a figure in Biblical literature. "In the seventh generation from Adam," he was considered the author of the Book of Enoch and also called Enoch the scribe of judgment. In addition to an appearance in the Book of Genesis of the Hebrew Bible, Enoch is the subject of many Jewish, Christian, and Muslim writings.
Enoch was the son of Jared (Genesis 5:19–21), the father of Methuselah, and the great-grandfather of Noah. At 65 years old, he begot Methuselah. Regim and Gaidad are also mentioned as his sons according to 2 Enoch.
The Bible says that Enoch lived 365 years before he was taken by God. The text reads that Enoch "walked with God: and he was no more; for God took him" (Gen 5:21–24), which some Christians interpret as Enoch entering Heaven alive.
This Enoch is not to be confused with Cain's son Enoch (Genesis 4:17). The Christian New Testament has three references to Enoch from the lineage of Seth (Luke 3:37, Hebrews 11:5, Jude 1:14–15).3 Enoch

3 Enoch is Biblical apocryphal in Hebrew. 3 Enoch purports to have been written in the 2nd century, but its origins can only be traced to the 5th century. Other names for 3 Enoch include "The Third Book of Enoch", "The Book of the Palaces", "The Book of Rabbi Ishmael the High Priest" and "The Revelation of Metatron".
Most commonly, the Book of Enoch refers to 1 Enoch, which survived completely only in Ge'ez. There is also a Second Book of Enoch, which has survived only in Old Slavonic.Machine learning

Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed.
Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term "Machine Learning" in 1959 while at IBM. Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms with good performance is difficult or infeasible; example applications include email filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), learning to rank, and computer vision.
Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning. Machine learning can also be unsupervised and be used to learn and establish baseline behavioral profiles for various entities and then used to find meaningful anomalies.
Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.
As of 2016, machine learning is a buzzword, and according to the Gartner hype cycle of 2016, at its peak of inflated expectations. Effective machine learning is difficult because finding patterns is hard and often not enough training data is available; as a result, machine-learning programs often fail to deliver.Active learning (machine learning)

Active learning is a special case of semi-supervised machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.  In statistics literature it is sometimes also called optimal experimental design. 
There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm be overwhelmed by uninformative examples. Recent developments are dedicated to hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of Machine Learning (e.g., conflict and ignorance) with adaptive, incremental learning policies in the field of Online machine learning.Outline of machine learning

The following outline is provided as an overview of and topical guide to machine learning:
Machine learning – subfield of computer science (more particularly soft computing) that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.Quantum machine learning

Quantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning. One can distinguish four different ways of merging the two parent disciplines. Quantum machine learning algorithms can use the advantages of quantum computation in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer. On the other hand, one can apply classical methods of machine learning to analyse quantum systems. Most generally, one can consider situations wherein both the learning device and the system under study are fully quantum.
A related branch of research explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks, which has revealed, for example, that certain mathematical and numerical techniques from quantum physics carry over to classical deep learning.Deep learning

Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised.
Some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain. Research attempts to create efficient systems to learn these representations from large-scale, unlabeled data sets.
Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation and bioinformatics where they produced results comparable to and in some cases superior to human experts.List of datasets for machine learning research

These datasets are used for machine-learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce. This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.Statistical classification

In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into "spam" or "non-spam" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.
In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type), ordinal (e.g. "large", "medium" or "small"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term "classifier" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term "classification" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.Online machine learning

In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction. Online learning algorithms may be prone to catastrophic interference. This problem is tackled by incremental learning approaches.Hyperparameter (machine learning)

In the context of machine learning, hyperparameters are parameters whose values are set prior to the commencement of the learning process. By contrast, the values of other parameters are derived via training.
Different model training algorithms require different hyperparameters, some simple algorithms (as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, LASSO is an algorithm that adds a regularization hyperparameter to OLS regression, that has to be set before estimating the parameters through the training algorithm.Supervised learning

Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).
The parallel task in human and animal psychology is often referred to as concept learning.Quantum mechanics

Quantum mechanics (QM; also known as quantum physics or quantum theory), including quantum field theory, is a branch of physics which is the fundamental theory of nature at the smallest scales of energy levels of atoms and subatomic particles.
Classical physics (the physics existing before quantum mechanics) derives from quantum mechanics as an approximation valid only at large (macroscopic) scales. Quantum mechanics differs from classical physics in that: energy, momentum and other quantities are often restricted to discrete values (quantization), objects have characteristics of both particles and waves (i.e. wave-particle duality), and there are limits to the precision with which quantities can be known (uncertainty principle).
Quantum mechanics gradually arose from Max Planck's solution in 1900 to the black-body radiation problem, and Albert Einstein's 1905 paper which offered a quantum-based theory to explain the photoelectric effect. Early quantum theory was profoundly re-conceived in the mid-1920s by Erwin Schrödinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical function, the wave function, provides information about the probability amplitude of position, momentum, and other physical properties of a particle.
Important applications of quantum theory include quantum chemistry, superconducting magnets, light-emitting diodes, and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.Introduction to quantum mechanics

Quantum mechanics is the science of the very small. It explains the behavior of matter and its interactions with energy on the scale of atoms and subatomic particles.
By contrast, classical physics only explains matter and energy on a scale familiar to human experience, including the behavior of astronomical bodies such as the Moon. Classical physics is still used in much of modern science and technology. However, towards the end of the 19th century, scientists discovered phenomena in both the large (macro) and the small (micro) worlds that classical physics could not explain. Coming to terms with these limitations led to two major revolutions in physics which created a shift in the original scientific paradigm: the theory of relativity and the development of quantum mechanics. This article describes how physicists discovered the limitations of classical physics and developed the main concepts of the quantum theory that replaced it in the early decades of the 20th century. These concepts are described in roughly the order in which they were first discovered. For a more complete history of the subject, see History of quantum mechanics.
Light behaves in some respects like particles and in other respects like waves. Matter—the "stuff" of the universe consisting of particles such as electrons and atoms—exhibits wavelike behavior too. Some light sources, including neon lights, give off only certain frequencies of light. Quantum mechanics shows that light, along with all other forms of electromagnetic radiation, comes in discrete units, called photons, and predicts its energies, colors, and spectral intensities. A single photon is a quantum, or smallest observable amount, of the electromagnetic field because a partial photon has never been observed. More broadly, quantum mechanics shows that many quantities, such as angular momentum, that appeared to be continuous in the zoomed-out view of classical mechanics, turn out to be (at the small, zoomed-in scale of quantum mechanics) quantized. Angular momentum is required to take on one of a set of discrete allowable values, and since the gap between these values is so minute, the discontinuity is only apparent at the atomic level.
Many aspects of quantum mechanics are counterintuitive and can seem paradoxical, because they describe behavior quite different from that seen at larger length scales. In the words of quantum physicist Richard Feynman, quantum mechanics deals with "nature as She is – absurd". For example, the uncertainty principle of quantum mechanics means that the more closely one pins down one measurement (such as the position of a particle), the less accurate another measurement pertaining to the same particle (such as its momentum) must become.Interpretations of quantum mechanics

An interpretation of quantum mechanics is a set of statements which attempt to explain how quantum mechanics informs our understanding of nature. Although quantum mechanics has held up to rigorous and thorough experimental testing, many of these experiments are open to different interpretations. There exist a number of contending schools of thought, differing over whether quantum mechanics can be understood to be deterministic, which elements of quantum mechanics can be considered "real", and other matters.
This question is of special interest to philosophers of physics, as physicists continue to show a strong interest in the subject. They usually consider an interpretation of quantum mechanics as an interpretation of the mathematical formalism of quantum mechanics, specifying the physical meaning of the mathematical entities of the theory.Hamiltonian (quantum mechanics)

In quantum mechanics, a Hamiltonian is an operator corresponding to the total energy of the system in most of the cases. It is usually denoted by H, also Ȟ or Ĥ. Its spectrum is the set of possible outcomes when one measures the total energy of a system. Because of its close relation to the time-evolution of a system, it is of fundamental importance in most formulations of quantum theory.
The Hamiltonian is named after William Rowan Hamilton, who also created a revolutionary reformation of Newtonian mechanics, now called Hamiltonian mechanics, that is important in quantum physics.Measurement in quantum mechanics

The framework of quantum mechanics requires a careful definition of measurement. The issue of measurement lies at the heart of the problem of the interpretation of quantum mechanics, for which there is currently no consensus.Quantum statistical mechanics

Quantum statistical mechanics is statistical mechanics applied to quantum mechanical systems. In quantum mechanics a statistical ensemble (probability distribution over possible quantum states) is described by a density operator S, which is a non-negative, self-adjoint, trace-class operator of trace 1 on the Hilbert space H describing the quantum system. This can be shown under various mathematical formalisms for quantum mechanics. One such formalism is provided by quantum logic.Symmetry in quantum mechanics

Symmetries in quantum mechanics describe features of spacetime and particles which are unchanged under some transformation, in the context of quantum mechanics, relativistic quantum mechanics and quantum field theory, and with applications in the mathematical formulation of the standard model and condensed matter physics. In general, symmetry in physics, invariance, and conservation laws, are fundamentally important constraints for formulating physical theories and models. In practice, they are powerful methods for solving problems and predicting what can happen. While conservation laws do not always give the answer to the problem directly, they form the correct constraints and the first steps to solving a multitude of problems.
This article outlines the connection between the classical form of continuous symmetries as well as their quantum operators, and relates them to the Lie groups, and relativistic transformations in the Lorentz group and Poincaré group.Expectation value (quantum mechanics)

In quantum mechanics, the expectation value is the probabilistic expected value of the result (measurement) of an experiment. It is not the most probable value of a measurement; indeed the expectation value may have zero probability of occurring. It is a fundamental concept in all areas of quantum physics.Timeline of quantum mechanics

This timeline of quantum mechanics shows the key steps, precursors and contributors to the development of quantum mechanics, quantum field theories and quantum chemistry.Mathematical formulation of quantum mechanics

The mathematical formulations of quantum mechanics are those mathematical formalisms that permit a rigorous description of quantum mechanics. Such are distinguished from mathematical formalisms for theories developed prior to the early 1900s by the use of abstract mathematical structures, such as infinite-dimensional Hilbert spaces and operators on these spaces. Many of these structures are drawn from functional analysis, a research area within pure mathematics that was influenced in part by the needs of quantum mechanics. In brief, values of physical observables such as energy and momentum were no longer considered as values of functions on phase space, but as eigenvalues; more precisely as spectral values of linear operators in Hilbert space.
These formulations of quantum mechanics continue to be used today. At the heart of the description are ideas of quantum state and quantum observable which are radically different from those used in previous models of physical reality. While the mathematics permits calculation of many quantities that can be measured experimentally, there is a definite theoretical limit to values that can be simultaneously measured. This limitation was first elucidated by Heisenberg through a thought experiment, and is represented mathematically in the new formalism by the non-commutativity of operators representing quantum observables.
Prior to the emergence of quantum mechanics as a separate theory, the mathematics used in physics consisted mainly of formal mathematical analysis, beginning with calculus, and increasing in complexity up to differential geometry and partial differential equations. Probability theory was used in statistical mechanics. Geometric intuition played a strong role in the first two and, accordingly, theories of relativity were formulated entirely in terms of geometric concepts. The phenomenology of quantum physics arose roughly between 1895 and 1915, and for the 10 to 15 years before the emergence of quantum theory (around 1925) physicists continued to think of quantum theory within the confines of what is now called classical physics, and in particular within the same mathematical structures. The most sophisticated example of this is the Sommerfeld–Wilson–Ishiwara quantization rule, which was formulated entirely on the classical phase space.Solar System

The Solar System is the gravitationally bound system comprising the Sun and the objects that orbit it, either directly or indirectly. Of those objects that orbit the Sun directly, the largest eight are the planets, with the remainder being significantly smaller objects, such as dwarf planets and small Solar System bodies. Of the objects that orbit the Sun indirectly, the moons, two are larger than the smallest planet, Mercury.
The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant interstellar molecular cloud. The vast majority of the system's mass is in the Sun, with the majority of the remaining mass contained in Jupiter. The four smaller inner planets, Mercury, Venus, Earth and Mars, are terrestrial planets, being primarily composed of rock and metal. The four outer planets are giant planets, being substantially more massive than the terrestrials. The two largest, Jupiter and Saturn, are gas giants, being composed mainly of hydrogen and helium; the two outermost planets, Uranus and Neptune, are ice giants, being composed mostly of substances with relatively high melting points compared with hydrogen and helium, called volatiles, such as water, ammonia and methane. All eight planets have almost circular orbits that lie within a nearly flat disc called the ecliptic.
The Solar System also contains smaller objects. The asteroid belt, which lies between the orbits of Mars and Jupiter, mostly contains objects composed, like the terrestrial planets, of rock and metal. Beyond Neptune's orbit lie the Kuiper belt and scattered disc, which are populations of trans-Neptunian objects composed mostly of ices, and beyond them a newly discovered population of sednoids. Within these populations are several dozen to possibly tens of thousands of objects large enough that they have been rounded by their own gravity. Such objects are categorized as dwarf planets. Identified dwarf planets include the asteroid Ceres and the trans-Neptunian objects Pluto and Eris. In addition to these two regions, various other small-body populations, including comets, centaurs and interplanetary dust clouds, freely travel between regions. Six of the planets, at least four of the dwarf planets, and many of the smaller bodies are orbited by natural satellites, usually termed "moons" after the Moon. Each of the outer planets is encircled by planetary rings of dust and other small objects.
The solar wind, a stream of charged particles flowing outwards from the Sun, creates a bubble-like region in the interstellar medium known as the heliosphere. The heliopause is the point at which pressure from the solar wind is equal to the opposing pressure of the interstellar medium; it extends out to the edge of the scattered disc. The Oort cloud, which is thought to be the source for long-period comets, may also exist at a distance roughly a thousand times further than the heliosphere. The Solar System is located in the Orion Arm, 26,000 light-years from the center of the Milky Way.Photovoltaic system

A photovoltaic system, also PV system or solar power system, is a power system designed to supply usable solar power by means of photovoltaics. It consists of an arrangement of several components, including solar panels to absorb and convert sunlight into electricity, a solar inverter to change the electric current from DC to AC, as well as mounting, cabling and other electrical accessories to set up a working system. It may also use a solar tracking system to improve the system's overall performance and include an integrated battery solution, as prices for storage devices are expected to decline. Strictly speaking, a solar array only encompasses the ensemble of solar panels, the visible part of the PV system, and does not include all the other hardware, often summarized as balance of system (BOS). Moreover, PV systems convert light directly into electricity and shouldn't be confused with other technologies, such as concentrated solar power or solar thermal, used for heating and cooling.
PV systems range from small, rooftop-mounted or building-integrated systems with capacities from a few to several tens of kilowatts, to large utility-scale power stations of hundreds of megawatts. Nowadays, most PV systems are grid-connected, while off-grid or stand-alone systems only account for a small portion of the market.
Operating silently and without any moving parts or environmental emissions, PV systems have developed from being niche market applications into a mature technology used for mainstream electricity generation. A rooftop system recoups the invested energy for its manufacturing and installation within 0.7 to 2 years and produces about 95 percent of net clean renewable energy over a 30-year service lifetime.
Due to the exponential growth of photovoltaics, prices for PV systems have rapidly declined in recent years. However, they vary by market and the size of the system. In 2014, prices for residential 5-kilowatt systems in the United States were around $3.29 per watt, while in the highly penetrated German market, prices for rooftop systems of up to 100 kW declined to €1.24 per watt. Nowadays, solar PV modules account for less than half of the system's overall cost, leaving the rest to the remaining BOS-components and to soft costs, which include customer acquisition, permitting, inspection and interconnection, installation labor and financing costs.List of Solar System objects by size

This is a partial list of Solar System objects by size, arranged in descending order of mean volumetric radius, and subdivided into several size classes. These lists can also be sorted according to an object's mass and, for the largest objects, volume, density and surface gravity, insofar as these values are available. This list contains the Sun, the planets, dwarf planets, many of the larger small Solar System bodies (which includes the asteroids), all named natural satellites, and a number of smaller objects of historical or scientific interest, such as comets and near-Earth objects.
The ordering may be different depending on whether one chooses radius or mass, because some objects are denser than others. For instance, Uranus is larger than Neptune but less massive, and although Ganymede and Titan are larger than Mercury, they have less than half Mercury's mass. This means some objects in the lower tables, despite their smaller radii, may be more massive than objects in the upper tables because they have a higher density.
Many trans-Neptunian objects (TNOs) have been discovered, and their approximate locations in this list are shown, even though there can be a large uncertainty in their measurement.
Solar System objects more massive than 1021 kilograms (one yottagram [Yg]) are known or expected to be approximately spherical. Astronomical bodies relax into rounded shapes (ellipsoids), achieving hydrostatic equilibrium, when the gravity of their mass is sufficient to overcome the structural strength of their material. Objects made of ice become round more easily than those made of rock, and many icy objects are spheroidal at far lower sizes. The cutoff boundary for roundness is somewhere between 100 km and 200 km in radius.
The larger objects in the mass range between 1018 kg to 1021 kg (1 to 1000 zettagrams [Zg]), such as Tethys, Ceres, and Mimas, have relaxed to an oblate-spheroid equilibrium due to their gravity, whereas the less massive rubble piles (e.g. Amalthea and Janus) are roughly rounded, but not spherical, dubbed "irregular".
Spheroidal bodies typically have some polar flattening due to the centrifugal force from their rotation, and can sometimes even have quite different equatorial diameters (scalene ellipsoids such as Haumea). Unlike bodies such as Haumea, the irregular bodies deviate significantly from the shape of an ellipsoid.
There can be difficulty in determining the diameter (within a factor of about 2) for typical objects beyond Saturn. (See 2060 Chiron as an example.) For TNOs there is some confidence in the diameters, but for non-binary TNOs there is no real confidence in the masses/densities. Many TNOs are often just assumed to have Pluto's density of 2.0 g/cm3, but it is just as likely that they have a comet-like density of only 0.5 g/cm3. For example, if a TNO is poorly assumed to have a mass of 3.59×1020 kg based on a radius of 350 km with a density of 2 g/cm3 and is later discovered to only have a radius of 175 km with a density of 1 g/cm3, the mass estimate would be only 2.24×1019 kg.
The sizes and masses of many of the moons of Jupiter and Saturn are fairly well known due to numerous observations and interactions of the Galileo and Cassini orbiters. But many of the moons with a radius less than ~100 km, such as Jupiter's Himalia, still have unknown masses. Again, as we get further from the Sun than Saturn, things get less clear. There has not yet been an orbiter around Uranus or Neptune for long-term study of their moons. For the small outer irregular moons of Uranus, such as Sycorax, which were not discovered by the Voyager 2 flyby, even different NASA web pages, such as the National Space Science Data Center and JPL Solar System Dynamics, have somewhat contradictory size and albedo estimates depending on which research paper is being cited.
Data for objects has varying reliability including uncertainties in the figures for mass and radius, and irregularities in the shape and density, with accuracy often depending on how close it is to Earth or whether it has been visited by a probe.List of Solar System objects

The following is a list of Solar System objects by orbit, ordered by increasing distance from the Sun. Most named objects in this list have a diameter of 500 km or more.
The Sun, a spectral class G2V main-sequence star
The inner Solar System and the terrestrial planets
Mercury
Mercury-crosser asteroids

Venus
Venus-crosser asteroids
2002 VE68, Venus's quasi-satellite

Earth
Moon
Near-Earth asteroids (including 99942 Apophis)
Earth trojan (2010 TK7)
Earth-crosser asteroids
Earth's quasi-satellites

Mars
Deimos
Phobos
Mars trojans
Mars-crosser asteroids

Asteroids in the asteroid belt, between the orbits of Mars and Jupiter
Ceres, a dwarf planet
Pallas
Vesta
Hygiea
Asteroids number in the hundreds of thousands. For longer lists, see list of notable asteroids, list of asteroids, or list of objects by mass.
Asteroid moons

A number of smaller groups distinct from the asteroid belt

The outer Solar System with the giant planets, their satellites, trojan asteroids and some minor planets
Jupiter
Rings of Jupiter
Complete list of Jupiter's natural satellites
Io
Europa
Ganymede
Callisto

Jupiter trojans
Jupiter-crossing minor planets

Saturn
Rings of Saturn
Complete list of Saturn's natural satellites
Mimas
Enceladus
Tethys (trojans: Telesto and Calypso)
Dione (trojans: Helene and Polydeuces)
Rhea
Rings of Rhea

Titan
Hyperion
Iapetus
Phoebe

Saturn-crossing minor planets

Uranus
Rings of Uranus
Complete list of Uranus's natural satellites
Miranda
Ariel
Umbriel
Titania
Oberon

Uranus trojan (2011 QF99)
Uranus-crossing minor planets

Neptune
Rings of Neptune
Complete list of Neptune's natural satellites
Proteus
Triton
Nereid

Neptune trojans
Neptune-crossing minor planets

Non-trojan minor planets
Centaurs
Damocloids

Trans-Neptunian objects (beyond the orbit of Neptune)
Kuiper-belt objects (KBOs)
Plutinos
Pluto, a dwarf planet
Complete list of Pluto's natural satellites
Charon

90482 Orcus
Vanth

Twotinos
Cubewanos (classical objects)
Haumea, a dwarf planet
Namaka
Hi'iaka

50000 Quaoar
Weywot

120347 Salacia
20000 Varuna
Makemake, a dwarf planet

Scattered-disc objects
Eris, a dwarf planet
Dysnomia

(225088) 2007 OR10
(84522) 2002 TC302
(87269) 2000 OO67

Detached objects
2004 XR190
90377 Sedna (possibly inner Oort cloud)
2012 VP113 (possibly inner Oort cloud)

Oort cloud (hypothetical)
Hills cloud/inner Oort cloud
Outer Oort cloud

The Solar System also contains:
Comets (icy bodies with eccentric orbits)
List of periodic comets
List of non-periodic comets

Small objects, including:
Meteoroids
Interplanetary dust
Helium focusing cone, around the Sun

Human-made objects orbiting the Sun, Mercury, Venus, Earth, Mars, and Saturn, including active artificial satellites and space junk

Heliosphere, a bubble in space produced by the solar wind
Heliosheath
Heliopause
Hydrogen wall, a pile up of hydrogen from the interstellar mediumFormation and evolution of the Solar System

The formation and evolution of the Solar System began 4.6 billion years ago with the gravitational collapse of a small part of a giant molecular cloud. Most of the collapsing mass collected in the center, forming the Sun, while the rest flattened into a protoplanetary disk out of which the planets, moons, asteroids, and other small Solar System bodies formed.
This model, known as the nebular hypothesis, was first developed in the 18th century by Emanuel Swedenborg, Immanuel Kant, and Pierre-Simon Laplace. Its subsequent development has interwoven a variety of scientific disciplines including astronomy, physics, geology, and planetary science. Since the dawn of the space age in the 1950s and the discovery of extrasolar planets in the 1990s, the model has been both challenged and refined to account for new observations.
The Solar System has evolved considerably since its initial formation. Many moons have formed from circling discs of gas and dust around their parent planets, while other moons are thought to have formed independently and later been captured by their planets. Still others, such as Earth's Moon, may be the result of giant collisions. Collisions between bodies have occurred continually up to the present day and have been central to the evolution of the Solar System. The positions of the planets often shifted due to gravitational interactions. This planetary migration is now thought to have been responsible for much of the Solar System's early evolution.
In roughly 5 billion years, the Sun will cool and expand outward to many times its current diameter (becoming a red giant), before casting off its outer layers as a planetary nebula and leaving behind a stellar remnant known as a white dwarf. In the far distant future, the gravity of passing stars will gradually reduce the Sun's retinue of planets. Some planets will be destroyed, others ejected into interstellar space. Ultimately, over the course of tens of billions of years, it is likely that the Sun will be left with none of the original bodies in orbit around it.List of gravitationally rounded objects of the Solar System

Gravitationally rounded objects in the Solar System have a rounded, ellipsoidal shape due to the forces of their own gravity (hydrostatic equilibrium) and their sizes range from dwarf planets and moons to the planets and the sun. This list does not include any small Solar System bodies, but it does include a sample of planetary-mass objects whose shape has yet to be accurately determined. The Sun's orbital characteristics are listed in relation to the Galactic Center, while all other objects are listed in order of their distance from the Sun.Small Solar System body

A Small Solar System Body (SSSB) is an object in the Solar System that is neither a planet, nor a dwarf planet, nor a natural satellite. The term was first defined in 2006 by the International Astronomical Union.

All other objects, except satellites, orbiting the Sun shall be referred to collectively as "Small Solar System Bodies" ... These currently include most of the Solar System asteroids, most Trans-Neptunian Objects (TNOs), comets, and other small bodies.

This encompasses all comets and all minor planets other than those that are dwarf planets. Thus SSSBs are: the classical asteroids with the exception of the dwarf planet Ceres; the trojans; and the centaurs and trans-Neptunian objects with the exception of Pluto, Haumea, Makemake, Eris, and others that may turn out to be dwarf planets.Passive solar building design

In passive solar building design, windows, walls, and floors are made to collect, store, and distribute solar energy in the form of heat in the winter and reject solar heat in the summer. This is called passive solar design because, unlike active solar heating systems, it does not involve the use of mechanical and electrical devices.
The key to design a passive solar building is to best take advantage of the local climate performing an accurate site analysis. Elements to be considered include window placement and size, and glazing type, thermal insulation, thermal mass, and shading. Passive solar design techniques can be applied most easily to new buildings, but existing buildings can be adapted or "retrofitted".Solar power

Solar power is the conversion of energy from sunlight into electricity, either directly using photovoltaics (PV), indirectly using concentrated solar power, or a combination. Concentrated solar power systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Photovoltaic cells convert light into an electric current using the photovoltaic effect.
Photovoltaics were initially solely used as a source of electricity for small and medium-sized applications, from the calculator powered by a single solar cell to remote homes powered by an off-grid rooftop PV system. Commercial concentrated solar power plants were first developed in the 1980s. The 392 MW Ivanpah installation is the largest concentrating solar power plant in the world, located in the Mojave Desert of California.
As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Solar PV is rapidly becoming an inexpensive, low-carbon technology to harness renewable energy from the Sun. The current largest photovoltaic power station in the world is the 850 MW Longyangxia Dam Solar Park, in Qinghai, China.
The International Energy Agency projected in 2014 that under its "high renewables" scenario, by 2050, solar photovoltaics and concentrated solar power would contribute about 16 and 11 percent, respectively, of the worldwide electricity consumption, and solar would be the world's largest source of electricity. Most solar installations would be in China and India. Currently, as of 2016, solar power provides just 1% of total worldwide electricity production but growing 33% per annum.List of tallest mountains in the Solar System

This is a list of the tallest mountains in the Solar System. The tallest peak or peaks on worlds where significant mountains have been measured are given; in some cases, the tallest peaks of different classes on a world are also listed. At 21.9 km, the enormous shield volcano Olympus Mons on Mars is the tallest mountain on any planet. For 40 years, following its discovery in 1971, it was the tallest mountain known in the Solar System. However, in 2011, the central peak of the crater Rheasilvia on the asteroid and protoplanet Vesta was found to be of comparable height.Source code

In computing, source code is any collection of computer instructions, possibly with comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.
Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.Source Code

In computing, source code is any collection of computer instructions, possibly with comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.
Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.Source code editor

A source code editor is a text editor program designed specifically for editing source code of computer programs by programmers. It may be a standalone application or it may be built into an integrated development environment (IDE) or web browser. Source code editors are the most fundamental programming tool, as the fundamental job of programmers is to write and edit source code.Source code escrow

Source code escrow is the deposit of the source code of software with a third party escrow agent. Escrow is typically requested by a party licensing software (the licensee), to ensure maintenance of the software instead of abandonment or orphaning. The software source code is released to the licensee if the licensor files for bankruptcy or otherwise fails to maintain and update the software as promised in the software license agreement.Source-to-source compiler

A source-to-source compiler, transcompiler or transpiler is a type of compiler that takes the source code of a program written in one programming language as its input and produces the equivalent source code in another programming language. A source-to-source compiler translates between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language. For example, a source-to-source compiler may perform a translation of a program from Pascal to C. An automatic parallelizing compiler will frequently take in a high level language program as an input and then transform the code and annotate it with parallel code annotations (e.g., OpenMP) or language constructs (e.g. Fortran's forall statements).
Another purpose of source-to-source-compiling is translating legacy code to use the next version of the underlying programming language or an API that breaks backward compatibility. It will perform automatic code refactoring which is useful when the programs to refactor are outside the control of the original implementer (for example, converting programs from Python 2 to Python 3, or converting programs from an old API to the new API) or when the size of the program makes it impractical or time consuming to refactor it by hand.
Transcompilers may either keep translated code as close to the source code as possible to ease development and debugging of the original source code, or may change the structure of the original code so much that the translated code does not look like the source code. There are also debugging utilities that map the transpiled source code back to the original code; for example, the JavaScript Source Map standard allows mapping of the JavaScript code executed by a web browser back to the original source in a transpiled-to-JavaScript language.
Examples of transcompiled languages include Closure Compiler, Coccinelle, CoffeeScript, Dart, Haxe, Nim, TypeScript and Emscripten.Source lines of code

Source lines of code (SLOC), also known as lines of code (LOC), is a software metric used to measure the size of a computer program by counting the number of lines in the text of the program's source code. SLOC is typically used to predict the amount of effort that will be required to develop a program, as well as to estimate programming productivity or maintainability once the software is produced.Open-source software

Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. According to scientists who studied it, open-source software is a prominent example of open collaboration. The term is often written without a hyphen as "open source software".
Open-source software development, or collaborative development from multiple independent sources, generates an increasingly more diverse scope of design perspective than any one company is capable of developing and sustaining long term. A 2008 report by the Standish Group states that adoption of open-source software models has resulted in savings of about $60 billion (£48 billion) per year to consumers.Source Code Control System

Source Code Control System (SCCS) is a version control system designed to track changes in source code and other text files during the development of a piece of software. This allows the user to retrieve any of the previous versions of the original source code and the changes which are stored. It was originally developed at Bell Labs in 1972 by Marc Rochkind for an IBM System/370 computer running OS/360.
SCCS is also known for the sccsid string, for example:

This string contains the file name, date, and can also contain a comment. After compilation, the string can be found in binary and object files by looking for the pattern "@(#)" and can be used to determine which source code files were used during compilation. The "what" command [1] is available to automate this search for version strings.Source Code Pro

Source Code Pro is a monospaced sans serif typeface created by Paul D. Hunt for Adobe Systems. It is the second open-source font family from Adobe, distributed under the SIL Open Font License.Repository (version control)

In revision control systems, a repository is an on-disk data structure which stores metadata for a set of files or directory structure. Depending on whether the version control system in use is distributed (for instance, Git or Mercurial) or centralized (Subversion or Perforce, for example), the whole set of information in the repository may be duplicated on every user's system or may be maintained on a single server. Some of the metadata that a repository contains includes, among other things:
A historical record of changes in the repository.
A set of commit objects.
A set of references to commit objects, called heads.History of Earth

The history of Earth concerns the development of planet Earth from its formation to the present day. Nearly all branches of natural science have contributed to the understanding of the main events of Earth's past. The age of the Earth is approximately one-third of the age of the universe. An immense amount of geological change has occurred in that timespan, accompanied by the emergence of life and its subsequent evolution.
Earth formed around 4.54 billion years ago by accretion from the solar nebula. Volcanic outgassing probably created the primordial atmosphere and then the ocean, but the early atmosphere contained almost no oxygen and so would not have supported known forms of life. Much of the Earth was molten because of frequent collisions with other bodies which led to extreme volcanism. A giant impact collision with a planet-sized body named Theia while Earth was in its earliest stage, also known as Early Earth, is thought to have been responsible for forming the Moon. Over time, the Earth cooled, causing the formation of a solid crust, and allowing liquid water to exist on the surface.
The geological time scale (GTS) depicts the larger spans of time, from the beginning of the Earth to the present, and it chronicles some definitive events of Earth history. The Hadean eon represents time before the reliable (fossil) record of life beginning on Earth; it began with the formation of the planet and ended at 4.0 billion years ago as defined by international convention. The Archean and Proterozoic eons follow; they produced the abiogenesis of life on Earth and then the evolution of early life. The succeeding eon is the Phanerozoic, which is represented by its three component eras: the Palaeozoic; the Mesozoic, which spanned the rise, reign, and climactic extinction of the non-avian dinosaurs; and the Cenozoic, which presented the subsequent development of dominant mammals on Earth.
Hominins, the earliest direct ancestors of the human clade, rose sometime during the latter part of the Miocene epoch; the precise time marking the first hominins is broadly debated over a current range of 13 to 4 million years ago. The succeeding Quaternary period is the time of recognizable humans, i.e., the genus Homo, but that period's two million-year-plus term of the recent times is too small to be visible at the scale of the GTS graphic. (Notes re the graphic: Ga means "billion years"; Ma, "million years".)
The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils such as stromatolites found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in southwestern Greenland as well as "remains of biotic life" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth … then it could be common in the universe."
Photosynthetic organisms appeared between 3.2 and 2.4 billion years ago and began enriching the atmosphere with oxygen. Life remained mostly small and microscopic until about 580 million years ago, when complex multicellular life arose, developed over time, and culminated in the Cambrian Explosion about 541 million years ago. This event drove a rapid diversification of life forms on Earth that produced most of the major phyla known today, and it marked the end of the Proterozoic Eon and the beginning of the Cambrian Period of the Paleozoic Era. More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million are documented, but over 86 percent have not been described. Scientists recently reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.
The Earth's crust has constantly changed since its formation. Likewise, life has constantly changed since its first appearance. Species continue to evolve, taking on new forms, splitting into daughter species or going extinct in the process of adapting or dying in response to ever-changing physical environments. The process of plate tectonics continues to shape the Earth's continents and oceans and the life they harbor. Human activity is now a dominant force affecting global change, adversely affecting the biosphere, the Earth's surface, hydrosphere, and atmosphere, with the loss of wild lands, over-exploitation of the oceans, production of greenhouse gases, degradation of the ozone layer, and general degradation of soil, air, and water quality.Geological history of Earth

The geological history of Earth follows the major events in Earth's past based on the geologic time scale, a system of chronological measurement based on the study of the planet's rock layers (stratigraphy). Earth formed about 4.54 billion years ago by accretion from the solar nebula, a disk-shaped mass of dust and gas left over from the formation of the Sun, which also created the rest of the Solar System.
Earth was initially molten due to extreme volcanism and frequent collisions with other bodies. Eventually, the outer layer of the planet cooled to form a solid crust when water began accumulating in the atmosphere. The Moon formed soon afterwards, possibly as a result of the impact of a planetoid with the Earth. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, augmented by ice delivered from comets, produced the oceans.
As the surface continually reshaped itself over hundreds of millions of years, continents formed and broke apart. They migrated across the surface, occasionally combining to form a supercontinent. Roughly 750 million years ago, the earliest-known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia, 600 to 540 million years ago, then finally Pangaea, which broke apart 200 million years ago.
The present pattern of ice ages began about 40 million years ago, then intensified at the end of the Pliocene. The polar regions have since undergone repeated cycles of glaciation and thaw, repeating every 40,000–100,000 years. The last glacial period of the current ice age ended about 10,000 years ago.The History of Middle-earth

The History of Middle-earth is a 12-volume series of books published between 1983 and 1996 that collect and analyse material relating to the fiction of J. R. R. Tolkien, compiled and edited by his son, Christopher Tolkien. The series shows the development over time of Tolkien's conception of Middle-earth as a fictional place with its own peoples, languages, and history, from his earliest notions of a "mythology for England" through to the development of the stories that make up The Silmarillion and The Lord of the Rings. It is not a "history of Middle-earth" in the sense of being a chronicle of events in Middle-earth written from an in-universe perspective. In 2000–01, the twelve volumes were republished in three limited edition omnibus volumes. Non-deluxe editions of the three volumes were published in 2002.World population

In demographics, the world population is the total number of humans currently living. The world population was estimated to have reached 7.6 billion as of October 2017 . The United Nations estimates it will further increase to 11.2 billion by the year 2100.
World population has experienced continuous growth since the end of the Great Famine of 1315–17 and the Black Death in 1350, when it was near 370 million. The highest population growth rates – global population increases above 1.8% per year – occurred between 1955-1975 peaking to 2.06% between 1965-1970. The growth rate has declined to 1.18% between 2010-2015 and is projected to decline to 0.13% by the year 2100. Total annual births were highest in the late 1980s at about 139 million, and are now expected to remain essentially constant at their 2011 level of 135 million, while deaths number 56 million per year and are expected to increase to 80 million per year by 2040. World population reached 7 billion on October 31, 2011 according to the United Nations Population Fund, and on March 12, 2012 according to the United States Census Bureau.
The median age of the world's population was estimated to be 30.1 years in 2016, with the male median age estimated to be 29.4 years and female, 30.9 years.
The 2012 UN projections show a continued increase in population in the near future with a steady decline in population growth rate; the global population is expected to reach between 8.3 and 10.9 billion by 2050. 2003 UN Population Division population projections for the year 2150 range between 3.2 and 24.8 billion. One of many independent mathematical models supports the lower estimate, while a 2014 estimate forecasts between 9.3 and 12.6 billion in 2100, and continued growth thereafter. Some analysts have questioned the sustainability of further world population growth, highlighting the growing pressures on the environment, global food supplies, and energy resources.
Estimates on the total number of humans who have ever lived range in the order of 106 to 108 billion.Earth

Earth is the third planet from the Sun and the only object in the Universe known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.
Earth's axis of rotation is tilted, producing seasonal variations on the planet's surface. The gravitational interaction between the Earth and Moon causes ocean tides, stabilizes the Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.
Earth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by its oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.
Within the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.4 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.The World's Billionaires

The World's Billionaires is an annual ranking by net worth of the world's wealthiest billionaires compiled and published in March annually by the American business magazine Forbes. The list was first published in March 1987. The total net worth of each individual on the list is estimated, in United States dollars, based on their assets and accounting for debt. Royalty and dictators whose wealth comes from their positions are excluded from these lists.
In 2017, there was a record of 2,043 people on the list, which is the first time over 2,000 people were listed, that included 195 newcomers that included 76 from China and 25 from the U.S.; there were 56 people under 40 and it had a record of 227 women. The average net worth of the list came in at US$3.75 billion, down US$110 million from 2015. Added together, the total net worth for 2017's billionaires was US$7.67 trillion, up from US$7.1 trillion in 2015. As of 2017, Microsoft founder Bill Gates has topped the list 18 of the past 23 years.
According to a 2017 Oxfam report, the top eight billionaires own as much combined wealth as "half the human race".Middle-earth: Shadow of War

Middle-earth: Shadow of War is an action role-playing game developed by Monolith Productions and published by Warner Bros. Interactive Entertainment. It is the sequel to 2014's Middle-earth: Shadow of Mordor, and was released worldwide for Microsoft Windows, PlayStation 4, and Xbox One on October 10, 2017.
Shadow of War continues the previous game's narrative, which is based on J. R. R. Tolkien's legendarium and set between the events of The Hobbit and The Lord of the Rings. Like its predecessor, the game also takes heavy inspiration from director Peter Jackson's The Hobbit and The Lord of the Rings film adaptations. The player continues the story of the ranger Talion and the spirit of the elf lord Celebrimbor, who shares Talion's body, as they forge a new Ring of Power to amass an army to fight against Sauron. The game builds upon the "Nemesis System" introduced in Shadow of Mordor, allowing Talion to gain followers from several races of Middle-earth, including Uruks and Ologs, and plan out complex strategies using these to complete missions.
Shadow of War was well-received by critics, with praise aimed towards gameplay and an improved nemesis system, although story elements and changes made to established characters received some negative reactions, as well as the inclusion of microtransactions and loot boxes. A free-to-play companion game for iOS and Android devices was also released.List of DC Multiverse worlds

The DC Multiverse is a fictional continuity construct that is used in DC Comics publications. The Multiverse has undergone numerous changes and has included various universes, listed below between the original Multiverse and its successors.Moon

The Moon is an astronomical body that orbits planet Earth, being Earth's only permanent natural satellite. It is the fifth-largest natural satellite in the Solar System, and the largest among planetary satellites relative to the size of the planet that it orbits (its primary). Following Jupiter's satellite Io, the Moon is second-densest satellite among those whose densities are known.
The Moon is thought to have formed about 4.51 billion years ago, not long after Earth. The most widely accepted explanation is that the Moon formed from the debris left over after a giant impact between Earth and a Mars-sized body called Theia.
The Moon is in synchronous rotation with Earth, always showing the same face, with its near side marked by dark volcanic maria that fill the spaces between the bright ancient crustal highlands and the prominent impact craters. As seen from the Earth, it is the second-brightest regularly visible celestial object in Earth's sky, after the Sun. Its surface is actually dark, although compared to the night sky it appears very bright, with a reflectance just slightly higher than that of worn asphalt. Its gravitational influence produces the ocean tides, body tides, and the slight lengthening of the day.
The Moon's average orbital distance at the present time is 384,402 km (238,856 mi), or 1.28 light-seconds. This is about thirty times the diameter of Earth, with its apparent size in the sky almost the same as that of the Sun (due to it being 400x farther and larger), resulting in the Moon covering the Sun nearly precisely in total solar eclipse. This matching of apparent visual size will not continue in the far future, because the Moon's distance from Earth is slowly increasing.
The Soviet Union's Luna programme was the first to reach the Moon with uncrewed spacecraft in 1959; the United States' NASA Apollo program achieved the only crewed missions to date, beginning with the first crewed lunar orbiting mission by Apollo 8 in 1968, and six crewed lunar landings between 1969 and 1972, with the first being Apollo 11. These missions returned lunar rocks which have been used to develop a geological understanding of the Moon's origin, internal structure, and later history. Since the Apollo 17 mission in 1972, the Moon has been visited only by uncrewed spacecraft.
Within human culture, both the Moon's natural prominence in the earthly sky, and its regular cycle of phases as seen from the Earth have provided cultural references and influences for human societies and cultures since time immemorial. Such cultural influences can be found in language, lunar based calendar systems, art, and mythology.History

History (from Greek ἱστορία, historia, meaning "inquiry, knowledge acquired by investigation") is the study of the past as it is described in written documents. Events occurring before written record are considered prehistory. It is an umbrella term that relates to past events as well as the memory, discovery, collection, organization, presentation, and interpretation of information about these events. Scholars who write about history are called historians.
History can also refer to the academic discipline which uses a narrative to examine and analyse a sequence of past events, and objectively determine the patterns of cause and effect that determine them. Historians sometimes debate the nature of history and its usefulness by discussing the study of the discipline as an end in itself and as a way of providing "perspective" on the problems of the present.
Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends, because they do not show the "disinterested investigation" required of the discipline of history. Herodotus, a 5th-century BC Greek historian is considered within the Western tradition to be the "father of history", and, along with his contemporary Thucydides, helped form the foundations for the modern study of human history. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In Asia, a state chronicle, the Spring and Autumn Annals was known to be compiled from as early as 722 BC although only 2nd-century BC texts survived.
Ancient influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematical elements of historical investigation. Often history is taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.Debugging

Debugging is the process of finding and resolving of defects or problem within the program that prevent correct operation of computer software or a system.
Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling.Debugger

A debugger or debugging tool is a computer program that is used to test and debug other programs (the "target" program). The code to be examined might alternatively be running on an instruction set simulator (ISS), a technique that allows great power in its ability to halt when specific conditions are encountered, but which will typically be somewhat slower than executing the code directly on the appropriate (or the same) processor. Some debuggers offer two modes of operation, full or partial simulation, to limit this impact.
A "trap" occurs when the program cannot normally continue because of a programming bug or invalid data. For example, the program might have tried to use an instruction not available on the current version of the CPU or attempted to access unavailable or protected memory. When the program "traps" or reaches a preset condition, the debugger typically shows the location in the original code if it is a source-level debugger or symbolic debugger, commonly now seen in integrated development environments. If it is a low-level debugger or a machine-language debugger it shows the line in the disassembly (unless it also has online access to the original source code and can display the appropriate section of code from the assembly or compilation).Debug (command)

debug is a command in DOS, OS/2 and Microsoft Windows (only in 32bit) which runs the program debug.exe (or DEBUG.COM in older versions of DOS). Debug can act as an assembler, disassembler, or hex dump program allowing users to interactively examine memory contents (in assembly language, hexadecimal or ASCII), make changes, and selectively execute COM, EXE and other file types. It also has several subcommands which are used to access specific disk sectors, I/O ports and memory addresses.Debug menu

A debug menu or debug mode is a user interface implemented in a computer program that allows the user to view and/or manipulate the program's internal state for the purpose of debugging. Some games format their debug menu as an in-game location, referred to as a debug room (distinct from the developer's room type of Easter egg). Debug menus and rooms are used during software development for ease of testing and are usually made inaccessible or otherwise hidden from the end user.
Compared to the normal user interfaces, debug menus usually are unpolished and not user-friendly, intended only to be used by the software's developers. They are often cryptic and may allow for destructive actions such as erasing data without warning.
Debug menus are often of interest to video game players as they can be used to cheat, access unused content, or change the game configuration beyond what is normally allowed. For example, a debug menu in Super Mario 64 can be used to instantly award all gold trophies or to play the game in a 2-player vertical split-screen mode which is not normally available. Some game developers will reveal methods to access these menus as bonus features, while others may lock them out of the final version entirely such that they can only be accessed by modifying the program.
Debugging functions can be found in many other programs and consumer electronics as well. For example, many TVs and DVD players contain hidden menus that can be used to change settings that aren't accessible through the normal menus. Many cell phones also contain debug menus, usually used to test out functions of the phone to make sure they are working. For example, the hidden menu of the Samsung Galaxy S III has test functions for the vibrator, proximity sensor, sound, and other basic aspects of the phone.Rubber duck debugging

In software engineering, rubber duck debugging or rubber ducking is a method of debugging code. The name is a reference to a story in the book The Pragmatic Programmer in which a programmer would carry around a rubber duck and debug their code by forcing themselves to explain it, line-by-line, to the duck. Many other terms exist for this technique, often involving different inanimate objects.
Many programmers have had the experience of explaining a programming problem to someone else, possibly even to someone who knows nothing about programming, and then hitting upon the solution in the process of explaining the problem. In describing what the code is supposed to do and observing what it actually does, any incongruity between these two becomes apparent. More generally, teaching a subject forces its evaluation from different perspectives and can provide a deeper understanding. By using an inanimate object, the programmer can try to accomplish this without having to interrupt anyone else.GNU Debugger

The GNU Debugger (GDB) is a portable debugger that runs on many Unix-like systems and works for many programming languages, including Ada, C, C++, Objective-C, Free Pascal, Fortran, Java and partially others.Computer programming

Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.JTAG

The Joint Test Action Group (JTAG) is an electronics industry association formed in 1985 for developing a method of verifying designs and testing printed circuit boards after manufacture. In 1990 the Institute of Electrical and Electronics Engineers codified the results of the effort in IEEE Standard 1149.1-1990, entitled Standard Test Access Port and Boundary-Scan Architecture.
JTAG implements standards for on-chip instrumentation in electronic design automation (EDA) as a complementary tool to digital simulation. It specifies the use of a dedicated debug port implementing a serial communications interface for low-overhead access without requiring direct external access to the system address and data buses. The interface connects to an on-chip test access port (TAP) that implements a stateful protocol to access a set of test registers that present chip logic levels and device capabilities of various parts.
The JTAG standards have been extended by many semiconductor chip manufacturers with specialized variants to provide vendor-specific features.Debug symbol

A debug symbol is a special kind of symbol that attaches additional information to the symbol table of a object file, such as a shared library or an executable. This information allows a symbolic debugger to gain access to information from the source code of the binary, such as the names of identifiers, including variables and routines.
The symbolic information may be compiled together with the module's binary file, or distributed in a separate file, or simply discarded during the compilation and/or linking.
This information can be helpful while trying to investigate and fix a crashing application or any other fault.Background debug mode interface

Background debug mode (BDM) interface is an electronic interface that allows debugging of embedded systems. Specifically, it provides in-circuit debugging functionality in microcontrollers. It requires a single wire and specialized electronics in the system being debugged. It appears in many Freescale Semiconductor products.
The interface allows a Host to manage and query a target. Specialized hardware is required in the target device. No special hardware is required in the host; a simple biidirectional I/O pin is sufficient.MySQL

MySQL (officially pronounced as  "My S-Q-L",) is an open-source relational database management system (RDBMS). Its name is a combination of "My", the name of co-founder Michael Widenius's daughter, and "SQL", the abbreviation for Structured Query Language. The MySQL development project has made its source code available under the terms of the GNU General Public License, as well as under a variety of proprietary agreements. MySQL was owned and sponsored by a single for-profit firm, the Swedish company MySQL AB, now owned by Oracle Corporation. For proprietary use, several paid editions are available, and offer additional functionality.
MySQL is a central component of the LAMP open-source web application software stack (and other "AMP" stacks). LAMP is an acronym for "Linux, Apache, MySQL, Perl/PHP/Python". Applications that use the MySQL database include: TYPO3, MODx, Joomla, WordPress, phpBB, MyBB, and Drupal. MySQL is also used in many high-profile, large-scale websites, including Google (though not for searches), Facebook, Twitter, Flickr, and YouTube.MySQL Cluster

MySQL Cluster is a technology providing shared-nothing clustering and auto-sharding for the MySQL database management system. It is designed to provide high availability and high throughput with low latency, while allowing for near linear scalability. MySQL Cluster is implemented through the NDB or NDBCLUSTER storage engine for MySQL ("NDB" stands for Network Database).MySQLi

The MySQLi Extension (MySQL Improved) is a relational database driver used in the PHP scripting language to provide an interface with MySQL databases.
There are three main API options when considering connecting to a MySQL database server:
PHP's MySQL Extension
PHP's MySQLi Extension
PHP Data Objects (PDO)
The PHP code consists of a core, with optional extensions to the core functionality. PHP's MySQL-related extensions, such as the MySQLi extension, and the MySQL extension, are implemented using the PHP extension framework. An extension typically exposes an API to the PHP developer, to allow its facilities to be used programmatically. However, some extensions which use the PHP extension framework do not expose an API to the PHP developer.
The PDO MySQL driver extension, for example, does not expose an API to the PHP developer, but provides an interface to the PDO layer above it.
MySQLi is an improved version of the older PHP MySQL driver, offering various benefits.
The authors of the PHP scripting language recommend using MySQLi when dealing with MySQL server versions 4.1.3 and newer (takes advantage of new functionality).MySQL Workbench

MySQL Workbench is a visual database design tool that integrates SQL development, administration, database design, creation and maintenance into a single integrated development environment for the MySQL database system. It is the successor to DBDesigner 4 from fabFORCE.net, and replaces the previous package of software, MySQL GUI Tools Bundle.MySQL AB

MySQL AB was a Swedish software company founded in 1995. It was acquired by Sun Microsystems in 2008; Sun was in turn acquired by Oracle Corporation in 2010. MySQL AB is the creator of MySQL, a relational database management system, as well as related products such as MySQL Cluster. The company was dually headquartered in Uppsala, Sweden and Cupertino, California with offices in other countries (France (Paris), Germany (Munich), Ireland (Dublin), Italy (Milan); and Japan (Tokyo)). With around 400 employees in 25 countries, MySQL AB was one of the largest open source companies. Around 70% of the employees worked for MySQL from their home offices.
Together with Linux, Apache, and PHP, the MySQL Server forms one of the building blocks of the LAMP technology stack. The company claimed over 5 million MySQL installations and over 10 million product downloads in 2004.SQL injection

SQL injection is a code injection technique, used to attack data-driven applications, in which nefarious SQL statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker). SQL injection must exploit a security vulnerability in an application's software, for example, when user input is either incorrectly filtered for string literal escape characters embedded in SQL statements or user input is not strongly typed and unexpectedly executed. SQL injection is mostly known as an attack vector for websites but can be used to attack any type of SQL database.
SQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues such as voiding transactions or changing balances, allow the complete disclosure of all data on the system, destroy the data or make it otherwise unavailable, and become administrators of the database server.
In a 2012 study, it was observed that the average web application received 4 attack campaigns per month, and retailers received twice as many attacks as other industries.//endOutline of MySQL

The following outline is provided as an overview of and topical guide to MySQL:
MySQL ("My Structured Query Language") – world's second most widely used relational database management system (RDBMS) and most widely used open-source RDBMS. It is named after co-founder Michael Widenius's daughter, My.MySQL Enterprise

MySQL Enterprise is a subscription-based service produced by Oracle Corporation and targeted toward the commercial market. Oracle's official support, training and certification focus on MySQL Enterprise.
MySQL Enterprise contains
MySQL Enterprise Server software, a distribution of the MySQL Server
MySQL Enterprise Monitor
MySQL Enterprise Backup
MySQL Enterprise Audit
MySQL Enterprise Firewall
MySQL Workbench Standard Edition
Production Support
New versions of MySQL Enterprise Server are released monthly as Rapid Updates (MRUs), and quarterly as Service Packs (QSPs).LAMP (software bundle)

LAMP is an archetypal model of web service stacks, named as an acronym of the names of its original four open-source components: the Linux operating system, the Apache HTTP Server, the MySQL relational database management system (RDBMS), and the PHP programming language. The LAMP components are largely interchangeable and not limited to the original selection. As a solution stack, LAMP is suitable for building dynamic web sites and web applications.
Since its creation, the LAMP model has been adapted to other componentry, though typically consisting of free and open-source software. For example, an equivalent installation on the Microsoft Windows family of operating systems is known as WAMP and an equivalent installation on macOS is known as MAMP.PhpMyAdmin

phpMyAdmin is a free and open source administration tool for MySQL and MariaDB. As a portable web application written primarily in PHP, it has become one of the most popular MySQL administration tools, especially for web hosting services.Abiogenesis

Abiogenesis (British English: , ), biopoiesis, or informally the origin of life, is the natural process by which life arises from non-living matter, such as simple organic compounds. On Earth, the transition from non-living to living entities was not a single event but a process of increasing complexity. Abiogenesis is studied through a combination of paleontology, chemistry, and extrapolation from the characteristics of modern organisms, and aims to determine how pre-life chemical reactions gave rise to life on Earth.
The study of abiogenesis can be geophysical, chemical, or biological, with more recent approaches attempting a synthesis of all three, as life arose under conditions that are strikingly different from those on Earth today. Life itself is dependent upon the specialized chemistry of carbon and water and is largely based upon five different families of chemicals. Lipids are fatty molecules comprising large chemical chains of hydrocarbons and play an important role in the structure of living cell membranes, actively and passively determining the transport of other molecules into and out of cells. Carbohydrates are sugars, and as monomer units can be assembled into polymers called polysaccharides, such as cellulose, the rigid chemical of most plant cell walls. Nitrogenous bases are organic molecules in which the amine group of nitrogen, combined with two hydrogen atoms, plays an important part. Chlorophyll is based upon a porphyrin ring derived from amine monomer units, and is important in the capture of the energy needed for life. Nucleic acid monomers are made from a carbohydrate monosaccharide, a nitrogenous base and one or more high energy phosphate groups. When joined together they form the unit of inheritance, the gene, made from DNA or RNA, which translates the genetic information into protein structures. The monomer unit of a protein is usually one of 20 amino acids, comprising an amine group, a hydrocarbon, and a carboxylic acid. Through a condensation reaction, in which the carboxylic acid of one amino acid is linked to the amine of another with removal of a water molecule, a peptide bond is formed. Polymers of amino acids are termed proteins and these molecules provide many catalytic metabolic functions for living processes. Any successful theory of abiogenesis must explain the origins and interactions of these five classes of molecules.
Many approaches to abiogenesis investigate how self-replicating molecules, or their components, came into existence. It is generally thought that current life on Earth is descended from an RNA world, although RNA-based life may not have been the first life to have existed. The classic Miller–Urey experiment and similar research demonstrated that most amino acids, the basic chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Various external sources of energy that may have triggered these reactions have been proposed, including lightning and radiation. Other approaches ("metabolism-first" hypotheses) focus on understanding how catalysis in chemical systems on the early Earth might have provided the precursor molecules necessary for self-replication. Complex organic molecules have been found in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.
The panspermia hypothesis alternatively suggests that microscopic life was distributed to the early Earth by meteoroids, asteroids and other small Solar System bodies and that life may exist throughout the Universe. It is speculated that the biochemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the age of the universe was only 10 to 17 million years old. The panspermia hypothesis proposes that life originated outside the Earth, not how life came to be.
Nonetheless, Earth remains the only place in the Universe known to harbour life, and fossil evidence from the Earth informs most studies of abiogenesis. More than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. The age of the Earth is about 4.54 billion years old; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, and possibly as early as the Eoarchean Era (between 3.6 and 4.0 billion years ago), after geological crust started to solidify following the molten Hadean Eon. In May 2017, evidence of the earliest known life on land may have been found in 3.48-billion-year-old geyserite and other related mineral deposits (often found around hot springs and geysers) uncovered in the Pilbara Craton of Western Australia. However, there have been a number of discoveries that suggested the earliest appearance of life on Earth was even earlier. Currently, microfossils within hydrothermal vent precipitates dated from 3.77 to 4.28 billion years old found in Quebec, Canada may be the oldest record of life on Earth, suggesting "an almost instantaneous emergence of life" after ocean formation 4.4 billion years ago. According to biologist Stephen Blair Hedges, "If life arose relatively quickly on Earth … then it could be common in the universe."Biogenesis

Biogenesis is the production of new living organisms or organelles. Conceptually, biogenesis is primarily attributed to Louis Pasteur and encompasses the belief that complex living things come only from other living things, by means of reproduction. That is, life does not spontaneously arise from non-living material, which was the position held by spontaneous generation. This is summarized in the phrase Omne vivum ex vivo, Latin for "all life [is] from life." A related statement is Omnis cellula e cellula, "all cells [are] from cells;" this conclusion is one of the central statements of cell theory.Timeline of human evolution

The timeline of human evolution outlines the major events in the development of the human species, Homo sapiens, and the evolution of our ancestors. It includes brief explanations of some of the species, genera, and the higher ranks of taxa that are seen today as possible ancestors of modern humans.
This timeline is based on studies from anthropology, paleontology, developmental biology, morphology, and from anatomical and genetic data. It does not address the origin of life, which discussion is provided by abiogenesis, but presents one possible line of evolutionary descent of species that eventually led to humans.Biogenic substance

A biogenic substance is a substance produced by life processes. It may be either constituents, or secretions, of plants or animals. A more specific name for these substances is biomolecules.Protoplanetary disk

A protoplanetary disk is a rotating circumstellar disk of dense gas and dust surrounding a young newly formed star, a T Tauri star, or Herbig Ae/Be star. The protoplanetary disk may also be considered an accretion disk for the star itself, because gases or other material may be falling from the inner edge of the disk onto the surface of the star. This process should nevertheless not be confused with the accretion process thought to build up the planets themselves. Externally illuminated photo-evaporating protoplanetary disks are called proplyds.Computer programming

Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.Computer program

A computer program is a collection of instructions that performs a specific task when executed by a computer. A computer requires programs to function and typically executes the program's instructions in a central processing unit.
A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
A part of a computer program that performs a well-defined task is known as an algorithm. A collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software or system software.Reflection (computer programming)

In computer science, reflection is the ability of a computer program to examine, introspect, and modify its own structure and behavior at runtime.This (computer programming)

Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.Comment (computer programming)

In computer programming, a comment is a programmer-readable explanation or annotation in the source code of a computer program. They are added with the purpose of making the source code easier for humans to understand, and are generally ignored by compilers and interpreters. The syntax of comments in various programming languages varies considerably.
Comments are sometimes processed in various ways to generate documentation external to the source code itself by documentation generators, or used for integration with source code management systems and other kinds of external programming tools.
The flexibility provided by comments allows for a wide degree of variability, but formal conventions for their use are commonly part of programming style guides.Callback (computer programming)

In computer programming, a callback is any executable code that is passed as an argument to other code, which is expected to call back (execute) the argument at a given time. This execution may be immediate as in a synchronous callback, or it might happen at a later time as in an asynchronous callback. In all cases, the intention is to specify a function or subroutine as an entity that is, depending on the language, more or less similar to a variable.
Programming languages support callbacks in different ways, often implementing them with subroutines, lambda expressions, blocks, or function pointers.Pointer (computer programming)

In computer science, a pointer is a programming language object, whose value refers to (or "points to") another value stored elsewhere in the computer memory using its memory address. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on the indexed page.
Pointers to data significantly improve performance for repetitive operations such as traversing strings, lookup tables, control tables and tree structures. In particular, it is often much cheaper in time and space to copy and dereference pointers than it is to copy and access the data to which the pointers point.
Pointers are also used to hold the addresses of entry points for called subroutines in procedural programming and for run-time linking to dynamic link libraries (DLLs). In object-oriented programming, pointers to functions are used for binding methods, often using what are called virtual method tables.
A pointer is a simple, more concrete implementation of the more abstract reference data type. Several languages support some type of pointer, although some have more restrictions on their use than others. While "pointer" has been used to refer to references in general, it more properly applies to data structures whose interface explicitly allows the pointer to be manipulated (arithmetically via pointer arithmetic) as a memory address, as opposed to a magic cookie or capability where this is not possible. Because pointers allow both protected and unprotected access to memory addresses, there are risks associated with using them particularly in the latter case. Primitive pointers are often stored in a format similar to an integer; however, attempting to dereference or "look up" a pointer whose value was never a valid memory address would cause a program to crash. To alleviate this potential problem, as a matter of type safety, pointers are considered a separate type parameterized by the type of data they point to, even if the underlying representation is an integer. Other measures may also be taken (such as validation & bounds checking), to verify the contents of the pointer variable contain a value that is both a valid memory address and within the numerical range that the processor is capable of addressing.Instrumentation (computer programming)

In the context of computer programming, instrumentation refers to an ability to monitor or measure the level of a product's performance, to diagnose errors and to write trace information. Programmers implement instrumentation in the form of code instructions that monitor specific components in a system (for example, instructions may output logging information to appear on screen). When an application contains instrumentation code, it can be managed using a management tool. Instrumentation is necessary to review the performance of the application. Instrumentation approaches can be of two types: Source instrumentation and binary instrumentation.Parameter (computer programming)

In computer programming, a parameter is a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine. These pieces of data are called arguments. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call can be assigned to the corresponding parameters.
Just as in standard mathematical usage, the argument is thus the actual input passed to a function, procedure, or routine, whereas the parameter is the variable inside the implementation of the subroutine. For example, if one defines the add subroutine as def add(x, y): return x + y, then x, y are parameters, while if this is called as add(2, 3), then 2, 3 are the arguments. Note that variables from the calling context can be arguments: if the subroutine is called as a = 2; b = 3; add(a, b) then the variables a, b are the arguments, not only the values 2, 3. See the Parameters and arguments section for more information.
In the most common case, call by value, a parameter acts within the subroutine as a variable initialized to the value of the argument (a local (isolated) copy of the argument if the argument is a variable), but in other cases, e.g. call by reference, the argument supplied by the caller can be affected by actions within the called subroutine (as discussed in evaluation strategy). In call by value, one can thus think of arguments as values (properly, think of the value of arguments as the "arguments" themselves), but in general arguments are not simply values.
The semantics for how parameters can be declared and how the arguments are passed to the parameters of subroutines are defined by the language, but the details of how this is represented in any particular computer system depend on the calling conventions of that system.Donald Trump

Donald John Trump (born June 14, 1946) is the 45th and current President of the United States, in office since January 20, 2017. Before entering politics, he was a businessman and television personality.
Trump was born in the New York City borough of Queens. He earned an economics degree from the Wharton School of the University of Pennsylvania. A third-generation businessman, Trump followed in the footsteps of his grandmother Elizabeth and father Fred in running the family real estate company. He served as chairman and president of The Trump Organization from 1971 until January 2017. Upon his inauguration as president, Trump delegated company management to his sons Donald Jr. and Eric. Trump's business career primarily focused on building or renovating office towers, hotels, casinos, and golf courses.
Trump also started several side ventures and branded various products with his name. He is credited as having written or co-written several books (including The Art of the Deal), and produced and hosted The Apprentice television series for 12 years. As of 2017, he was the 544th richest person in the world, with an estimated net worth of $3.5 billion.
Trump had long expressed interest in politics. He entered the 2016 presidential race as a Republican and defeated sixteen opponents in the primaries. Commentators described his political positions as populist, protectionist, and nationalist. His campaign received extensive free media coverage; many of his public statements were controversial or false. Trump won the general election on November 8, 2016, in a surprise victory against Democratic opponent Hillary Clinton. He became the oldest (by time of inauguration) and wealthiest person ever to assume the presidency, the first without prior military or government service, and the fifth to have won the election despite losing the popular vote. His election and policies have sparked numerous protests.
In domestic policy, Trump has unsuccessfully attempted to repeal and replace the Affordable Care Act. He appointed Neil Gorsuch to the Supreme Court. He ordered a travel ban on citizens from six Muslim-majority countries, citing security concerns; the ban was partially implemented after several legal challenges. In foreign policy, he withdrew the United States from the Trans-Pacific Partnership and the Paris Climate Agreement, undid parts of the Cuban Thaw, and ordered missile strikes in Syria in response to the Khan Shaykhun chemical attack. After Trump dismissed FBI Director James Comey, the Justice Department appointed his predecessor Robert Mueller as special counsel to investigate Russia's interference in the presidential election, potential links between Russia and Trump campaign associates, and any related matters.Family of Donald Trump

The family of Donald Trump, the President of the United States, is a prominent American family active in real estate, entertainment, business, and politics. Trump's immediate family circle is the First Family of the United States. They are part of the broader Trump family originating from Germany. Donald Trump has five children (between three wives) and nine grandchildren.Cabinet of Donald Trump

This article lists the members of President Donald Trump's Cabinet. Trump assumed office on January 20, 2017.
The President of the United States has the authority to nominate members of his or her Cabinet to the United States Senate for confirmation under Appointments Clause of the United States Constitution. Before confirmation, a high level career member of an executive department heads this pre-confirmed cabinet on an acting basis. The Cabinet's creation is part of the transition of power following the 2016 United States presidential election.
This page documents the confirmation process for any successful or unsuccessful cabinet nominees of Donald Trump's administration. They are listed in order of creation of the cabinet position (also used as the basis for the United States presidential line of succession).Donald Trump on social media

The presence of Donald Trump on social media has attracted attention worldwide since he joined Twitter in March 2009. He has frequently used Twitter to comment on politicians and celebrities, and he relied on Twitter significantly to communicate during the 2016 United States presidential election. The attention on Trump's Twitter activity has significantly increased since he was sworn in as the 45th President of the United States and continued to post controversial opinions and statements. Many of the assertions made by Trump on his Twitter account have been proven to be false.
Former White House Press Secretary Sean Spicer said during his tenure that Trump's tweets are "considered official statements by the President of the United States." According to a June 2017 Fox News poll, 70 percent of people say Trump's tweets are hurting his agenda and 17 percent say the tweets are helpful.Donald Trump Jr.

Donald John Trump Jr. (born December 31, 1977) is an American businessman and former reality TV personality. He is the oldest child of the 45th President of the United States, Donald Trump, and his first wife, Ivana.
A fourth generation businessman following in the footsteps of his great-grandmother who founded the company (Elizabeth Trump), grandfather Fred Trump, and father, he currently works alongside his brother Eric as a trustee and executive director of The Trump Organization. The trust was established to oversee all his father's assets during the latter's presidency.
Trump also has been involved in politics, especially since his father's presidential campaign, including facing criticism following the 2017 revelation of a meeting with a Russian lawyer, with the promise of receiving damaging information about Hillary Clinton's campaign in the 2016 presidential election.Presidency of Donald Trump

The presidency of Donald Trump began at noon EST on January 20, 2017, when Donald Trump was inaugurated as 45th President of the United States, succeeding Barack Obama. Trump, the Republican nominee, was a businessman and reality television personality from New York City at the time of his victory in the 2016 presidential election over the Democratic nominee Hillary Clinton. His running mate, former Governor Mike Pence of Indiana, took office as the 48th Vice President of the United States on the same day. Trump's term in office is set to end on January 20, 2021, though he is eligible for election to a second term and has declared his intention to run.
As of August 2017, Trump has issued 42 executive orders and 51 presidential memoranda. The executive order 13769 was revoked and replaced by executive order 13780; both orders denied admission to the U.S. of people from several foreign countries and were halted by federal courts until the Supreme Court partially reinstated order 13780. Trump's nominee to the Supreme Court, Neil Gorsuch, was confirmed by the United States Senate on April 7, 2017.Ivana Trump

Ivana Marie Trump (née Zelníčková, Czech pronunciation: [ˈɪvana ˈmarɪjɛ ˈzɛlɲiːtʃkovaː]; February 20, 1949) is a Czech-American businesswoman and former fashion model. She was the first wife of Donald Trump from 1977 until 1992.Political appointments by Donald Trump

This is a list of political appointments made by the 45th President of the United States, Donald Trump.
There are 1,212 presidential appointments which require confirmation by the U.S. Senate and 353 presidential appointments which do not require confirmation. The Washington Post has identified 601 key positions requiring U.S. Senate confirmation. As of October 11, 2017, 142 of Trump's nominees have been confirmed for those key positions, 165 are awaiting confirmation, and 11 have been announced but not yet formally nominated.
All members of the Cabinet require the advice and consent of the United States Senate following appointment by the President prior to taking office. The Vice Presidency is exceptional in that the position requires election to office pursuant to the United States Constitution. Although some are afforded Cabinet-level rank, non-cabinet members within the Executive Office of the President, such as White House Chief of Staff, National Security Advisor, and White House Press Secretary, do not hold constitutionally created positions and most do not require Senate confirmation for appointment.Donald Trump presidential campaign, 2016

The 2016 presidential campaign of Donald Trump, an American businessman, television personality, and author, was formally launched on June 16, 2015, at Trump Tower in New York City. Trump was the Republican nominee for President of the United States in the 2016 election, having won the most state primaries, caucuses, and delegates at the 2016 Republican National Convention. He chose Mike Pence, the sitting Governor of Indiana, as his vice presidential running mate. On November 8, 2016, Trump and Pence were elected president and vice president of the United States.
Trump's populist positions in opposition to illegal immigration and various trade agreements, such as the Trans-Pacific Partnership, earned him support especially among voters who were male, white, blue-collar and those without college degrees. Some of his remarks were controversial and helped his campaign garner extensive coverage by the mainstream media, trending topics, and social media.
Trump's campaign rallies attracted large crowds, as well as public controversy. Some of the events were marked by incidents of violence between Trump supporters and protesters, mistreatment of some journalists, and disruption by a large group of protesters who effectively shut down a major rally in Chicago. Trump was accused of inciting violence at his rallies.
Trump's disdain for political correctness was a staple theme of his campaign and proved popular among his supporters. Many, including some mainstream commentators and some prominent Republicans, viewed him as appealing to racism, a charge that he "repeatedly and vehemently denies." Trump's most polarizing and widely reported proposals were about issues of immigration and border security, especially his proposed deportation of all illegal immigrants, the proposed construction of a substantial wall on the Mexico–United States border at Mexican expense, his characterizations of many Mexican immigrants as "criminals, drug dealers, rapists, etc", and a temporary ban on foreign Muslims entering the U.S. (which he later modified to apply to people originating from countries with a history of terrorism against the United States or its allies).
Opposition to Trump grew during his campaign among both Republicans (who viewed Trump as irrevocably damaging to the party and its chances of winning elections during and after 2016, leading to the coalescence of the Stop Trump movement) and Democrats (who decried Trump's anti-immigrant and anti-Muslim policies, his behavior toward critics, his treatment of the media, and the ethno-nationalist alt-right's support of his campaign because of said policies and his anti-political correctness stance, which many cited to be a factor in the rise of hate crimes and other hate-motivated incidents against ethnic and religious minorities prior to and following Trump's win); although, some conservatives, liberals and independents criticized Republican Congressmembers for prioritizing party loyalty and avoiding alienation of Trump supporters to ensure re-election, over condemning several of Trump's actions. Some critics of Trump (among them Robert Reich, Keith Olbermann and Glenn Beck) had went so far – based on his immigration platforms, his circumvention of democratic norms, his tendencies to vehemently push back against critics (including revoking campaign access to certain members of the press for reporting stories he perceives as negative or critical), and his perceived narcissism – as to call Trump's mental health into question and warn that, if elected, he could put America in danger of either falling into an authoritarian regime or a dictatorship, or endangering its citizens by starting a nuclear war against foreign countries without justifiable provocation.Linux

Linux ( LIN-əks, less frequently  LY-nəks) is a name which broadly denotes a family of free and open-source software operating system distributions built around the Linux kernel. The defining component of a Linux distribution is the Linux kernel, an operating system kernel first released on September 17, 1991 by Linus Torvalds. Many Linux distributions use the word "Linux" in their name. The Free Software Foundation uses the name GNU/Linux to refer to the operating system family, as well as specific distributions, to emphasize that most Linux distributions are not just the Linux kernel, and that they have in common not only the kernel, but also numerous utilities and libraries, a large proportion of which are from the GNU project. This has led to some controversy.
Linux was originally developed for personal computers based on the Intel x86 architecture, but has since been ported to more platforms than any other operating system. Because of the dominance of the Linux kernel-based Android OS on smartphones, Linux has the largest installed base of all general-purpose operating systems. Linux is also the leading operating system on servers and other big iron systems such as mainframe computers, and is used on 99.6% of the TOP500 supercomputers. It is used by around 2.3% of desktop computers. The Chromebook, which runs the Linux kernel-based Chrome OS, dominates the US K–12 education market and represents nearly 20% of the sub-$300 notebook sales in the US. Linux also runs on embedded systems – devices whose operating system is typically built into the firmware and is highly tailored to the system. This includes TiVo and similar DVR devices, network routers, facility automation controls, televisions, video game consoles and smartwatches. Many smartphones and tablet computers run Android and other Linux derivatives.
The development of Linux is one of the most prominent examples of free and open-source software collaboration. The underlying source code may be used, modified and distributed‍—‌commercially or non-commercially‍—‌by anyone under the terms of its respective licenses, such as the GNU General Public License.
Typically, Linux is packaged in a form known as a Linux distribution (or distro for short) for both desktop and server use. Some of the most popular and mainstream Linux distributions are Arch Linux, CentOS, Debian, Fedora, Gentoo Linux, Linux Mint, Mageia, openSUSE and Ubuntu, together with commercial distributions such as Red Hat Enterprise Linux and SUSE Linux Enterprise Server. Distributions include the Linux kernel, supporting utilities and libraries, many of which are provided by the GNU Project, and usually a large amount of application software to fulfil the distribution's intended use. Desktop Linux distributions include a windowing system, such as X11, Mir or a Wayland implementation, and an accompanying desktop environment such as GNOME or KDE Plasma 5; some distributions may also include a less resource-intensive desktop, such as LXDE or Xfce. Distributions intended to run on servers may omit all graphical environments from the standard install, and instead include other software to set up and operate a solution stack such as LAMP. Because Linux is freely redistributable, anyone may create a distribution for any intended use.Bharat Operating System Solutions

Bharat Operating System Solutions (BOSS Linux) is a Linux distribution developed by C-DAC, Chennai in order to benefit the usage of Free/Open Source Software in India. BOSS Linux is a key deliverable of NRCFOSS.It has enhanced Desktop Environment integrated with Indian language support and other software. The latest stable release, version 6.0, was released in August 2015.
The software has been endorsed by the Government of India for adoption and implementation on a national scale. It was developed at Centre for Development of Advanced Computing (CDAC), Chennai. BOSS Linux is an "LSB certified" Linux distribution: the software has been certified by the Linux Foundation for compliance with the Linux Standard Base standard. BOSS Linux is derived from Debian Linux.
BOSS (Bharat Operating System Solutions)Linux distribution developed by C-DAC (Centre for Development of Advanced Computing) derived from Debian for enhancing the use of Free/ Open source software throughout India. BOSS Linux – a key deliverable of NRCFOSS has upgraded from entry-level server to advanced server. It supports Intel and AMD x86/x86-64 architecture. BOSS Linux advanced server has unique features such as web server, proxy server, database server, mail server, network server, file and print server, SMS server, LDAP server. BOSS Linux advanced server comprises administration tools such as webmin which is a web-based interface, Gadmin, PHP myadmin, PHP LDAP admin, PG admin. The accessibility of BOSS Linux will have a constructive impact on the digital divide in India as more people can now have access to software in their local language to use the Internet and other information and communications technology (ICT) facilities. Community Information centers (CICs) and internet cafes will also benefit from BOSS Linux as this software can be utilized to power these outlets and is affordable and easy to install, use and support.Hanthana Linux (operating system)

Hanthana Linux is a computer operating system based on the Fedora distribution, distributed as free and open source software.
It is specially designed to cater to the needs of Sri Lankan computer users who are unable to access the Internet frequently, with many most-wanted applications built in.
Hanthana is developed by the Sri Lanka-based Hanthana Community.Linux distribution

A Linux distribution (often abbreviated as distro) is an operating system made from a software collection, which is based upon the Linux kernel and, often, a package management system. Linux users usually obtain their operating system by downloading one of the Linux distributions, which are available for a wide variety of systems ranging from embedded devices (for example, OpenWrt) and personal computers (for example, Linux Mint) to powerful supercomputers (for example, Rocks Cluster Distribution).
A typical Linux distribution comprises a Linux kernel, GNU tools and libraries, additional software, documentation, a window system (the most common being the X Window System), a window manager, and a desktop environment. Most of the included software is free and open-source software made available both as compiled binaries and in source code form, allowing modifications to the original software. Usually, Linux distributions optionally include some proprietary software that may not be available in source code form, such as binary blobs required for some device drivers. A Linux distribution may also be described as a particular assortment of application and utility software (various GNU tools and libraries, for example), packaged together with the Linux kernel in such a way that its capabilities meet the needs of many users. The software is usually adapted to the distribution and then packaged into software packages by the distribution's maintainers. The software packages are available online in so-called repositories, which are storage locations usually distributed around the world. Beside glue components, such as the distribution installers (for example, Debian-Installer and Anaconda) or the package management systems, there are only very few packages that are originally written from the ground up by the maintainers of a Linux distribution.
Almost six hundred Linux distributions exist, with close to five hundred out of those in active development. Because of the huge availability of software, distributions have taken a wide variety of forms, including those suitable for use on desktops, servers, laptops, netbooks, mobile phones and tablets, as well as minimal environments typically for use in embedded systems. There are commercially backed distributions, such as Fedora (Red Hat), openSUSE (SUSE) and Ubuntu (Canonical Ltd.), and entirely community-driven distributions, such as Debian, Slackware, Gentoo and Arch Linux. Most distributions come ready to use and pre-compiled for a specific instruction set, while some distributions (such as Gentoo) are distributed mostly in source code form and compiled locally during installation.Ubuntu (operating system)

Ubuntu ( uu-BUUN-too, stylized as ubuntu) is an open source operating system for computers. It is one of the distribution systems of Linux, and is based on the Debian architecture. It is usually run on personal computers, and is also popular on network servers, usually running the Ubuntu Server variant, with enterprise-class features. Ubuntu runs on the most popular architectures, including Intel, AMD, and ARM-based machines. Ubuntu is also available for tablets and smartphones, with the Ubuntu Touch edition.
Ubuntu is published by Canonical Ltd, who offer commercial support. It is based on free software and named after the Southern African philosophy of ubuntu (literally, 'human-ness'), which Canonical Ltd. suggests can be loosely translated as "humanity to others" or "I am what I am because of who we all are".
Ubuntu is the most popular operating system running in hosted environments, so–called "clouds", as it is the most popular server Linux distribution.
Development of Ubuntu is led by UK-based Canonical Ltd., a company founded by South African entrepreneur Mark Shuttleworth. Canonical generates revenue through the sale of technical support and other services related to Ubuntu. The Ubuntu project is publicly committed to the principles of open-source software development; people are encouraged to use free software, study how it works, improve upon it, and distribute it.Operating system

An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs. All computer programs, excluding firmware, require an operating system to function.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.
The dominant desktop operating system is Microsoft Windows with a market share of around 83.3%. macOS by Apple Inc. is in second place (11.2%), and the varieties of Linux are collectively in third place (1.55%). In the mobile (smartphone and tablet combined) sector, according to third quarter 2016 data, Android by Google is dominant with 87.5 percent and a growth rate 10.3 percent per year, followed by iOS by Apple with 12.1 percent and a per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems, such as embedded and real-time systems, exist for many applications.Fedora (operating system)

Fedora  (formerly Fedora Core) is a Unix-like operating system based on the Linux kernel and GNU programs (a Linux distribution), developed by the community-supported Fedora Project and sponsored by the Red Hat company. Fedora contains software distributed under various free and open-source licenses and aims to be on the leading edge of such technologies. Fedora is the upstream source of the commercial Red Hat Enterprise Linux distribution.
Since the release of Fedora 21, three different editions are available: Workstation, focused on the personal computer, Server and Cloud for servers, and Atomic being the edition meant for cloud computing.
As of February 2016, Fedora has an estimated 1.2 million users, including Linus Torvalds, creator of the Linux kernel.Operating-system-level virtualization

Operating-system-level virtualization, also known as containerization, refers to an operating system feature in which the kernel allows the existence of multiple isolated user-space instances. Such instances, called containers, partitions, virtualization engines (VEs) or jails (FreeBSD jail or chroot jail), may look like real computers from the point of view of programs running in them. A computer program running on an ordinary person's computer's operating system can see all resources (connected devices, files and folders, network shares, CPU power, quantifiable hardware capabilities) of that computer. However, programs running inside a container can only see the container's contents and devices assigned to the container.
On Unix-like operating systems, this feature can be seen as an advanced implementation of the standard chroot mechanism, which changes the apparent root folder for the current running process and its children. In addition to isolation mechanisms, the kernel often provides resource-management features to limit the impact of one container's activities on other containers.Android (operating system)

Android is a mobile operating system developed by Google, based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets. Android's user interface is mainly based on direct manipulation, using touch gestures that loosely correspond to real-world actions, such as swiping, tapping and pinching, to manipulate on-screen objects, along with a virtual keyboard for text input. In addition to touchscreen devices, Google has further developed Android TV for televisions, Android Auto for cars, and Android Wear for wrist watches, each with a specialized user interface. Variants of Android are also used on game consoles, digital cameras, PCs and other electronics.
Initially developed by Android Inc., which Google bought in 2005, Android was unveiled in 2007, along with the founding of the Open Handset Alliance – a consortium of hardware, software, and telecommunication companies devoted to advancing open standards for mobile devices. Beginning with the first commercial Android device in September 2008, the operating system has gone through multiple major releases, with the current version being 8.0 "Oreo", released in August 2017. Android applications ("apps") can be downloaded from the Google Play store, which features over 2.7 million apps as of February 2017. Android has been the best-selling OS on tablets since 2013, and runs on the vast majority of smartphones. As of May 2017, Android has two billion monthly active users, and it has the largest installed base of any operating system.
Android's source code is released by Google under an open source license, although most Android devices ultimately ship with a combination of free and open source and proprietary software, including proprietary software required for accessing Google services. Android is popular with technology companies that require a ready-made, low-cost and customizable operating system for high-tech devices. Its open nature has encouraged a large community of developers and enthusiasts to use the open-source code as a foundation for community-driven projects, which deliver updates to older devices, add new features for advanced users or bring Android to devices originally shipped with other operating systems. The extensive variation of hardware in Android devices causes significant delays for software upgrades, with new versions of the operating system and security patches typically taking months before reaching consumers, or sometimes not at all. The success of Android has made it a target for patent and copyright litigation between technology companies.Network operating system

The term network operating system is used to refer to two rather different concepts:
A specialized operating system for a network device such as a router, switch or firewall.
An operating system oriented to computer networking, to allow shared file and printer access among multiple computers in a network, to enable the sharing of data, users, groups, security, applications, and other networking functions, typically over a local area network (LAN), or private network. This sense is now largely historical, as common operating systems generally now have such features included.Book of Enoch

The Book of Enoch (also 1 Enoch; Ge'ez: መጽሐፈ ሄኖክ mätṣḥäfä henok) is an ancient Jewish religious work, ascribed by tradition to Enoch, the great-grandfather of Noah, although modern scholars estimate the older sections (mainly in the Book of the Watchers) to date from about 300 BC, and the latest part (Book of Parables) probably to the first century BC.
It is not part of the biblical canon as used by Jews, apart from Beta Israel. Most Christian denominations and traditions may accept the Books of Enoch as having some historical or theological interest, but they generally regard the Books of Enoch as non-canonical or non-inspired. It is regarded as canonical by the Ethiopian Orthodox Tewahedo Church and Eritrean Orthodox Tewahedo Church, but not by any other Christian groups.
It is wholly extant only in the Ge'ez language, with Aramaic fragments from the Dead Sea Scrolls and a few Greek and Latin fragments. For this and other reasons, the traditional Ethiopian belief is that the original language of the work was Ge'ez, whereas non-Ethiopian scholars tend to assert that it was first written in either Aramaic or Hebrew; Ephraim Isaac suggests that the Book of Enoch, like the Book of Daniel, was composed partially in Aramaic and partially in Hebrew. No Hebrew version is known to have survived. It is asserted in the book itself that its author was Enoch, before the Biblical Flood.
Some of the authors of the New Testament were familiar with some of the content of the story. A short section of 1 Enoch (1:9) is cited in the New Testament, Epistle of Jude, Jude 1:14–15, and is attributed there to "Enoch the Seventh from Adam" (1 En 60:8), although this section of 1 Enoch is a midrash on Deuteronomy 33. Several copies of the earlier sections of 1 Enoch were preserved among the Dead Sea Scrolls.Second Book of Enoch

The Second Book of Enoch (usually abbreviated 2 Enoch, and otherwise variously known as Slavonic Enoch or The Secrets of Enoch) is a pseudepigraphic text (a text whose claimed authorship is unfounded) of the Old Testament. It is usually considered to be part of the Apocalyptic literature. The dating often preferred for the writing of 2 Enoch is late 1st century CE. The text has been preserved in full only in Slavonic, but in 2009 it was announced that Coptic fragments of the book had been identified. Greek is indicated as the language from which the Slavonic version was translated. 2 Enoch is not regarded as scripture by Jews or any Christian group. It was rediscovered and published at the end of the 19th century.
Most scholars consider 2 Enoch to be composed by an unknown Jewish sectarian group, while some authors think it is a 1st-century Christian text. Very few scholars consider it a later Christian work.
2 Enoch is distinct from the Book of Enoch, known as 1 Enoch. There is also an unrelated 3 Enoch. The numbering of these texts has been applied by scholars to distinguish the texts from one another.Enoch (ancestor of Noah)

Enoch (; Hebrew: חֲנוֹךְ, Modern H̱anokh, Tiberian Ḥănōḵ; Arabic: أَخْنُوخ‎‎ ʼAkhnūkh, [commonly in Qur'ānic literature]: إِدْرِيس ʼIdrīs) is a figure in Biblical literature. "In the seventh generation from Adam," he was considered the author of the Book of Enoch and also called Enoch the scribe of judgment. In addition to an appearance in the Book of Genesis of the Hebrew Bible, Enoch is the subject of many Jewish, Christian, and Muslim writings.
Enoch was the son of Jared (Genesis 5:19–21), the father of Methuselah, and the great-grandfather of Noah. At 65 years old, he begot Methuselah. Regim and Gaidad are also mentioned as his sons according to 2 Enoch.
The Bible says that Enoch lived 365 years before he was taken by God. The text reads that Enoch "walked with God: and he was no more; for God took him" (Gen 5:21–24), which some Christians interpret as Enoch entering Heaven alive.
This Enoch is not to be confused with Cain's son Enoch (Genesis 4:17). The Christian New Testament has three references to Enoch from the lineage of Seth (Luke 3:37, Hebrews 11:5, Jude 1:14–15).3 Enoch

3 Enoch is Biblical apocryphal in Hebrew. 3 Enoch purports to have been written in the 2nd century, but its origins can only be traced to the 5th century. Other names for 3 Enoch include "The Third Book of Enoch", "The Book of the Palaces", "The Book of Rabbi Ishmael the High Priest" and "The Revelation of Metatron".
Most commonly, the Book of Enoch refers to 1 Enoch, which survived completely only in Ge'ez. There is also a Second Book of Enoch, which has survived only in Old Slavonic.Python (programming language)

Python is a widely used high-level programming language for general-purpose programming, created by Guido van Rossum and first released in 1991. An interpreted language, Python has a design philosophy that emphasizes code readability (notably using whitespace indentation to delimit code blocks rather than curly brackets or keywords), and a syntax that allows programmers to express concepts in fewer lines of code than might be used in languages such as C++ or Java. The language provides constructs intended to enable writing clear programs on both a small and large scale.
Python features a dynamic type system and automatic memory management and supports multiple programming paradigms, including object-oriented, imperative, functional programming, and procedural styles. It has a large and comprehensive standard library.
Python interpreters are available for many operating systems, allowing Python code to run on a wide variety of systems. CPython, the reference implementation of Python, is open source software and has a community-based development model, as do nearly all of its variant implementations. CPython is managed by the non-profit Python Software Foundation.History of Python

The programming language Python was conceived in the late 1980s, and its implementation was started in December 1989 by Guido van Rossum at CWI in the Netherlands as a successor to the ABC (programming language) capable of exception handling and interfacing with the Amoeba operating system. Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, Benevolent Dictator for Life (BDFL). Python was named for the BBC TV show Monty Python's Flying Circus.
Python 2.0 was released on October 16, 2000, with many major new features, including a cycle-detecting garbage collector (in addition to reference counting) for memory management and support for Unicode. However, the most important change was to the development process itself, with a shift to a more transparent and community-backed process.
Python 3.0, a major, backwards-incompatible release, was released on December 3, 2008 after a long period of testing. Many of its major features have also been backported to the backwards-compatible Python 2.6 and 2.7.Python syntax and semantics

The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted (by both the runtime system and by human readers).Interpreted language

An interpreted language is a programming language for which most of its implementations execute instructions directly, without previously compiling a program into machine-language instructions. The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines already compiled into machine code.
The terms interpreted language and compiled language are not well defined because, in theory, any programming language can be either interpreted or compiled. In modern programming language implementation it is increasingly popular for a platform to provide both options.
Interpreted languages can also be contrasted with machine languages. Functionally, both execution and interpretation mean the same thing — fetching the next instruction/statement from the program and executing it. Although interpreted byte code is additionally identical to machine code in form and has an assembler representation, the term "interpreted" is practically reserved for "software processed" languages (by virtual machine or emulator) on top of the native (i.e. hardware) processor.
In principle, programs in many languages may be compiled or interpreted, emulated or executed natively, so this designation is applied solely based on common implementation practice, rather than representing an essential property of a language.
Many languages have been implemented using both compilers and interpreters, including BASIC, C, Lisp, Pascal, and Python. Java and C# are compiled into bytecode, the virtual machine-friendly interpreted language. Lisp implementations can freely mix interpreted and compiled code.Open-source model

The open-source model is a decentralized software development model that encourages open collaboration. A main principle of open-source software development is peer production, with products such as source code, blueprints, and documentation freely available to the public. The open-source movement in software began as a response to the limitations of proprietary code. The model is used for projects such as in open source appropriate technology, and open source drug discovery.
Open source promotes universal access via an open-source or free license to a product's design or blueprint, and universal redistribution of that design or blueprint. Before the phrase open source became widely adopted, developers and producers used a variety of other terms. Open source gained hold with the rise of the Internet. The open-source software movement arose to clarify copyright, licensing, domain, and consumer issues.
Generally, open source refers to a computer program in which the source code is available to the general public for use or modification from its original design. Open-source code is meant to be a collaborative effort, where programmers improve upon the source code and share the changes within the community. Code is released under the terms of a software license. Depending on the license terms, others may then download, modify, and publish their version (fork) back to the community.
Many large formal institutions have sprung up to support the development of the open-source movement, including the Apache Software Foundation, which supports community projects such as the open-source framework Apache Hadoop and the open-source HTTP server Apache HTTP.Open-source software

Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. According to scientists who studied it, open-source software is a prominent example of open collaboration. The term is often written without a hyphen as "open source software".
Open-source software development, or collaborative development from multiple independent sources, generates an increasingly more diverse scope of design perspective than any one company is capable of developing and sustaining long term. A 2008 report by the Standish Group states that adoption of open-source software models has resulted in savings of about $60 billion (£48 billion) per year to consumers.Open-source software movement

The open-source software movement is a movement that supports the use of open-source licenses for some or all software, a part of the broader notion of open collaboration. The open-source movement was started to spread the concept/idea of open-source software. Programmers who support the open-source movement philosophy contribute to the open-source community by voluntarily writing and exchanging programming code for software development. The term "open source" requires that no one can discriminate against a group in not sharing the edited code or hinder others from editing their already-edited work. This approach to software development allows anyone to obtain and modify open-source code. These modifications are distributed back to the developers within the open-source community of people who are working with the software. In this way, the identities of all individuals participating in code modification are disclosed and the transformation of the code is documented over time. This method makes it difficult to establish ownership of a particular bit of code but is in keeping with the open-source movement philosophy. These goals promote the production of "high quality programs" as well as "working cooperatively with other similarly minded people" to improve open-source technology.Free and open-source software

Free and open-source software (FOSS) is software that can be classified as both free software and open-source software. That is, anyone is freely licensed to use, copy, study, and change the software in any way, and the source code is openly shared so that people are encouraged to voluntarily improve the design of the software. This is in contrast to proprietary software, where the software is under restrictive copyright and the source code is usually hidden from the users.
The benefits of using FOSS can include decreased software costs, increased security and stability (especially in regard to malware), protecting privacy, and giving users more control over their own hardware. Free, open-source operating systems such as Linux and descendents of BSD are widely utilized today, powering millions of servers, desktops, smartphones (e.g. Android), and other devices. Free software licenses and open-source licenses are used by many software packages. The open-source software movement is an online social movement behind widespread production and adoption of FOSS.Open-source license

An open-source license is a type of license for computer software and other products that allows the source code, blueprint or design to be used, modified and/or shared under defined terms and conditions. This allows end users and commercial companies to review and modify the source code, blueprint or design for their own customization, curiosity or troubleshooting needs. Open-source licensed software is mostly available free of charge, though this does not necessarily have to be the case. Licenses which only permit non-commercial redistribution or modification of the source code for personal use only are generally not considered as open-source licenses. However, open-source licenses may have some restrictions, particularly regarding the expression of respect to the origin of software, such as a requirement to preserve the name of the authors and a copyright statement within the code, or a requirement to redistribute the licensed software only under the same license (as in a copyleft license). One popular set of open-source software licenses are those approved by the Open Source Initiative (OSI) based on their Open Source Definition (OSD).List of free and open-source software packages

This is a list of free and open-source software packages, computer software licensed under free software licenses and open-source licenses. Software that fits the Free Software Definition may be more appropriately called free software; the GNU project in particular objects to their works being referred to as open-source. For more information about the philosophical background for open-source software, see free software movement and Open Source Initiative. However, nearly all software meeting the Free Software Definition also meets the Open Source Definition and vice versa. A small fraction of the software that meets either definition is listed here.
Some of the open-source applications are also the basis of commercial products, shown in the List of commercial open-source applications and services.Open-source hardware

Open-source computing hardware comprises computers and computer components with an open design. They are designed as open-source hardware using open-source principles.Open-source governance

Open-source governance (also known as open politics) is a political philosophy which advocates the application of the philosophies of the open-source and open-content movements to democratic principles to enable any interested citizen to add to the creation of policy, as with a wiki document. Legislation is democratically opened to the general citizenry, employing their collective wisdom to benefit the decision-making process and improve democracy.
Theories on how to constrain, limit or enable this participation vary. Accordingly, there is no one dominant theory of how to go about authoring legislation with this approach. There are a wide array of projects and movements which are working on building open-source governance systems.
Many left-libertarian and radical centrist organizations around the globe have begun advocating open-source governance and its related political ideas as a reformist alternative to current governance systems. Often, these groups have their origins in decentralized structures such as the Internet and place particular importance on the need for anonymity to protect an individual's right to free speech in democratic systems. Opinions vary, however, not least because the principles behind open-source government are still very loosely defined.The Open Source Definition

The Open Source Definition is a document published by the Open Source Initiative, to determine whether a software license can be labeled with the open-source certification mark.
The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Bruce Perens with input from Eric S. Raymond and others.Open-source software development

Open-source software development is the process by which open-source software, or similar software whose source code is publicly available, is developed. These are software products available with its source code under an open-source license to study, change, and improve its design. Examples of some popular open-source software products are Mozilla Firefox, Google Chromium, Android, LibreOffice and the Apache OpenOffice Suite. Open-source software development has been a large part of the creation of the World Wide Web as we know it, with Tim Berners-Lee contributing his HTML code development as the original platform upon which the internet is now built.Asteroid belt

The asteroid belt is the circumstellar disc in the Solar System located roughly between the orbits of the planets Mars and Jupiter. It is occupied by numerous irregularly shaped bodies called asteroids or minor planets. The asteroid belt is also termed the main asteroid belt or main belt to distinguish it from other asteroid populations in the Solar System such as near-Earth asteroids and trojan asteroids. About half the mass of the belt is contained in the four largest asteroids: Ceres, Vesta, Pallas, and Hygiea. The total mass of the asteroid belt is approximately 4% that of the Moon, or 22% that of Pluto, and roughly twice that of Pluto's moon Charon (whose diameter is 1200 km).
Ceres, the asteroid belt's only dwarf planet, is about 950 km in diameter, whereas 4 Vesta, 2 Pallas, and 10 Hygiea have mean diameters of less than 600 km. The remaining bodies range down to the size of a dust particle. The asteroid material is so thinly distributed that numerous unmanned spacecraft have traversed it without incident. Nonetheless, collisions between large asteroids do occur, and these can produce an asteroid family whose members have similar orbital characteristics and compositions. Individual asteroids within the asteroid belt are categorized by their spectra, with most falling into three basic groups: carbonaceous (C-type), silicate (S-type), and metal-rich (M-type).
The asteroid belt formed from the primordial solar nebula as a group of planetesimals. Planetesimals are the smaller precursors of the protoplanets. Between Mars and Jupiter, however, gravitational perturbations from Jupiter imbued the protoplanets with too much orbital energy for them to accrete into a planet. Collisions became too violent, and instead of fusing together, the planetesimals and most of the protoplanets shattered. As a result, 99.9% of the asteroid belt's original mass was lost in the first 100 million years of the Solar System's history. Some fragments eventually found their way into the inner Solar System, leading to meteorite impacts with the inner planets. Asteroid orbits continue to be appreciably perturbed whenever their period of revolution about the Sun forms an orbital resonance with Jupiter. At these orbital distances, a Kirkwood gap occurs as they are swept into other orbits.
Classes of small Solar System bodies in other regions are the near-Earth objects, the centaurs, the Kuiper belt objects, the scattered disc objects, the sednoids, and the Oort cloud objects.
On 22 January 2014, ESA scientists reported the detection, for the first definitive time, of water vapor on Ceres, the largest object in the asteroid belt. The detection was made by using the far-infrared abilities of the Herschel Space Observatory. The finding was unexpected because comets, not asteroids, are typically considered to "sprout jets and plumes". According to one of the scientists, "The lines are becoming more and more blurred between comets and asteroids."Asteroid

Asteroids are minor planets, especially those of the inner Solar System. The larger ones have also been called planetoids. These terms have historically been applied to any astronomical object orbiting the Sun that did not show the disc of a planet and was not observed to have the characteristics of an active comet. As minor planets in the outer Solar System were discovered and found to have volatile-based surfaces that resemble those of comets, they were often distinguished from asteroids of the asteroid belt. In this article, the term "asteroid" refers to the minor planets of the inner Solar System including those co-orbital with Jupiter.
There are millions of asteroids, many thought to be the shattered remnants of planetesimals, bodies within the young Sun's solar nebula that never grew large enough to become planets. The large majority of known asteroids orbit in the asteroid belt between the orbits of Mars and Jupiter, or are co-orbital with Jupiter (the Jupiter trojans). However, other orbital families exist with significant populations, including the near-Earth objects. Individual asteroids are classified by their characteristic spectra, with the majority falling into three main groups: C-type, M-type, and S-type. These were named after and are generally identified with carbon-rich, metallic, and silicate (stony) compositions, respectively. The size of asteroids varies greatly, some reaching as much as 1000 km across.
Asteroids are differentiated from comets and meteoroids. In the case of comets, the difference is one of composition: while asteroids are mainly composed of mineral and rock, comets are composed of dust and ice. In addition, asteroids formed closer to the sun, preventing the development of the aforementioned cometary ice. The difference between asteroids and meteoroids is mainly one of size: meteoroids have a diameter of less than one meter, whereas asteroids have a diameter of greater than one meter. Finally, meteoroids can be composed of either cometary or asteroidal materials.
Only one asteroid, 4 Vesta, which has a relatively reflective surface, is normally visible to the naked eye, and this only in very dark skies when it is favorably positioned. Rarely, small asteroids passing close to Earth may be visible to the naked eye for a short time. As of October 2017, the Minor Planet Center had data on almost 745,000 objects in the inner and outer Solar System, of which almost 504,000 had enough information to be given numbered designations.
The United Nations declared June 30 as International Asteroid Day to educate the public about asteroids. The date of International Asteroid Day commemorates the anniversary of the Tunguska asteroid impact over Siberia, Russian Federation, on 30 June 1908.List of exceptional asteroids

The following is a collection of lists of exceptional asteroids in the Solar System. For the purposes of this article "asteroid" means minor planet up to the orbit of Jupiter, which includes the dwarf planet Ceres. For a complete list of minor planets in numerical order, see List of minor planets.
Asteroids are given a unique sequential identifying number once their orbit is precisely determined. Prior to this, they are known only by their systematic name or provisional designation, such as 1950 DA.Asteroid Belt (album)

Asteroid Belt is Velvet Chain's fifth album. It was released in November 2003 under the Freak Records label. It is a mix between trip-hop/electronica and jazz. It has a very out of this world feel to it, with many references to outer space in song titles and lyrics, sci-fi style noises in the background, as well using pictures of celestial bodies in the liner notes. This theme fits in well with both the electronica style songs as well as the more laid back pieces.Amor asteroid

The Amor asteroids are a group of near-Earth asteroids named after the asteroid 1221 Amor. They approach the orbit of Earth from beyond, but do not cross it. Most Amors cross the orbit of Mars. The two moons of Mars, Deimos and Phobos, may be Amor asteroids that were captured by Mars's gravity.
The most famous member of this group is 433 Eros, which was the first asteroid to be orbited and then landed upon by a human probe (NEAR Shoemaker).Asteroids in fiction

Asteroids and asteroid belts are a staple of science fiction stories. Asteroids play several potential roles in science fiction: as places which human beings might colonize; as resources for extracting minerals; as a hazard encountered by spaceships traveling between two other points; and as a threat to life on Earth due to potential impactsD-type asteroid

D-type asteroids have a very low albedo and a featureless reddish electromagnetic spectrum. It has been suggested that they have a composition of organic-rich silicates, carbon and anhydrous silicates, possibly with water ice in their interiors. D-type asteroids are found in the outer asteroid belt and beyond; examples are 152 Atala, and 944 Hidalgo as well as the majority of Jupiter trojans. It has been suggested that the Tagish Lake meteorite was a fragment from a D-type asteroid, and that the Martian moon Phobos is closely related.
The Nice model suggests that D-type asteroids may have originated in the Kuiper belt. 46 D-type asteroids are known, including 3552 Don Quixote, 944 Hidalgo, 624 Hektor, and 10199 Chariklo.Trojan (astronomy)

In astronomy, a trojan is a minor planet or moon that shares the orbit of a planet or larger moon, wherein the trojan remains in the same, stable position relative to the larger object. In particular, a trojan remains near one of the two trojan points of stability – designated L4 and L5 – which lie approximately 60° ahead of and behind the larger body, respectively. Trojan points make up two of five types of Lagrangian points, and a trojan is a type of Lagrangian object.
They are one type of co-orbital object. In this arrangement, the massive star and the smaller planet orbit about their common barycenter. A much smaller mass located at one of the Lagrangian points is subject to a combined gravitational force that acts through this barycenter. Hence the object can orbit around the barycenter with the same orbital period as the planet, and the arrangement can remain stable over time.
The Jupiter trojans account for most known trojans in the Solar System. They are divided into the Greek camp (L4) in front of and the Trojan camp (L5) trailing behind Jupiter in their orbit. More than 6,000 have been found so far and more than a million Jupiter trojans larger than one kilometer are thought to exist, whereas only a few Mars trojans (4) and Neptune trojans (17) have been found to date. Numerical calculations of the orbital dynamics involved indicate that Saturn and Uranus probably do not have any primordial trojans. The discovery of the first Earth trojan, 2010 TK7, was announced by NASA in 2011.
Unlike trojan minor planets that share the orbit with a planet, a trojan moon is a moon orbiting near the trojan point of another, larger moon. All known trojan moons are part of the Saturn system. Telesto and Calypso are trojans of Tethys, and Helene and Polydeuces of Dione. There is also a theoretical concept of a trojan planet, a planet that orbits at the trojan point of another, larger planet. Such a pair of co-orbital exoplanets was already thought to exist in another star system, but this claim was later retracted.Solar System

The Solar System is the gravitationally bound system comprising the Sun and the objects that orbit it, either directly or indirectly. Of those objects that orbit the Sun directly, the largest eight are the planets, with the remainder being significantly smaller objects, such as dwarf planets and small Solar System bodies. Of the objects that orbit the Sun indirectly, the moons, two are larger than the smallest planet, Mercury.
The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant interstellar molecular cloud. The vast majority of the system's mass is in the Sun, with the majority of the remaining mass contained in Jupiter. The four smaller inner planets, Mercury, Venus, Earth and Mars, are terrestrial planets, being primarily composed of rock and metal. The four outer planets are giant planets, being substantially more massive than the terrestrials. The two largest, Jupiter and Saturn, are gas giants, being composed mainly of hydrogen and helium; the two outermost planets, Uranus and Neptune, are ice giants, being composed mostly of substances with relatively high melting points compared with hydrogen and helium, called volatiles, such as water, ammonia and methane. All eight planets have almost circular orbits that lie within a nearly flat disc called the ecliptic.
The Solar System also contains smaller objects. The asteroid belt, which lies between the orbits of Mars and Jupiter, mostly contains objects composed, like the terrestrial planets, of rock and metal. Beyond Neptune's orbit lie the Kuiper belt and scattered disc, which are populations of trans-Neptunian objects composed mostly of ices, and beyond them a newly discovered population of sednoids. Within these populations are several dozen to possibly tens of thousands of objects large enough that they have been rounded by their own gravity. Such objects are categorized as dwarf planets. Identified dwarf planets include the asteroid Ceres and the trans-Neptunian objects Pluto and Eris. In addition to these two regions, various other small-body populations, including comets, centaurs and interplanetary dust clouds, freely travel between regions. Six of the planets, at least four of the dwarf planets, and many of the smaller bodies are orbited by natural satellites, usually termed "moons" after the Moon. Each of the outer planets is encircled by planetary rings of dust and other small objects.
The solar wind, a stream of charged particles flowing outwards from the Sun, creates a bubble-like region in the interstellar medium known as the heliosphere. The heliopause is the point at which pressure from the solar wind is equal to the opposing pressure of the interstellar medium; it extends out to the edge of the scattered disc. The Oort cloud, which is thought to be the source for long-period comets, may also exist at a distance roughly a thousand times further than the heliosphere. The Solar System is located in the Orion Arm, 26,000 light-years from the center of the Milky Way.16 Psyche

16 Psyche is one of the ten most massive asteroids in the asteroid belt. It is over 200 km (120 mi) in diameter and contains a little less than 1% of the mass of the entire asteroid belt. It is thought to be the exposed iron core of a protoplanet. It is the most massive metallic M-type asteroid. Psyche was discovered by the Italian astronomer Annibale de Gasparis on 17 March 1852 from Naples and named after the Greek mythological figure Psyche.Cradle of civilization

The term "cradle of civilization" refers to locations where, according to current archeological data, civilization is understood to have emerged. Current thinking is that there was no single "cradle", but several civilizations that developed independently, with the Fertile Crescent (Mesopotamia and Ancient Egypt) understood to be the earliest. Other civilizations arose in Asia among cultures situated along large river valleys, such as Indo-Gangetic Plain in the Indian subcontinent and the Yellow River in China. The extent to which there was significant influence between the early civilizations of the Near East and those of East Asia is disputed. Scholars accept that the civilizations of Mesoamerica, mainly in modern Mexico, and Norte Chico in present-day Peru emerged independently from those in Eurasia.
Scholars have defined civilization using various criteria such as the use of writing, cities, a class-based society, agriculture, animal husbandry, public buildings, metallurgy, and monumental architecture. The term cradle of civilization has frequently been applied to a variety of cultures and areas, in particular the Ancient Near Eastern Chalcolithic (Ubaid period) and Fertile Crescent, Ancient India and Ancient China. It has also been applied to ancient Anatolia, the Levant and Iran, and used to refer to culture predecessors—such as Ancient Greece as the predecessor of Western Civilization—even when such sites are not understood as an independent development of civilization, as well as within national rhetoric.In Search of the Cradle of Civilization

The term "cradle of civilization" refers to locations where, according to current archeological data, civilization is understood to have emerged. Current thinking is that there was no single "cradle", but several civilizations that developed independently, with the Fertile Crescent (Mesopotamia and Ancient Egypt) understood to be the earliest. Other civilizations arose in Asia among cultures situated along large river valleys, such as Indo-Gangetic Plain in the Indian subcontinent and the Yellow River in China. The extent to which there was significant influence between the early civilizations of the Near East and those of East Asia is disputed. Scholars accept that the civilizations of Mesoamerica, mainly in modern Mexico, and Norte Chico in present-day Peru emerged independently from those in Eurasia.
Scholars have defined civilization using various criteria such as the use of writing, cities, a class-based society, agriculture, animal husbandry, public buildings, metallurgy, and monumental architecture. The term cradle of civilization has frequently been applied to a variety of cultures and areas, in particular the Ancient Near Eastern Chalcolithic (Ubaid period) and Fertile Crescent, Ancient India and Ancient China. It has also been applied to ancient Anatolia, the Levant and Iran, and used to refer to culture predecessors—such as Ancient Greece as the predecessor of Western Civilization—even when such sites are not understood as an independent development of civilization, as well as within national rhetoric.Cradle-to-cradle design

Cradle-to-cradle design (also referred to as Cradle to Cradle, C2C, cradle 2 cradle, or regenerative design) is a biomimetic approach to the design of products and systems that models human industry on nature's processes viewing materials as nutrients circulating in healthy, safe metabolisms. The term itself is a play on the popular corporate phrase "Cradle to Grave," implying that the C2C model is sustainable and considerate of life and future generations (i.e. from the birth, or "cradle," of one generation to the next versus from birth to death, or "grave," within the same generation.)
C2C suggests that industry must protect and enrich ecosystems and nature's biological metabolism while also maintaining a safe, productive technical metabolism for the high-quality use and circulation of organic and technical nutrients. It is a holistic economic, industrial and social framework that seeks to create systems that are not only efficient but also essentially waste free. The model in its broadest sense is not limited to industrial design and manufacturing; it can be applied to many aspects of human civilization such as urban environments, buildings, economics and social systems.
The term Cradle to Cradle is a registered trademark of McDonough Braungart Design Chemistry (MBDC) consultants. Cradle to Cradle product certification began as a proprietary system; however, in 2012 MBDC turned the certification over to an independent non-profit called the Cradle to Cradle Products Innovation Institute. Independence, openness, and transparency are the Institute's first objectives for the certification protocols. The phrase "cradle to cradle" itself was coined by Walter R. Stahel in the 1970s. The current model is based on a system of "lifecycle development" initiated by Michael Braungart and colleagues at the Environmental Protection Encouragement Agency (EPEA) in the 1990s and explored through the publication A Technical Framework for Life-Cycle Assessment.
In 2002, Braungart and William McDonough published a book called Cradle to Cradle: Remaking the Way We Make Things, a manifesto for cradle to cradle design that gives specific details of how to achieve the model. The model has been implemented by a number of companies, organizations and governments around the world, predominantly in the European Union, China and the United States. Cradle to cradle has also been the subject of many documentary films, including the critically acclaimed Waste=Food.Nephilim

The Nephilim  (Hebrew: נְפִילִים‎) were the offspring of the "sons of God" and the "daughters of men" before the Deluge, according to Genesis 6:1-4 of the Bible.

When people began to multiply on the face of the ground, and daughters were born to them, the sons of God saw that they were fair; and they took wives for themselves of all that they chose. Then the Lord said, “My spirit shall not abide in mortals forever, for they are flesh; their days shall be one hundred twenty years.” The Nephilim were on the earth in those days—and also afterward—when the sons of God went in to the daughters of humans, who bore children to them. These were the heroes that were of old, warriors of renown.
— Genesis 6:1–4, New Revised Standard Version

The word is loosely translated as giants in some Bibles and left untranslated in others. The "sons of God" have been interpreted to be fallen angels according to some classical Judaic explanations.
According to Numbers 13:33, they later inhabited Canaan at the time of the Israelite conquest of Canaan.

The Lord said to Moses, "Send men to spy out the land of Canaan, which I am giving to the Israelites" ... So they went up and spied out the land ... And they told him: "... Yet the people who live in the land are strong, and the towns are fortified and very large; and besides, we saw the descendants of Anak there." ... So they brought to the Israelites an unfavorable report of the land that they had spied out, saying, "The land that we have gone through as spies is a land that devours its inhabitants; and all the people that we saw in it are of great size. 33 There we saw the Nephilim (the Anakites come from the Nephilim); and to ourselves we seemed like grasshoppers, and so we seemed to them."
— Numbers 13:1–2; 21; 27–28; 32–33. New Revised Standard Version.

A similar or identical biblical Hebrew term, read as "Nephilim" by some scholars, or as the word "fallen" by others, appears in Ezekiel 32:27.Book of Enoch

The Book of Enoch (also 1 Enoch; Ge'ez: መጽሐፈ ሄኖክ mätṣḥäfä henok) is an ancient Jewish religious work, ascribed by tradition to Enoch, the great-grandfather of Noah, although modern scholars estimate the older sections (mainly in the Book of the Watchers) to date from about 300 BC, and the latest part (Book of Parables) probably to the first century BC.
It is not part of the biblical canon as used by Jews, apart from Beta Israel. Most Christian denominations and traditions may accept the Books of Enoch as having some historical or theological interest, but they generally regard the Books of Enoch as non-canonical or non-inspired. It is regarded as canonical by the Ethiopian Orthodox Tewahedo Church and Eritrean Orthodox Tewahedo Church, but not by any other Christian groups.
It is wholly extant only in the Ge'ez language, with Aramaic fragments from the Dead Sea Scrolls and a few Greek and Latin fragments. For this and other reasons, the traditional Ethiopian belief is that the original language of the work was Ge'ez, whereas non-Ethiopian scholars tend to assert that it was first written in either Aramaic or Hebrew; Ephraim Isaac suggests that the Book of Enoch, like the Book of Daniel, was composed partially in Aramaic and partially in Hebrew. No Hebrew version is known to have survived. It is asserted in the book itself that its author was Enoch, before the Biblical Flood.
Some of the authors of the New Testament were familiar with some of the content of the story. A short section of 1 Enoch (1:9) is cited in the New Testament, Epistle of Jude, Jude 1:14–15, and is attributed there to "Enoch the Seventh from Adam" (1 En 60:8), although this section of 1 Enoch is a midrash on Deuteronomy 33. Several copies of the earlier sections of 1 Enoch were preserved among the Dead Sea Scrolls.Nephilim (role-playing game)

Nephilim is a role-playing game about powerful elemental entities reincarnating into human beings. The players take the roles of these beings as they adapt to their newly symbiotic existence and learn the secrets hidden behind veils of obscurity and mysticism, seeking the path toward enlightenment, Agartha. The game contains much symbolism, primarily related to the Hermetic tradition.Nephilim in popular culture

The Nephilim of Genesis 6 have become a notable motif in popular culture. This interlinks with other similar motifs regarding Christian demons in popular culture.The Book of Giants

The Book of Giants is an apocryphal Jewish book expanding a narrative in the Hebrew Bible. Its discovery at Qumran dates the text's creation to before the 2nd century BCE.
The Book of Giants is an antediluvian (pre-flood) narrative that was received primarily in Manichaean literature and known at Turpan. The Manicheans were a religious group based on the teachings of Mani. The earliest form of the book stems from an ancient Jewish Aramaic tradition regarding the Nephilim. The Nephilim in the Enoch version, are the offspring of fallen angels. The angels saw the beauty of the daughters of men, married them, and thus fathered giants. The book concerns itself with filling in the details about the giants and their offspring that the Book of Enoch is lacking. However, references are found in Genesis 6:1-4 and expanded upon in 1 Enoch. The Book of Giants tells of the background and fate of these Nephilim in the flood.Watcher (angel)

Watcher (Aramaic עִיר ʿiyr, plural עִירִין ʿiyrin, IPA ʕiːr(iːn); Theodotian trans: ir; from the root of Heb. ʿer, "awake, watchful"; Greek: ἐγρήγοροι, transl.: egrḗgoroi; Slav transliteration, Grigori, "Watchers", "those who are awake"; "guard", "watcher") is a term used in connection with biblical angels. Watcher occurs in both plural and singular forms in the Book of Daniel (4th–2nd century BC), where reference is made to their holiness. The apocryphal Books of Enoch (2nd–1st centuries BC) refer to both good and bad Watchers, with a primary focus on the rebellious ones.Many Waters

Many Waters is a 1986 novel by Madeleine L'Engle, part of the author's Time Quartet (also known as the Time Quintet). The title is taken from the Song of Solomon 8:7: "Many waters cannot quench love, neither can the floods drown it. If a man were to give all his wealth for love, it would be utterly scorned."
The principal characters of the story are Sandy and Dennys Murry, twin brothers who are, ironically, somewhat out of place (they are "normal") in the context of the multifarious and eccentric Murry family from A Wrinkle in Time. The action of the story follows that of A Wind in the Door but precedes the climactic, apocalyptic event in A Swiftly Tilting Planet.Elioud

In the Jewish mysticism set forth in the Book of Enoch and Book of Jubilees that was carried on by groups including the religious community of Qumran that produced the Dead Sea Scrolls, the Elioud (also transliterated Eljo) are the antediluvian children of the Nephilim and are considered a part-angel hybrid race of their own. Like the Nephilim, the Elioud are exceptional in both ability and wickedness.Brian Godawa

Brian James Godawa (born November 10, 1961) is an American screenwriter and author. He wrote the screenplay for To End All Wars and The Visitation, and co-wrote Change Your Life! with Adam Christing.
Godawa's book, Hollywood Worldviews: Watching Films with Wisdom and Discernment (ISBN 0830837132), is in its ninth printing. He is also a contributor to the BioLogos Forum.Azazeal

Azazeal (pronounced "ahz-azeel") is a fictional character in the British television series Hex, played by Michael Fassbender.
The character is seemingly based on the similarly spelled Azazel from The Book of Enoch. However, past the name, the similarities end. In biblical Apocrypha, the fallen angels referred to in Hex as the Nephilim were in fact called the Grigori, or "Watchers." In the Apocrypha, the Nephilim were offspring of the Grigori by mortal women. It is most likely the difference in spelling of Azazeal/Azazel and the change to Nephilim from Grigori were done for aesthetic reasons, though this has not been officially confirmed.
Azazeal seduced a series of women throughout the ages, dating from ancient Egypt to the present, but these women were usually killed by Ella Dee in order to stop the freedom of the Nephilim that his son's birth would signal.
In the second episode of Hex, Azazeal sacrifices Cassie Hughes' friend and roommate Thelma Bates, turning her into a ghost and releasing his power. Later in the series, he enters into a sexual relationship with Cassie and Cassie's teacher Jo, eventually turning her to his side. Through a series of demonic possessions, he conceives an heir, Malachi, with Cassie, and convinces the abortionist to put the child in his care at Christmas time.
In series two, after Cassie's death, Azazeal uses the mystical Stone of Belial to make Ella Dee relive her torture and execution, eventually causing her to lose her immortality and rapidly age to 500 or so years old, nearing death. He, an adult Malachi and Perie the "fairy" form a small alliance, before Azazeal is suddenly notified by higher powers that he is no longer to be Malachi's caregiver, replaced with Mephistopheles. He and Ella share a goodbye without trying to kill each other.International Monetary Fund

The International Monetary Fund (IMF) is an international organization headquartered in Washington, D.C., of "189 countries working to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world." Formed in 1944 at the Bretton Woods Conference primarily by the ideas of Harry Dexter White and John Maynard Keynes, it came into formal existence in 1945 with 29 member countries and the goal of reconstructing the international payment system. It now plays a central role in the management of balance of payments difficulties and international financial crises. Countries contribute funds to a pool through a quota system from which countries experiencing balance of payments problems can borrow money. As of 2016, the fund had SDR477 billion (about $668 billion).
Through the fund, and other activities such as the gathering of statistics and analysis, surveillance of its members' economies and the demand for particular policies, the IMF works to improve the economies of its member countries. The organisation's objectives stated in the Articles of Agreement are: to promote international monetary co-operation, international trade, high employment, exchange-rate stability, sustainable economic growth, and making resources available to member countries in financial difficulty.The International Monetary Fund and MalawiMexico and the International Monetary FundInternational monetary systems

International monetary systems are sets of internationally agreed rules, conventions and supporting institutions, that facilitate international trade, cross border investment and generally the reallocation of capital between nation states. They provide means of payment acceptable buyers and sellers of different nationality, including deferred payment. To operate successfully, they need to inspire confidence, to provide sufficient liquidity for fluctuating levels of trade and to provide means by which global imbalances can be corrected. The systems can grow organically as the collective result of numerous individual agreements between international economic factors spread over several decades. Alternatively, they can arise from a single architectural vision as happened at Bretton Woods in 1944.Arab Monetary Fund

The Arab Monetary Fund (AMF) is a regional Arab organization, a working sub-organization of the Arab League. It was founded 1976, and has been operational since 1977.Bretton Woods Conference

The Bretton Woods Conference, formally known as the United Nations Monetary and Financial Conference, was the gathering of 730 delegates from all 44 Allied nations at the Mount Washington Hotel, situated in Bretton Woods, New Hampshire, United States, to regulate the international monetary and financial order after the conclusion of World War II.
The conference was held from July 1–22, 1944. Agreements were signed that, after legislative ratification by member governments, established the International Bank for Reconstruction and Development (IBRD) and the International Monetary Fund (IMF).Venezuela and the International Monetary Fund

The Bolivarian Republic of Venezuela was a founding member of the International Monetary Fund in 1946. Venezuela's economy is highly dependent on oil production and exportation. This has led the economy to be prone to rapid changes given the price of oil fluctuates due to changes in demand and other macroeconomic conditions. In 2014, oil prices globally dropped considerably and Venezuela has since dealt with a significant decrease in state revenue. The economy has been contracting each quarter since then. This has caused Venezuela's Central Bank reserves to decline as a way of meeting their financial obligations. A chronic shortage of goods has occurred as well, with necessities such as medical supplies in short supply. Since 2007, Venezuela has not had official financial relations with either the International Monetary Fund or the World Bank when it paid off its debt five years ahead of schedule under Hugo Chávez. However, due to the current crisis and financial difficulties of the current administration the World Bank and IMF have stated their readiness to offer assistance if called upon.Annual Meetings of the International Monetary Fund and the World Bank Group

The IMF and World Bank meet each autumn in what is officially known as the Annual Meetings of the International Monetary Fund and the World Bank Group and each spring in the Spring Meetings of the International Monetary Fund and the World Bank Group. Names of the two groups are alternated each year so a different one has top billing.
The autumn meetings are customarily held in Washington, D.C., United States for two consecutive years, and in another member country in the third year. On the weekend of the autumn meetings the G7 also meets. The 2006 Annual Meetings and their supporting events were held from 11–20 September 2006 at the Suntec Singapore International Convention and Exhibition Centre in Singapore.
Since the mid-1990s, these meetings have centerpoints for anti-globalization movement protests. There have been complete bans on outdoor protests in the 2003 meetings in Dubai, United Arab Emirates as well as the 2006 meeting in Singapore, where only indoor demonstrations within a designated area is permitted. Some argue that such bans are out of safety concerns, while others consider them an effort to curb dissent. These measures has led to retaliatory actions by NGOs who targeted the organisers, as well as the IMF and World Bank for allegedly picking venues which are known to impose such restrictions.Asian Monetary Fund

The Asian Monetary Fund (AMF) was initially proposed by the Japanese government during the 1997 Asian financial crisis at the G7-IMF meetings in Hong Kong during September 20–25, 1997. It was aimed towards securing a regional network funded by Asian countries to overcome current and future economic crisis. During the Asian financial crisis, Asian leaders experienced the incompetence of both regional and international institutions such as the Association of Southeast Asian Nations (ASEAN) and particularly the International Monetary Fund (IMF). Countries such as Indonesia, Republic of Korea, and Thailand had to turn to bailouts from the IMF. However, the strict conditions of the IMF bailouts evoked discontent among Asian countries. This discontent was largely because Asian nations had little leverage on IMF crisis resolution measures despite the IMF being an international organisation. Another source of discontent was due to the financial volatility concerning the US dollar. More specifically it owed to the fixed exchange-rate system that Asian governments adopted throughout the 1990s, in which currency was pegged to the US dollar at rates incompatible with domestic economies. The initial proposal of the AMF stirred conflict between Japanese authorities and the United States.Hong Kong Monetary Authority

The Hong Kong Monetary Authority (HKMA, Chinese: 香港金融管理局 or 金管局) is Hong Kong's currency board and de facto central bank. It is a government authority founded on 1 April 1993 when the Office of the Exchange Fund and the Office of the Commissioner of Banking merged. The organisation reports directly to the Financial Secretary.Extraterrestrial life

Extraterrestrial life, also called alien life (or, if it is a sentient or relatively complex individual, an "extraterrestrial" or "alien"), is life that does not originate from Earth. These hypothetical life forms may range from simple prokaryotes to beings with civilizations far more advanced than humanity. The Drake equation speculates about the existence of intelligent life elsewhere in the universe. The science of extraterrestrial life in all its forms is known as exobiology.
Since the mid-20th century, there has been an ongoing search for signs of extraterrestrial life. The search encompasses a search for current and historic extraterrestrial life, and a narrower search for extraterrestrial intelligent life. Reflecting the category of search, methods range from the analysis of telescope and specimen data to radios used to detect and send signals of communication.
The concept of extraterrestrial life, and particularly extraterrestrial intelligence, has had a major cultural impact, chiefly including works of science fiction. Over the years, science fiction both communicated scientific ideas and influenced public interest and perspectives of extraterrestrial life. One shared space is the debate over the wisdom of attempting communication with possible extraterrestrial intelligence: Some encourage aggressive methods to try for contact with intelligent extraterrestrial life, while others argue that it may be dangerous to actively call attention to Earth.Assembly language

An assembly (or assembler) language, often abbreviated asm, is a low-level programming language for a computer, or other programmable device, in which there is a very strong (but often not one-to-one) correspondence between the language and the architecture's machine code instructions. Each assembly language is specific to a particular computer architecture. In contrast, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling. Assembly language may also be called symbolic machine code.
Assembly language is converted into executable machine code by a utility program referred to as an assembler. The conversion process is referred to as assembly, or assembling the source code. Assembly time is the computational step where an assembler is run.
Assembly language uses a mnemonic to represent each low-level machine instruction or opcode, typically also each architectural register, flag, etc. Many operations require one or more operands in order to form a complete instruction and most assemblers can take expressions of numbers and named constants as well as registers and labels as operands, freeing the programmer from tedious repetitive calculations. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.X86 assembly language

x86 assembly language is a family of backward-compatible assembly languages, which provide some level of compatibility all the way back to the Intel 8008 introduced in April 1972. x86 assembly languages are used to produce object code for the x86 class of processors. Like all assembly languages, it uses short mnemonics to represent the fundamental instructions that the CPU in a computer can understand and follow. Compilers sometimes produce assembly code as an intermediate step when translating a high level program into machine code. Regarded as a programming language, assembly coding is machine-specific and low level. Assembly languages are more typically used for detailed and time critical applications such as small real-time embedded systems or operating system kernels and device drivers.ARB assembly language

ARB assembly language is a low-level shading language, which can be characterized as an assembly language. It was created by the OpenGL Architecture Review Board (ARB) to standardize GPU instructions controlling the hardware graphics pipeline.Conditional assembly language

A conditional assembly language is that part of an assembly language used to write macros.Parrot assembly language

The Parrot assembly language (PASM) is the basic assembly language used by the Parrot virtual machine.
PASM is the lowest level assembly language in the Parrot stack. The Parrot intermediate representation (PIR) is PASM extended to simplify development of compilers.
The hello world program in PASM is simply:

print "Hello world!\n"
end

Although it appears similar to source code in some high-level programming languages, more complex PASM programs will resemble other assembly languages. The main exceptions to this low level programming in PASM are string handling and, as shown above, input and output. Additionally, PASM has automatic garbage collection from the virtual machine, and it does not allow pointer arithmetic.
Parrot assembly language has more instructions than hardware assembly languages, even CISC processors. This is because the marginal cost of creating a new instruction in Parrot is low compared to the marginal cost of doing so in hardware, and the creators of Parrot had no particular goal of minimalism.Typed assembly language

In computer science, a typed assembly language (TAL) is an assembly language that is extended to include a method of annotating the datatype of each value that is manipulated by the code. These annotations can then be used by a program (type checker) that processes the assembly language code in order to analyse how it will behave when it is executed. Specifically, such a type checker can be used to prove the type safety of code that meets the criteria of some appropriate type system.
Typed assembly languages usually include a high-level memory management system based on garbage collection.
A typed assembly language with a suitably expressive type system can be used to enable the safe execution of untrusted code without using an intermediate representation like bytecode, allowing features similar to those currently provided by virtual machine environments like Java and .NET.Directive (programming)

In computer programming, a directive or pragma (from "pragmatic") is a language construct that specifies how a compiler (or other translator) should process its input. Directives are not part of the grammar of a programming language, and may vary from compiler to compiler. They can be processed by a preprocessor to specify compiler behavior, or function as a form of in-band parameterization.
In some cases directives specify global behavior, while in other cases they only affect a local section, such as a block of programming code. In some cases, such as some C programs, directives are optional compiler hints, and may be ignored, but normally they are prescriptive, and must be followed. However, a directive does not perform any action in the language itself, but rather only a change in the behavior of the compiler.
This term could be used to refer to proprietary third party tags and commands (or markup) embedded in code that result in additional executable processing that extend the existing compiler, assembler and language constructs present in the development environment. The term "directive" is also applied in a variety of ways that are similar to the term command.IBM Basic assembly language and successors

Basic Assembly Language (BAL) is the commonly used term for a low-level programming language used on IBM System/360 and successor mainframes. Originally "Basic Assembly Language" applied only to an extremely restricted dialect designed to run under control of IBM Basic Programming Support (BPS/360) on systems with only 8 KB of main memory, and only a card reader, a card punch, and a printer for input/output — thus the word "Basic". However, the full name and the initialism "BAL" almost immediately attached themselves in popular use to all assembly-language dialects on the System/360 and its descendants. BAL for BPS/360 was introduced with the System/360 in 1964.
Assemblers on other System/360 operating systems through System/370, System/390, and System z, as well as the UNIVAC Series 90 mainframes made by Sperry Corporation, and the BS2000 Mainframes currently made by Fujitsu, inherited and extended its syntax. The latest derived language is known as the IBM High-Level Assembler (HLASM). Programmers utilizing this family of assemblers also refer to them as ALC, (for Assembly Language Coding), or simply "assembler".
BAL is also the mnemonic of the "Branch And Link" instruction.Offset (computer science)

In computer science, an offset within an array or other data structure object is an integer indicating the distance (displacement) between the beginning of the object and a given element or point, presumably within the same object. The concept of a distance is valid only if all elements of the object are of the same size (typically given in bytes or words).
For example, in A as an array of characters containing "abcdef", the fourth element containing the character 'd' has an offset of three from the start of A.Ethereum

Ethereum is an open-source, public, blockchain-based distributed computing platform featuring smart contract (scripting) functionality. It provides a decentralized Turing-complete virtual machine, the Ethereum Virtual Machine (EVM), which can execute scripts using an international network of public nodes. Ethereum also provides a cryptocurrency token called "ether", which can be transferred between accounts and used to compensate participant nodes for computations performed. "Gas", an internal transaction pricing mechanism, is used to mitigate spam and allocate resources on the network.
Ethereum was proposed in late 2013 by Vitalik Buterin, a cryptocurrency researcher and programmer. Development was funded by an online crowdsale during July–August 2014. The system went live on 30 July 2015.
In 2016 Ethereum was forked into two blockchains, as a result of the collapse of The DAO project, thereby creating Ethereum Classic.. The new forked version is Ethereum (ETH), subject of this article and the one that continued its existence is Ethereum Classic (ETC).Knights Templar

The Poor Fellow-Soldiers of Christ and of the Temple of Solomon (Latin: Pauperes commilitones Christi Templique Salomonici), also known as the Order of Solomon's Temple (French: Ordre du Temple or Templiers), the Knights Templar or simply as Templars, was a Catholic military order recognised in 1139 by papal bull Omne Datum Optimum of the Holy See. The order was founded in 1119 and active from about 1129 to 1312.
The order, which was among the wealthiest and most powerful, became a favoured charity throughout Christendom and grew rapidly in membership and power. They were prominent in Christian finance. Templar knights, in their distinctive white mantles with a red cross, were among the most skilled fighting units of the Crusades. Non-combatant members of the order managed a large economic infrastructure throughout Christendom, developing innovative financial techniques that were an early form of banking, and building fortifications across Europe and the Holy Land.
The Templars were closely tied to the Crusades; when the Holy Land was lost, support for the order faded. Rumours about the Templars' secret initiation ceremony created distrust, and King Philip IV of France – deeply in debt to the order – took advantage of the situation to gain control over them. In 1307, he had many of the order's members in France arrested, tortured into giving false confessions, and burned at the stake. Pope Clement V disbanded the order in 1312 under pressure from King Philip.
The abrupt reduction in power of a significant group in European society gave rise to speculation, legend, and legacy through the ages. The appropriation of their name by later organizations has kept the name "Templar" alive to the present day, while helping to obscure its origin.Knights Templar (Freemasonry)

This page is about a Masonic organization. For the medieval Knights Templar, see Knights Templar. See also Knights Templar and popular culture.
The Knights Templar, full name The United Religious, Military and Masonic Orders of the Temple and of St John of Jerusalem, Palestine, Rhodes and Malta, is a fraternal order affiliated with Freemasonry. Unlike the initial degrees conferred in a regular Masonic Lodge, which only require a belief in a Supreme Being regardless of religious affiliation, the Knights Templar is one of several additional Masonic Orders in which membership is open only to Freemasons who profess a belief in Christianity. One of the obligations entrants to the order are required to declare is to protect and defend the Christian faith. The word "United" in its full title indicates that more than one historical tradition and more than one actual order are jointly controlled within this system. The individual orders 'united' within this system are principally the Knights of the Temple (Knights Templar), the Knights of Malta, the Knights of St Paul, and only within the York Rite, the Knights of the Red Cross.
Like the Masonic Red Cross of Constantine being inspired by the Sacred Military Constantinian Order of Saint George and the Order of Malta being inspired by the Sovereign Military Order of Malta, the Masonic order of Knights Templar derives its name from the medieval Catholic military order Knights Templar. However, it does not claim any direct lineal descent from the original Templar order.Knights Templar in England

The history of the Knights Templar in England began when the French nobleman Hughes de Payens, the founder and Grand Master of the order of the Knights Templar, visited the country in 1128 to raise men and money for the Crusades.Knights Templar Cartel

Knights Templar—Guard of Michoacán (Spanish: Los Caballeros Templarios Guardia Michoacana) commonly known as the Knights Templar Cartel (Spanish: Los Caballeros Templarios) is a Mexican criminal organization composed of remnants of the defunct La Familia Michoacana drug cartel based in the Mexican State of Michoacán.
After the first alleged death of Nazario Moreno, leader of the La Familia Michoacana cartel, on 9 December 2010, a split between the cartel leaders emerged. Some of the cartel co-founders, Enrique Plancarte Solís, Servando Gómez Martínez, and Dionisio Loya Plancarte, formed an offshoot of La Familia calling itself Caballeros Templarios (or Knights Templar). A large part of "La Familia" left with them to form their Templars Organization, while José de Jesús Méndez Vargas kept the leadership of the now disbanded "Familia Michoacana", starting a fight for the control of Michoacán.
The Knights Templar Cartel indoctrinates its operatives to "fight and die" for the cartel. They have taken full control of the now-defunct La Familia Michoacana operations in states including Michoacán, Guerrero, the state of Mexico, and Morelos.
Along with the Sinaloa Cartel and Gulf Cartel, the Knights Templar formed a short-lived joint enforcer gang called Cárteles Unidos (English: United Cartels) or La Resistencia, composed of well-trained gunmen dedicated to kill and expel Los Zetas Cartel operatives who were invading the former Familia Michoacana territories in Michoacán and Jalisco.
The Templars' most recent feud is against the Jalisco New Generation Cartel, which is trying to gain full control of Jalisco and Michoacán, and also against Civilian vigilante and Militia groups that are fighting back the criminals in an attempt to clear Michoacan from the Knights Templar.
On February 27, 2015, Servando "La Tuta" Gomez Leader of the Knights Templar was arrested by the Mexican federal police. A non-specific number of his associates were also arrested and many properties were also seized by the Mexican government.List of Knights Templar

This is a list of some members of the Knights Templar, a powerful Christian military order during the time of the Crusades. At peak, the Order had approximately 20,000 members.
The Knights Templar were led by the Grand Master, originally based in Jerusalem, whose deputy was the Seneschal. Next in importance was the Marshal, who was responsible for individual commanders, horses, arms and equipment. He usually carried the standard or nominated a standard-bearer. The Commander of the Kingdom of Jerusalem was the treasurer and shared some authority with the Grand Master, balancing his power. Other cities also had Commanders with specific regional responsibilities.
The Grand Master and his Seneschal ruled over eight Templar provincial Masters in Europe, who were responsible for Apulia, Aragon (Catalonia), England, France, Hungary, Poitiers, Portugal and Scotland.
The bulk of the fighting force was made up of knights and sergeants. Knights, who usually came from the nobility, were the most prestigious and wore the white mantle and red cross over their armour, carried knightly weapons, rode horses and had the services of a squire. Sergeants filled other roles such as blacksmith or mason as well as fighting in battle. There were also squires who performed the task of caring for the horses.
For a separate list of Grand Masters, see Grand Masters of the Knights Templar.History of the Knights Templar

The history of the Order of the Knights Templar as a trans-national military-religious order spans two centuries of the High Middle Ages, from the Order's founding in the early 12th century to its suppression early in the 14th century.

The Knights Templar trace their origin back to shortly after the First Crusade. Around 1119, a French nobleman from the Champagne region, Hugues de Payens, collected eight of his knighted relatives including Godfrey de Saint-Omer, and began the Order, their stated mission to protect pilgrims on their journey to visit the Holy Places. They approached King Baldwin II of Jerusalem, who allowed them to set up headquarters on the Temple Mount. The Dome of the Rock, at the centre of the Mount, was understood to occupy the site of the Jewish Temple. Known to Christians throughout the Muslim occupation of Jerusalem as the Holy of Holies, the Dome of the Rock became a Christian church, the Templum Domini, the Temple of the Lord. But the Templars were lodged in the Aqsa Mosque, which was assumed to stand on the site of Solomon's Temple. Because the Aqsa mosque was known as the Templum Solomonis, it was not long before the knights had encompassed the association in their name. They became known as the Pauperes commilitones Christi Templique Solomonici – the Poor Fellow-Soldiers of Christ and of the Temple of Solomon, which was eventually shortened to "Knights Templar".
The original order consisted of Hugues de Payens and eight knights, two of whom were brothers and all of whom were his relatives by either blood or marriage: Godfrey de Saint-Omer, Payne de Monteverdi, Archambaud de St. Agnan, Andre de Montbard, Geoffrey Bison, and two men recorded only by the names of Rossal and Gondamer. The ninth knight remains unknown, although some have speculated that it was Count Hugh of Champagne himself — despite the Count returning to France in 1116 and documentary evidence showing that he joined the Knights on his third visit to the Holy Land in 1125.
Little was heard of the Order for their first nine years. But in 1129, after they were officially sanctioned by the church at the Council of Troyes, they became well known in Europe. Their fundraising campaigns asked for donations of money, land, or noble-born sons to join the Order, with the implication that donations would help both to defend Jerusalem, and to ensure the charitable giver of a place in Heaven. The Order's efforts were helped substantially by the patronage of Bernard of Clairvaux, the leading churchman of the time, and a nephew of one of the original nine knights. The Order at its outset had been subject to strong criticism, especially of the concept that religious men could also carry swords. In response to these critics, the influential Bernard of Clairvaux wrote a multi-page treatise entitled De Laude Novae Militae ("In Praise of the New Knighthood"), in which he championed their mission and defended the idea of a military religious order by appealing to the long-held Christian theory of just war, which legitimized “taking up the sword” to defend the innocent and the Church from violent attack. In doing so, Bernard legitimized the Templars, who became the first "warrior monks" of the Western world. Bernard wrote:
[A Templar Knight] is truly a fearless knight, and secure on every side, for his soul is protected by the armor of faith, just as his body is protected by the armor of steel. He is thus doubly-armed, and need fear neither demons nor men.

Shortly after its foundation in Jerusalem and due to possible previous links of the founding knights with the crusader Count Henry of Burgundy and with the House of Burgundy, and perhaps because of the family ties that Henry and his son Afonso had with Bernard of Clairvaux, the Knights Templar were already in the western edge of Europe, in the County of Portugal, at least from May 1122. The Templars settled there first, where the Order received donations and bought lands during the successive years of 1122, 1123, 1125, and 1126 (donated by D. Theresa), and 1127–28. Another possible reason for such exceptional early donations before the Council of Troyes, may be the alleged links of one or two founding knights of the Temple in Jerusalem, among the founding French knights of Champagne, Languedoc or other regions, Burgundy and possibly Flanders, with the County of Portugal - being of Portuguese origin, or Franco-Portuguese or Burgundian-Portuguese origin; claims sustained by chroniclers of the Templar Order in Portugal, written in the 16th, 17th and 18th centuries, supposedly basing themselves on original medieval source material of the Order of Christ.
Donations to the Order were considerable. The King of Aragon, in the Iberian Peninsula, left large tracts of land to the Order upon his death in the 1130s. New members to the Order were also required to swear religious vows of obedience, chastity, poverty and piety, and hand over all of their goods to the monastic brotherhood. This could include land, horses and any other items of material wealth, including labor from serfs, and interest in any businesses.
In 1139, even more power was conferred upon the Order by Pope Innocent II, who issued the papal bull, Omne Datum Optimum. It stated that the Knights Templar could pass freely through any border, owed no taxes, and were subject to no one's authority except that of the Pope. It was a remarkable confirmation of the Templars and their mission, which may have been brought about by the Order's patron, Bernard of Clairvaux, who had helped Pope Innocent in his own rise.
The Order grew rapidly throughout Western Europe, with chapters appearing in France, England, and Scotland, and then spreading to Spain and Portugal.Knights Templar Seal

The Grand Masters of the Knights Templar during the later 12th and the 13th century used a double-sided seal which showed a representation of The Dome of the Rock (or a circular dome of the Church of the Holy Sepulchre) on one side, and the Order's symbol of two knights on one horse on the other side. This design is first attested as in use by Bertrand de Blanquefort, the order's sixth Grand Master, in 1158, forty years after its foundation, and it remained in use until the dissolution of the order in 1312.
There was also a smaller, single-sided seal, which showed the Dome of the Rock (or the Holy Sepulchre), only.
Different seals were used by provincial masters of the order. According to a papal bull issued by Innocent IV in 1251, it was customary for successive provincial masters to use the same seal. The master of Provence continued to use an Agnus Dei seal, while the seal of the Aragonese master William of Cardona and his successors depicted a knight on horseback, carrying a lance and shield, on which was a cross bearing the legend: S. MINISTRI TEMPLI 1 ARAGON 7 CATALON ("Seal of the minister of the Temple in Aragon and Catalonia").Trials of the Knights Templar

The Knights Templar trace their beginnings to the Latin Kingdom of Jerusalem in c. 1120 when eight Christian knights, under the auspices of King Baldwin II and the Patriarch Warmund, were given the task of protecting pilgrims on the roads to Jerusalem, which they did for nine years until elevated to a military order at the Council of Troyes in 1129. They became an elite fighting force in the Crusades known for their propensity not to retreat or surrender.
Eventually, their rules of secrecy, their power, privileges and their wealth, made them vulnerable to the King of France’s accusations, and with the Pope’s unsuccessful attempts to prevent it, their destruction. The Templar leader, Master Jacques de Molay had recently come to France for meetings with the pope. In 1307, members of the Templar order in France were suddenly charged with heresy and arrested. In France, many ultimately, including their leader, were burned at the stake while others were sentenced to perpetual imprisonment. The events in France led to a series of trials in other locations, not all of which had the same outcome.Knights Templar and popular culture

The original historic Knights Templar were a Christian military order, the Order of the Poor Fellow Soldiers of Christ and of the Temple of Solomon, that existed from the 12th to 14th centuries to provide warriors in the Crusades. These men were famous in the high and late Middle Ages, but the Order was disbanded very suddenly by King Philip IV of France, who took action against the Templars in order to avoid repaying his own financial debts. He accused them of heresy, ordered the arrest of all Templars within his realm, and had many of them burned at the stake. The dramatic and rapid end of the organization led to many stories and legends developing about them over the following centuries. The Order and its members increasingly appear in modern fiction, though most of these references portray the medieval organization inaccurately.
In modern works, the Templars generally are portrayed as villains, misguided zealots, representatives of an evil secret society, or as the keepers of a long-lost treasure. Several modern organizations also claim heritage from the medieval Templars, as a way of enhancing their own image or mystique.List of Knights Templar sites

With their military mission and extensive financial resources, the Knights Templar funded a large number of building projects around Europe and the Holy Land, many structures remain standing today.Compiler

A compiler is computer software that transforms computer code written in one programming language (the source language) into another computer language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.
However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that is compiled. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.
Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.Compiler-compiler

A compiler is computer software that transforms computer code written in one programming language (the source language) into another computer language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.
However, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that is compiled. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.
A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.
Compilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.GNU Compiler Collection

The GNU Compiler Collection (GCC) is a compiler system produced by the GNU Project supporting various programming languages. GCC is a key component of the GNU toolchain and the standard compiler for most Unix-like Operating Systems. The Free Software Foundation (FSF) distributes GCC under the GNU General Public License (GNU GPL). GCC has played an important role in the growth of free software, as both a tool and an example.
Originally named the GNU C Compiler, when it only handled the C programming language, GCC 1.0 was released in 1987. It was extended to compile C++ in December of that year. Front ends were later developed for Objective-C, Objective-C++, Fortran, Java, Ada, and Go among others.
Version 4.5 of the OpenMP specification is now supported in the C and C++ compilers and a "much improved" implementation of the OpenACC 2.0a specification is also supported. By default, the current version supports gnu++14, a superset of C++14 and gnu11, a superset of C11, with strict standard support also available. It also provides experimental support for C++17 and later.
GCC has been ported to a wide variety of instruction set architectures, and is widely deployed as a tool in the development of both free and proprietary software. GCC is also available for most embedded systems, including ARM-based; AMCC, and Freescale Power Architecture-based chips. The compiler can target a wide variety of platforms.
As well as being the official compiler of the GNU operating system, GCC has been adopted as the standard compiler by many other modern Unix-like computer operating systems, including Linux and the BSD family, although FreeBSD and macOS have moved to the LLVM system. Versions are also available for Microsoft Windows and other operating systems; GCC can compile code for Android and iOS.Tiny C Compiler

The Tiny C Compiler (a.k.a. TCC, tCc, or TinyCC) is an x86, X86-64 and ARM processor C compiler created by Fabrice Bellard. It is designed to work for slow computers with little disk space (e.g. on rescue disks). Windows operating system support was added in version 0.9.23 (17 Jun 2005). TCC is distributed under the GNU Lesser General Public License (LGPL).
TCC claims to implement all of ANSI C (C89/C90), much of the C99 ISO standard, and many GNU C extensions including inline assembly.List of compilers

This page is intended to list all current compilers, compiler generators, interpreters, translators, tool foundations, assemblers, automatable command line interfaces (shells), etc.Intel C++ Compiler

Intel C++ Compiler, also known as icc or icl, is a group of C and C++ compilers from Intel available for Windows, macOS, Linux and Intel-based Android devices.Portable C Compiler

The Portable C Compiler (also known as pcc or sometimes pccm - portable C compiler machine) is an early compiler for the C programming language written by Stephen C. Johnson of Bell Labs in the mid-1970s, based in part on ideas proposed by Alan Snyder in 1973, and "distributed as the C compiler by Bell Labs... with the blessing of Dennis Ritchie."
One of the first compilers that could easily be adapted to output code for different computer architectures, the compiler had a long life span. It debuted in Seventh Edition Unix and shipped with BSD Unix until the release of 4.4BSD in 1994, when it was replaced by the GNU C Compiler. It was very influential in its day, so much so that at the beginning of the 1980s, the majority of C compilers were based on it. Anders Magnusson and Peter A Jonsson restarted development of pcc in 2007, rewriting it extensively to support the C99 standard.Small Device C Compiler

The Small Device C Compiler (SDCC) is a free-software, partially retargetable C compiler for microcontrollers. It is distributed under the GNU General Public License. The package also contains a linker, assembler, simulator and debugger. As of March 2007, SDCC is the only open-source C compiler for Intel 8051-compatible microcontrollers. In 2011 the compiler was downloaded on average more than 200 times per day.
The SDCC compiler was used by the FreeRTOS project to port its real-time operating system to the 8051-based Silabs (formerly Cygnal) series of microcontrollers.AMD Optimizing C/C++ Compiler

The AMD Optimizing C/C++ Compiler (AOCC) is a free, open source, optimizing compiler from AMD targeting 32-bit and 64-bit Linux platforms. It is based on LLVM Clang 4.0 with various additional patches to improve performance for AMD's Ryzen microprocessors. AOCC also includes a version of DragonEgg gcc plugin for Fortran sources.
Michael Larabel of Phoronix didn't find any real advantage of the Advanced Optimizing C/C++ Compiler over mainline Clang in a benchmark.Watcom C/C++

Watcom C/C++ (currently Open Watcom C/C++) is an integrated development environment (IDE) product from Watcom International Corporation for the C, C++, and Fortran programming languages. Watcom C/C++ was a commercial product until it was discontinued, then released as freeware under the name Open Watcom C/C++. It features tools for developing and debugging code for MS-DOS, OS/2, Windows, Linux operating systems, which are based upon x86, IA-32, x86-64 compatible processors.Human–computer interaction

Human–computer interaction (commonly referred to as HCI) researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.Outline of human–computer interaction

The following outline is provided as an overview of and topical guide to human–computer interaction:
Human–computer interaction – the intersection of computer science and behavioral sciences, this field involves the study, planning, and design of the interaction between people (users) and computers. Attention to human-machine interaction is important, because poorly designed human-machine interfaces can lead to many unexpected problems. A classic example of this is the Three Mile Island accident where investigations concluded that the design of the human–machine interface was at least partially responsible for the disaster.Human-Computer Interaction Institute

The Human-Computer Interaction Institute (HCII) is a department within the School of Computer Science at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania. It is considered one of the leading centers of human-computer interaction research, and was named one of the top ten most innovative schools in information technology by Computer World in 2008. For the past three decades, the institute has been the predominant publishing force at leading HCI venues, most notably ACM CHI, where it regularly contributes more than 10% of the papers. Research at the institute aims to understand and create technology that harmonizes with and improves human capabilities by integrating aspects of computer science, design, social science, and learning science.
HCII offers Human Computer Interaction (HCI) as an additional major for undergraduates, as well as a master's degree and PhDs in HCI. Students from various academic backgrounds come together from around the world to participate in this program. Students hold undergraduate degrees in psychology, design, and computer science, as well as many others. Students enter the program at various stages in their academic and professional careers. HCII research and educational programs span a full cycle of knowledge creation. The cycle includes research on how people work, play, and communicate within groups, organizations, and social structures. It includes the design, creation, and evaluation of technologies and tools to support human and social activities.Transparency (human–computer interaction)

Any change in a computing system, such as a new feature or new component, is transparent if the system after change adheres to previous external interface as much as possible while changing its internal behaviour. The purpose is to shield from change all systems (or human users) on the other end of the interface. Confusingly, the term refers to overall invisibility of the component, it does not refer to visibility of component's internals (as in white box or open system). The term transparent is widely used in computing marketing in substitution of the term invisible, since the term invisible has a bad connotation (usually seen as something that the user can't see and has no control over) while the term transparent has a good connotation (usually associated with not hiding anything). The vast majority of the times, the term transparent is used in a misleading way to refer to the actual invisibility of a computing process. Because of this misleading and counter-intuitive definition, modern computer literature tends to prefer use of "agnostic" over "transparent".
The term is used particularly often with regard to an abstraction layer that is invisible either from its upper or lower neighbouring layer.
Also temporarily used later around 1969 in IBM and Honeywell programming manuals the term referred to a certain computer programming technique. An application code was transparent when it was clear of the low-level detail (such as device-specific management) and contained only the logic solving a main problem. It was achieved through encapsulation – putting the code into modules that hid internal details, making them invisible for the main application.Modality (human–computer interaction)

Human–computer interaction (commonly referred to as HCI) researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.Human–computer interaction (security)

Any change in a computing system, such as a new feature or new component, is transparent if the system after change adheres to previous external interface as much as possible while changing its internal behaviour. The purpose is to shield from change all systems (or human users) on the other end of the interface. Confusingly, the term refers to overall invisibility of the component, it does not refer to visibility of component's internals (as in white box or open system). The term transparent is widely used in computing marketing in substitution of the term invisible, since the term invisible has a bad connotation (usually seen as something that the user can't see and has no control over) while the term transparent has a good connotation (usually associated with not hiding anything). The vast majority of the times, the term transparent is used in a misleading way to refer to the actual invisibility of a computing process. Because of this misleading and counter-intuitive definition, modern computer literature tends to prefer use of "agnostic" over "transparent".
The term is used particularly often with regard to an abstraction layer that is invisible either from its upper or lower neighbouring layer.
Also temporarily used later around 1969 in IBM and Honeywell programming manuals the term referred to a certain computer programming technique. An application code was transparent when it was clear of the low-level detail (such as device-specific management) and contained only the logic solving a main problem. It was achieved through encapsulation – putting the code into modules that hid internal details, making them invisible for the main application.Human–robot interaction

Human–robot interaction is the study of interactions between humans and robots. It is often referred as HRI by researchers. Human–robot interaction is a multidisciplinary field with contributions from human–computer interaction, artificial intelligence, robotics, natural language understanding, design, and social sciences.Interaction design

Interaction design, often abbreviated as IxD, is "the practice of designing interactive digital products, environments, systems, and services." While the digital side of this statement is true, interaction design is also useful when creating physical (non-digital) products, exploring how a user might interact with it. Common topics of interaction design include design, human–computer interaction, and software development. While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior. Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what clearly marks IxD as a design field as opposed to a science or engineering field.
While disciplines such as software engineering have a heavy focus on designing for technical stakeholders, interaction design is geared toward satisfying the majority of users.Keystroke-level model

In human–computer interaction, the keystroke-level model (KLM) predicts how long it will take an expert user to accomplish a routine task without errors using an interactive computer system. It was proposed by Stuart K. Card, Thomas P. Moran and Allen Newell in 1980 in the Communications of the ACM and published in their book The Psychology of Human-Computer Interaction in 1983, which is considered as a classic in the HCI field. The foundations were laid in 1974, when Card and Moran joined the Palo Alto Research Center (PARC) and created a group named Applied Information-Processing Psychology Project (AIP) with Newell as a consultant aiming to create an applied psychology of human-computer interaction. The keystroke-level model is still relevant today, which is shown by the recent research about mobile phones and touchscreens (see Adaptions).ACM Transactions on Computer-Human Interaction

Human–computer interaction (commonly referred to as HCI) researches the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. As a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.World economy

The world economy or global economy is the economy of the world, considered as the international exchange of goods and services that is expressed in monetary units of account (money). In some contexts, the two terms are distinguished: the "international" or "global economy" being measured separately and distinguished from national economies while the "world economy" is simply an aggregate of the separate countries' measurements. Beyond the minimum standard concerning value in production, use, and exchange the definitions, representations, models, and valuations of the world economy vary widely. It is inseparable from the geography and ecology of Earth.
It is common to limit questions of the world economy exclusively to human economic activity, and the world economy is typically judged in monetary terms, even in cases in which there is no efficient market to help valuate certain goods or services, or in cases in which a lack of independent research or government cooperation makes establishing figures difficult. Typical examples are illegal drugs and other black market goods, which by any standard are a part of the world economy, but for which there is by definition no legal market of any kind.
However, even in cases in which there is a clear and efficient market to establish a monetary value, economists do not typically use the current or official exchange rate to translate the monetary units of this market into a single unit for the world economy, since exchange rates typically do not closely reflect worldwide value, for example in cases where the volume or price of transactions is closely regulated by the government.

Rather, market valuations in a local currency are typically translated to a single monetary unit using the idea of purchasing power. This is the method used below, which is used for estimating worldwide economic activity in terms of real US dollars or euros. However, the world economy can be evaluated and expressed in many more ways. It is unclear, for example, how many of the world's 7.13 billion people have most of their economic activity reflected in these valuations.
As of 2017, the following 15 countries or regions have reached an economy of at least US$2 trillion by GDP in nominal or PPP terms: Brazil, China, France, Germany, India, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom, the United States, and the European Union.Economy

An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution, or trade, and consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasizes the practices, discourses, and material expressions associated with the production, use, and management of resources'. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. Monetary transactions only account for a small part of the economic domain.
Economic activity is spurred by production which uses natural resources, labor, and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, new markets, expands markets, diversification of markets, niche markets, increases revenue functions) such as that which produces intellectual property and changes in industrial relations (for example, child labor being replaced in some parts of the world with universal access to education).
A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone.
A market-based economy is where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency.
A command-based economy is where political agents directly control what is produced and how it is sold and distributed.
A green economy is low-carbon, resource efficient, and socially inclusive. In a green economy, growth in income and employment are driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services.Developed country

A developed country, industrialized country, more developed country, or "more economically developed country" (MEDC), is a sovereign state that has a highly developed economy and advanced technological infrastructure relative to other less industrialized nations. Most commonly, the criteria for evaluating the degree of economic development are gross domestic product (GDP), gross national product (GNP), the per capita income, level of industrialization, amount of widespread infrastructure and general standard of living. Which criteria are to be used and which countries can be classified as being developed are subjects of debate.
Developed countries have post-industrial economies, meaning the service sector provides more wealth than the industrial sector. They are contrasted with developing countries, which are in the process of industrialization, or undeveloped countries, which are pre-industrial and almost entirely agrarian. As of 2015, advanced economies comprise 60.8% of global GDP based on nominal values and 42.9% of global GDP based on purchasing-power parity (PPP) according to the International Monetary Fund. In 2015, the ten largest advanced economies by GDP in both nominal and PPP terms were Australia, Canada, France, Germany, Italy, Japan, South Korea, Spain, the United Kingdom, and the United States.Occam's razor

Occam's razor (also Ockham's razor; Latin: lex parsimoniae "law of parsimony") is a problem-solving principle attributed to William of Ockham (c. 1287–1347), who was an English Franciscan friar, scholastic philosopher, and theologian. His principle states that among competing hypotheses, the one with the fewest assumptions should be selected.
In science, Occam's razor is used as a heuristic guide in the development of theoretical models, rather than as a rigorous arbiter between candidate models. In the scientific method, Occam's razor is not considered an irrefutable principle of logic or a scientific result; the preference for simplicity in the scientific method is based on the falsifiability criterion. For each accepted explanation of a phenomenon, there may be an extremely large, perhaps even incomprehensible, number of possible and more complex alternatives. Since one can always burden failing explanations with ad hoc hypotheses to prevent them from being falsified, simpler theories are preferable to more complex ones because they are more testable.List of countries by GDP (nominal)

Gross domestic product (GDP) is the market value of all final goods and services from a nation in a given year. Countries are sorted by nominal GDP estimates from financial and statistical institutions, which are calculated at market or government official exchange rates. Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.
Comparisons of national wealth are also frequently made on the basis of purchasing power parity (PPP), to adjust for differences in the cost of living in different countries. PPP largely removes the exchange rate problem, but has its own drawbacks; it does not reflect the value of economic output in international trade, and it also requires more estimation than nominal GDP. On the whole, PPP per capita figures are less spread than nominal GDP per capita figures.
The United States is the world's largest economy with a GDP of approximately $18.56 trillion, notably due to high average incomes, a large population, capital investment, moderate unemployment, high consumer spending, a relatively young population, and technological innovation. Tuvalu is the world's smallest national economy with a GDP of about $32 million because of its very small population, a lack of natural resources, reliance on foreign aid, negligible capital investment, demographic problems, and low average incomes.
Although the rankings of national economies have changed considerably over time, the United States has maintained its top position since the Gilded Age, a time period in which its economy saw rapid expansion, surpassing the British Empire and Qing dynasty in aggregate output. Since China's transition to a market-based economy through privatisation and deregulation, the country has seen its ranking increase from ninth in 1978 to second to only the United States in 2016 as economic growth accelerated and its share of global nominal GDP surged from 2% in 1980 to 15% in 2016. India has also experienced a similar economic boom since the implementation of economic liberalisation in the early 1990s. When supranational entities are included, the European Union is the second largest economy in the world. It was the largest from 2004, when ten countries joined the union, to 2014, after which it was surpassed by the United States.
The first list largely includes data compiled by the International Monetary Fund's World Economic Outlook for 2016, the second list shows the World Bank's 2016 estimates, and the third list includes data compiled by the United Nations Statistics Division for 2015. Several economies which are not considered to be countries (the world, the European Union, and some dependent territories) are included in the lists because they appear in the sources as distinct economies. These economies are italicized and not ranked in the charts, but are listed where applicable.World Bank high-income economy

A high-income economy is defined by the World Bank as a country with a gross national income per capita US$12,236 or more in 2016, calculated using the Atlas method. While the term "high-income" is often used interchangeably with "First World" and "developed country", the technical definitions of these terms differ. The term "first world" commonly refers to countries that aligned themselves with the U.S. and NATO during the cold war. Several institutions, such as the Central Intelligence Agency (CIA) or International Monetary Fund (IMF), take factors other than high per capita income into account when classifying countries as "developed" or "advanced economies". According to the United Nations, for example, some high-income countries may also be developing countries. The GCC countries, for example, are classified as developing high-income countries. Thus, a high-income country may be classified as either developed or developing. Although the Holy See is a sovereign state, it is not classified by the World Bank under this definition.Nihilism

Nihilism ( or ; from the Latin nihil, nothing) is a philosophical doctrine that suggests the lack of belief in one or more reputedly meaningful aspects of life. Most commonly, nihilism is presented in the form of existential nihilism, which argues that life is without objective meaning, purpose, or intrinsic value. Moral nihilists assert that there is no inherent morality, and that accepted moral values are abstractly contrived. Nihilism may also take epistemological, ontological, or metaphysical forms, meaning respectively that, in some aspect, knowledge is not possible, or reality does not actually exist.
The term is sometimes used in association with anomie to explain the general mood of despair at a perceived pointlessness of existence that one may develop upon realising there are no necessary norms, rules, or laws. Movements such as futurism and deconstruction, among others, have been identified by commentators as "nihilistic".
Nihilism has also been described as conspicuous in or constitutive of certain historical periods: for example, Jean Baudrillard and others have called postmodernity a nihilistic epoch; and some religious theologians and figures of religious authority have asserted that postmodernity and many aspects of modernity represent a rejection of theism, and that such rejection of theistic doctrine entails nihilism.List of countries by past and projected GDP (nominal) per capita

This is an alphabetical list of countries by past and future gross domestic product per capita, based on official exchange rates, not on the purchasing power parity (PPP) methodology. Values are given in USDs and have not been adjusted for inflation. These figures have been taken from the International Monetary Fund's World Economic Outlook (WEO) database, April 2017 edition, World Bank, or various sources.Economy of India

The economy of India is a developing mixed economy. It is the world's sixth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP). The country ranks 141st in per capita GDP (nominal) with $1723 and 123rd in per capita GDP (PPP) with $6,616 as of 2016. After 1991 economic liberalisation, India achieved 6-7% average GDP growth annually. In FY 2015 and 2017 India's economy became the world's fastest growing major economy surpassing China.
The long-term growth prospective of the Indian economy is positive due to its young population, corresponding low dependency ratio, healthy savings and investment rates, and increasing integration into the global economy. India topped the World Bank's growth outlook for the first time in fiscal year 2015–16, during which the economy grew 7.6%. Growth is expected to have declined slightly to 7.1% for the 2016–17 fiscal year. According to the IMF, India's growth is expected to rebound to 7.2% in the 2017–18 fiscal and 7.7% in 2018–19.
India has one of the fastest growing service sectors in the world with an annual growth rate above 9% since 2001, which contributed to 57% of GDP in 2012–13. India has become a major exporter of IT services, Business Process Outsourcing (BPO) services, and software services with $154 billion revenue in FY 2017. This is the fastest-growing part of the economy. The IT industry continues to be the largest private-sector employer in India. India is the third-largest start-up hub in the world with over 3,100 technology start-ups in 2014–15 The agricultural sector is the largest employer in India's economy but contributes to a declining share of its GDP (17% in 2013–14). India ranks second worldwide in farm output. The industry sector has held a steady share of its economic contribution (26% of GDP in 2013–14). The Indian automobile industry is one of the largest in the world with an annual production of 21.48 million vehicles (mostly two and three wheelers) in 2013–14. India had $600 billion worth of retail market in 2015 and one of world's fastest growing e-commerce markets.Developing country

A developing country, also called a less developed country or an underdeveloped country, is a nation or a sovereign state with a less developed industrial base and a low Human Development Index (HDI) relative to other countries. There are no universally agreed-upon criteria for what makes a country developing versus developed and which countries fit these two categories, although there are general reference points such as a nation's GDP per capita compared with other nations. Also the general term less-developed country should not be confused with the specific least developed country. The term "developing" describes a currently observed situation and not a dynamic or expected direction of progress. Since the late 1990s developing countries tended to demonstrate higher growth rates than the developed ones.
There is criticism for using the term developing country. The term implies inferiority of a developing country or undeveloped country compared with a developed country, which many countries dislike. It assumes a desire to develop along the traditional Western model of economic development which a few countries, such as Cuba and Bhutan, choose not to follow. An alternative measurement that has been suggested is that of gross national happiness. Countries on the boundary between developed and developing are often categorized under the term newly industrialized countries. An alternative term also used is "low and middle income countries" (LMICs).
According to authors such as Walt Whitman Rostow developing countries are in transition from traditional lifestyles towards the modern lifestyle which began in the Industrial Revolution in the 18th and 19th centuries.
In the 2016 edition of its World Development Indicators, the World Bank made a decision to no longer distinguish between “developed” and “developing” countries in the presentation of its data. Nobody has ever agreed on a definition for these terms in the first place.Manuscript

A manuscript (abbreviated MS for singular and MSS for plural) is any document written by hand or typewritten, as opposed to being mechanically printed or reproduced in some indirect or automated way. More recently, it is understood to be an author's written, typed, or word-processed copy of a work, as distinguished from the print of the same. Before the arrival of printing, all documents and books were manuscripts. Manuscripts are not defined by their contents, which may combine writing with mathematical calculations, maps, explanatory figures or illustrations. Manuscripts may be in book form, scrolls or in codex format. Illuminated manuscripts are enriched with pictures, border decorations, elaborately embossed initial letters or full-page illustrations.Matenadaran

The Mesrop Mashtots Institute of Ancient Manuscripts (Armenian: Մեսրոպ Մաշտոցի անվան հին ձեռագրերի ինստիտուտ (Mesrop Mashtots'i anvan hin dzeragreri institut)), commonly referred to as the Matenadaran (Armenian:  Մատենադարան), is a repository of ancient manuscripts, research institute and museum in Yerevan, Armenia. It holds one of the world's richest depositories of medieval manuscripts and books which span a broad range of subjects, including theology, philosophy, history, medicine, literature, art history, and cosmography in Armenian and many other languages.Lamsa Bible

The Holy Bible from Ancient Eastern Manuscripts (commonly called the Lamsa Bible) was published by George M. Lamsa in 1933. It was derived, both Old and New Testaments, from the Syriac Peshitta, the Bible used by the Assyrian Church of the East and other Syriac Christian traditions.
Lamsa, following the tradition of his church, claimed that the Aramaic New Testament was written before the Greek version, a view known as Aramaic primacy. This contrasts with the academic mainstream opinion that the language of the New Testament was Greek. Lamsa thus claimed his translation was superior to versions based on later Greek manuscripts. The New Testament translators of the King James Version used an edition of Erasmus' Greek Textus Receptus.Timbuktu Manuscripts

Timbuktu Manuscripts or (Tombouctou Manuscripts) is a blanket term for the large number of historically important manuscripts that have been preserved for centuries in private households in Timbuktu, Mali. The collections include manuscripts about art, medicine, philosophy, and science, as well as priceless copies of the Quran. The number of manuscripts in the collections has been estimated as high as 700,000.
The manuscripts were written in Arabic and local languages like Songhay and Tamasheq. The dates of the manuscripts ranged between the late 13th and the early 20th centuries (i.e., from the Islamisation of the Mali Empire until the decline of traditional education in French Sudan). Their subject matter ranged from scholarly works to short letters. The manuscripts were passed down in Timbuktu families and were mostly in poor condition. Most of the manuscripts remain unstudied and uncatalogued, and their total number is unknown, affording only rough estimates. A selection of about 160 manuscripts from the Mamma Haidara Library in Timbuktu and the Ahmed Baba collection were digitized by the Tombouctou Manuscripts Project in the 2000s.
With the demise of Arabic education in Mali under French colonial rule, appreciation for the medieval manuscripts declined in Timbuktu, and many were being sold off. Time magazine related the account of an imam who picked up four of them for $50 each. In October 2008 one of the households was flooded, destroying 700 manuscripts.Biblical manuscript

A biblical manuscript is any handwritten copy of a portion of the text of the Bible. The word Bible comes from the Greek biblia (books); manuscript comes from Latin manu (hand) and scriptum (written). Biblical manuscripts vary in size from tiny scrolls containing individual verses of the Jewish scriptures (see Tefillin) to huge polyglot codices (multi-lingual books) containing both the Hebrew Bible (Tanakh) and the New Testament, as well as extracanonical works.
The study of biblical manuscripts is important because handwritten copies of books can contain errors. The science of textual criticism attempts to reconstruct the original text of books, especially those published prior to the invention of the printing press.Illuminated manuscript

An illuminated manuscript is a manuscript in which the text is supplemented with such decoration as initials, borders (marginalia) and miniature illustrations. In the strictest definition, the term refers only to manuscripts decorated with gold or silver; but in both common usage and modern scholarship, the term refers to any decorated or illustrated manuscript from Western traditions. Comparable Far Eastern and Mesoamerican works are described as painted. Islamic manuscripts may be referred to as illuminated, illustrated or painted, though using essentially the same techniques as Western works. This article covers the technical, social and economic history of the subject; for an art-historical account, see miniature.
The earliest surviving substantive illuminated manuscripts are from the period 400 to 600, produced in the Kingdom of the Ostrogoths and the Eastern Roman Empire. The significance of these works lies not only in their inherent artistic and historical value, but also in the maintenance of a link of literacy offered by non-illuminated texts. Had it not been for the monastic scribes of Late Antiquity, most literature of Greece and Rome would have perished in Europe. As it was, the patterns of textual survivals were shaped by their usefulness to the severely constricted literate group of Christians. Illumination of manuscripts, as a way of aggrandizing ancient documents, aided their preservation and informative value in an era when new ruling classes were no longer literate, at least in the language used in the manuscripts.
The majority of surviving manuscripts are from the Middle Ages, although many survive from the Renaissance, along with a very limited number from Late Antiquity. The majority of these manuscripts are of a religious nature. However, especially from the 13th century onward, an increasing number of secular texts were illuminated. Most illuminated manuscripts were created as codices, which had superseded scrolls. A very few illuminated manuscript fragments survive on papyrus, which does not last nearly as long as vellum or parchment. Most medieval manuscripts, illuminated or not, were written on parchment (most commonly of calf, sheep, or goat skin), but most manuscripts important enough to illuminate were written on the best quality of parchment, called vellum.
Beginning in the late Middle Ages manuscripts began to be produced on paper. Very early printed books were sometimes produced with spaces left for rubrics and miniatures, or were given illuminated initials, or decorations in the margin, but the introduction of printing rapidly led to the decline of illumination. Illuminated manuscripts continued to be produced in the early 16th century, but in much smaller numbers, mostly for the very wealthy. Manuscripts are among the most common items to survive from the Middle Ages; many thousands survive. They are also the best surviving specimens of medieval painting, and the best preserved. Indeed, for many areas and time periods, they are the only surviving examples of painting.List of Roman deities

A vast number of ancient Roman deities are known by name. The most familiar today are those the Romans identified with Greek counterparts (see interpretatio graeca), integrating Greek myths, iconography, and sometimes religious practices into Roman culture, including Latin literature, Roman art, and religious life as it was experienced throughout the Empire. Many of the Romans' own gods remain obscure, known only by name and function, through inscriptions and texts that are often fragmentary—particularly those who belong to the archaic religion of the Romans dating back to the era of kings, the so-called "religion of Numa," perpetuated or revived over the centuries. Some archaic deities have Italic or Etruscan counterparts, as identified both by ancient sources and by modern scholars. Throughout the Empire, the deities of peoples in the provinces were given new theological interpretations in light of functions or attributes they shared with Roman deities.
An extensive alphabetical list follows a survey of theological groups as constructed by the Romans themselves. For cult pertaining to deified Roman emperors (divi), see Imperial cult.Nubians

Nubians are an ethnolinguistic group indigenous to present-day Sudan and southern Egypt who originate from the early inhabitants of the central Nile valley, believed to be one of the earliest cradles of civilization. Nubian people have an ancient history predating dynastic Egypt. They speak the Nubian languages, which belong to the Nilo-Saharan language family.
In the pre-dynastic period, early Neolithic settlements have been found in the central Nubian region dating back to 7000 BCE, with Wadi Halfa believed to be the oldest settlement in the central Nile valley. During the dynastic period, parts of Nubia such as Ta-Seti (the first nome or administrative region of ancient Egypt) were continuously a part of ancient Egypt throughout the dynastic era Other parts of Nubia, particularly Southern or Upper Nubia, were at times a part of ancient Pharaonic Egypt and at other times a rival state representing parts of the Empire of Meroë or the Kushite Kingdom. However, at the Twenty-fifth Dynasty of Egypt, all of Nubia was united with ancient Egypt, or Kemet, extending down to modern day Khartoum.
Towards the end of the dynastic era, Upper Nubia broke off from Egypt proper. During that time, the Nubians founded a dynasty that ruled Upper and Lower Egypt during the 8th century BCE. As warriors, the ancient Nubians were famous for their skill and precision with the bow.
Today, people of Nubian descent primarily live in southern Egypt, especially in the Aswan area, and in northern Sudan, particularly in the region between the city of Wadi Halfa on the Egyptian-Sudanese border and Al Dabbah. Additionally, several groups known as the Hill Nubians live in the northern Nuba Mountains in South Kordofan state, Sudan. The main Nubian groups from north to south are the Halfaweyen, Sikut, Mahas and Dongola.H. P. Lovecraft

Howard Phillips Lovecraft (; August 20, 1890 – March 15, 1937) was an American author who achieved posthumous fame through his influential works of horror fiction. He was virtually unknown and published only in pulp magazines before he died in poverty, but he is now regarded as one of the most significant 20th-century authors in his genre. Lovecraft was born in Providence, Rhode Island, where he spent most of his life. Among his most celebrated tales are "The Call of Cthulhu" and "The Shadow over Innsmouth", both canonical to the Cthulhu Mythos. Lovecraft was never able to support himself from earnings as author and editor. He saw commercial success increasingly elude him in this latter period, partly because he lacked the confidence and drive to promote himself. He subsisted in progressively strained circumstances in his last years; an inheritance was completely spent by the time that he died at age 46.Boustrophedon

Boustrophedon  (Ancient Greek: βουστροφηδόν, boustrophēdón "ox-turning" from βοῦς, bous, "ox", στροφή, strophē, "turn" and the adverbial suffix -δόν, "like, in the manner of"; that is, turning like oxen in ploughing) is a kind of bi-directional text, mostly seen in ancient manuscripts and other inscriptions. Every other line of writing is flipped or reversed, with reversed letters. Rather than going left-to-right as in modern European languages, or right-to-left as in Arabic and Hebrew, alternate lines in boustrophedon must be read in opposite directions. Also, the individual characters are reversed, or mirrored. It was a common way of writing in stone in Ancient Greece.Data (computing)

Data ( DAY-tə, or  DAH-tə; treated as singular, plural, or as a mass noun) is any sequence of one or more symbols given meaning by specific act(s) of interpretation.
Data (or datum – a single unit of data) requires interpretation to become information. To translate data to information, there must be several known factors considered. The factors involved are determined by the creator of the data and the desired information. The term metadata is used to reference the data about the data. Metadata may be implied, specified or given. Data relating to physical events or processes will also have a temporal component. In almost all cases this temporal component is implied. This is the case when a device such as a temperature logger received data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal references of "now". So the device records the date, time and temperature together. When the data logger communicates temperatures, it must also report the date and time (metadata) for each temperature.
Digital data is data that is represented using the binary number system of ones (1) and zeros (0), as opposed to analog representation. In modern (post 1960) computer systems, all data is digital. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. See Parallel communication and Serial communication. Data sourced from an analog device, such as a temperature sensor, must pass through an "analog to digital converter" or "ADC" (see Analog-to-digital converter) to convert the analog data to digital data.
Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, or mechanical recording media, and transmitted in the form of digital electrical signals.
A program is a set of data that consists of a series of coded software instructions to control the operation of a computer or other machine. Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs.
Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures. Data pass in and out of computers via peripheral devices.
In an alternate usage, binary files (which are not human-readable) are sometimes called "data" as distinguished from human-readable "text". The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (= 281 exabytes). Digital data comes in these three states: data at rest, data in transit and data in use.Data type

In computer science and computer programming, a data type or simply type is a classification of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support various types of data, for example: real, integer or Boolean. A data type provides a set of values from which an expression (i.e. variable, function ...) may take its values. The type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored.Computer network

A computer network or data network is a digital telecommunications network which allows nodes to share resources. In computer networks, networked computing devices exchange data with each other using a data link. The connections between nodes are established using either cable media or wireless media.
Network computer devices that originate, route and terminate the data are called network nodes. Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols. This formidable collection of information technology requires skilled network management to keep it all running reliably.
Computer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. Computer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology and organizational intent. The best-known computer network is the Internet.Data structure

In computer science, a data structure is a particular way of organizing data in a computer so that it can be used efficiently.Computer data storage

Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.
The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) are referred to as "memory", while slower persistent technologies are referred to as "storage"; however, "memory" is sometimes also used when referring to persistent storage.
In the Von Neumann architecture, the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.Gun data computer

The gun data computer was a series of artillery computers used by the U.S. Army for coastal artillery, field artillery and antiaircraft artillery applications. In antiaircraft applications they were used in conjunction with a director.Central Air Data Computer

A Central Air Data Computer computes altitude, vertical speed, air speed, and mach number from sensor inputs such as pitot and static pressure and temperature. Early CADC systems were electromechanical computers, such as in the F-111. From 1968 to 1970, the first digital CADC was developed for the F-14. In the 1980s, the Standard Central Air Data Computer was developed to retrofit U.S. Air Force and U.S. Navy aircraft.Air data computer

An air data computer (ADC) is an essential avionics component found in modern glass cockpits. This computer, rather than individual instruments, can determine the calibrated airspeed, Mach number, altitude, and altitude trend data from an aircraft's pitot-static system.  In some very high speed aircraft such as the Space Shuttle, equivalent airspeed is calculated instead of calibrated airspeed.
The first air data computer patented in the US was developed by John H. Andresen.
Air data computers usually also have an input of total air temperature. This enables computation of static air temperature and true airspeed.
In Airbus aircraft the air data computer is combined with altitude, heading and navigation sources in a single unit known as the Air Data Inertial Reference Unit (ADIRU). This has now been replaced by Global Navigation Air Data Inertial Reference System (GNADIRS).Data Matrix

A Data Matrix code is a two-dimensional barcode consisting of black and white "cells" or modules arranged in either a square or rectangular pattern, also known as a matrix. The information to be encoded can be text or numeric data. Usual data size is from a few bytes up to 1556 bytes. The length of the encoded data depends on the number of cells in the matrix. Error correction codes are often used to increase reliability: even if one or more cells are damaged so it is unreadable, the message can still be read. A Data Matrix symbol can store up to 2,335 alphanumeric characters.
Data Matrix symbols are rectangular, usually square in shape and composed of square "cells" which represent bits. Depending on the coding used, a "light" cell represents a 0 and a "dark" cell is a 1, or vice versa. Every Data Matrix is composed of two solid adjacent borders in an "L" shape (called the "finder pattern") and two other borders consisting of alternating dark and light "cells" or modules (called the "timing pattern"). Within these borders are rows and columns of cells encoding information. The finder pattern is used to locate and orient the symbol while the timing pattern provides a count of the number of rows and columns in the symbol. As more data is encoded in the symbol, the number of cells (rows and columns) increases. Each code is unique. Symbol sizes vary from 10×10 to 144×144 in the new version ECC 200, and from 9×9 to 49×49 in the old version ECC 000 - 140.Torpedo Data Computer

The Torpedo Data Computer (TDC) was an early electromechanical analog computer used for torpedo fire-control on American submarines during World War II. Britain, Germany, and Japan also developed automated torpedo fire control equipment, but none were as advanced as the US Navy's TDC, as it was able to automatically track the target rather than simply offering an instantaneous firing solution. This unique capability of the TDC set the standard for submarine torpedo fire control during World War II.
Replacing the previously standard hand-held slide rule-type devices (known as the "banjo" & "is/was"), the TDC was designed to provide fire-control solutions for submarine torpedo firing against ships running on the surface (surface warships used a different computer).
The TDC was a rather bulky addition to the sub's conning tower and required two extra crewmen: one as an expert in its maintenance, the other as its actual operator. Despite these drawbacks, the use of the TDC was an important factor in the successful commerce raiding program conducted by American submarines during the Pacific campaign of World War II. Accounts of the American submarine campaign in the Pacific often cite the use of TDC. Some officers became highly skilled in its use, and the navy set up a training school for its use.
Two upgraded World War II-era U.S. Navy fleet submarines (USS Tusk and Cutlass) with their TDCs continue to serve with Taiwan's navy and U.S. Nautical Museum staff are assisting them with maintaining their equipment. The museum also has a fully restored and functioning TDC from USS Pampanito, docked in San Francisco.Source code

In computing, source code is any collection of computer instructions, possibly with comments, written using a human-readable programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.
Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.Spaghetti code

Spaghetti code is a pejorative phrase for source code that has a complex and tangled control structure, especially one using many GOTO statements, exceptions, threads, or other "unstructured" branching constructs. It is named such because program flow is conceptually like a bowl of spaghetti, i.e. twisted and tangled. Spaghetti code can be caused by several factors, such as continuous modifications by several people with different programming styles over a long life cycle. Structured programming greatly decreases the incidence of spaghetti code.Computer programming

Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.Assembly language

An assembly (or assembler) language, often abbreviated asm, is a low-level programming language for a computer, or other programmable device, in which there is a very strong (but often not one-to-one) correspondence between the language and the architecture's machine code instructions. Each assembly language is specific to a particular computer architecture. In contrast, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling. Assembly language may also be called symbolic machine code.
Assembly language is converted into executable machine code by a utility program referred to as an assembler. The conversion process is referred to as assembly, or assembling the source code. Assembly time is the computational step where an assembler is run.
Assembly language uses a mnemonic to represent each low-level machine instruction or opcode, typically also each architectural register, flag, etc. Many operations require one or more operands in order to form a complete instruction and most assemblers can take expressions of numbers and named constants as well as registers and labels as operands, freeing the programmer from tedious repetitive calculations. Depending on the architecture, these elements may also be combined for specific instructions or addressing modes using offsets or other data as well as fixed addresses. Many assemblers offer additional mechanisms to facilitate program development, to control the assembly process, and to aid debugging.